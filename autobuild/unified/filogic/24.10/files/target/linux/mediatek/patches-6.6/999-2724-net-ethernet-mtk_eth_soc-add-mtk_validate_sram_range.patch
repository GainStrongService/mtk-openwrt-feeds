From 980ca92e889eaebfc72fc14105b76e7a215ebf0c Mon Sep 17 00:00:00 2001
From: Bo-Cun Chen <bc-bocun.chen@mediatek.com>
Date: Wed, 21 May 2025 17:56:06 +0800
Subject: [PATCH] net: ethernet: mtk_eth_soc: add mtk_validate_sram_range to
 MTK_SRAM feature

To prevent the ETH Driver from allocating a non-existent SRAM region to
the TX/RX/FQ rings, we have introduced a new function, mtk_validate_sram
_range(), which verifies whether the SRAM region is available for these
rings. Consequently, even if certain platforms lack sufficient SRAM to
allocate for all TX/RX/FQ rings, they can still enable MTK_SRAM feature.

Signed-off-by: Bo-Cun Chen <bc-bocun.chen@mediatek.com>
---
 drivers/net/ethernet/mediatek/mtk_eth_soc.c | 91 +++++++++++++--------
 drivers/net/ethernet/mediatek/mtk_eth_soc.h | 23 ++++--
 2 files changed, 75 insertions(+), 39 deletions(-)

diff --git a/drivers/net/ethernet/mediatek/mtk_eth_soc.c b/drivers/net/ethernet/mediatek/mtk_eth_soc.c
index 67f9ae5..63dee21 100644
--- a/drivers/net/ethernet/mediatek/mtk_eth_soc.c
+++ b/drivers/net/ethernet/mediatek/mtk_eth_soc.c
@@ -1436,6 +1436,18 @@ static void *mtk_max_lro_buf_alloc(gfp_t gfp_mask)
 	return (void *)data;
 }
 
+static bool mtk_validate_sram_range(struct mtk_eth *eth, u64 offset, u64 size)
+{
+	u64 start, end;
+
+	start = (u64)eth->sram_base;
+	end = (u64)eth->sram_base + eth->sram_size - 1;
+	if ((offset >= start) && (offset + size - 1 <= end))
+		return true;
+
+	return false;
+}
+
 /* the qdma core needs scratch memory to be setup */
 static int mtk_init_fq_dma(struct mtk_eth *eth)
 {
@@ -1445,28 +1457,31 @@ static int mtk_init_fq_dma(struct mtk_eth *eth)
 	dma_addr_t dma_addr;
 	int i, j, len;
 
-	if (MTK_HAS_CAPS(eth->soc->caps, MTK_SRAM))
-		eth->scratch_ring = eth->sram_base;
-	else
-		eth->scratch_ring = dma_alloc_coherent(eth->dma_dev,
-						       cnt * soc->tx.desc_size,
-						       &eth->phy_scratch_ring,
-						       GFP_KERNEL);
+	if (MTK_HAS_CAPS(eth->soc->caps, MTK_SRAM) &&
+	    mtk_validate_sram_range(eth, (u64)eth->sram_base,
+				    cnt * soc->tx.desc_size)) {
+		eth->fq_ring.scratch_ring = eth->sram_base;
+		eth->fq_ring.in_sram = true;
+	} else
+		eth->fq_ring.scratch_ring = dma_alloc_coherent(eth->dma_dev,
+							       cnt * soc->tx.desc_size,
+							       &eth->fq_ring.phy_scratch_ring,
+							       GFP_KERNEL);
 
-	if (unlikely(!eth->scratch_ring))
+	if (unlikely(!eth->fq_ring.scratch_ring))
 		return -ENOMEM;
 
-	phy_ring_tail = eth->phy_scratch_ring + soc->tx.desc_size * (cnt - 1);
+	phy_ring_tail = eth->fq_ring.phy_scratch_ring + soc->tx.desc_size * (cnt - 1);
 
 	for (j = 0; j < DIV_ROUND_UP(soc->tx.fq_dma_size, MTK_FQ_DMA_LENGTH); j++) {
 		len = min_t(int, cnt - j * MTK_FQ_DMA_LENGTH, MTK_FQ_DMA_LENGTH);
-		eth->scratch_head[j] = kcalloc(len, MTK_QDMA_PAGE_SIZE, GFP_KERNEL);
+		eth->fq_ring.scratch_head[j] = kcalloc(len, MTK_QDMA_PAGE_SIZE, GFP_KERNEL);
 
-		if (unlikely(!eth->scratch_head[j]))
+		if (unlikely(!eth->fq_ring.scratch_head[j]))
 			return -ENOMEM;
 
 		dma_addr = dma_map_single(eth->dma_dev,
-					  eth->scratch_head[j], len * MTK_QDMA_PAGE_SIZE,
+					  eth->fq_ring.scratch_head[j], len * MTK_QDMA_PAGE_SIZE,
 					  DMA_FROM_DEVICE);
 
 		if (unlikely(dma_mapping_error(eth->dma_dev, dma_addr)))
@@ -1475,10 +1490,10 @@ static int mtk_init_fq_dma(struct mtk_eth *eth)
 		for (i = 0; i < len; i++) {
 			struct mtk_tx_dma_v2 *txd;
 
-			txd = eth->scratch_ring + (j * MTK_FQ_DMA_LENGTH + i) * soc->tx.desc_size;
+			txd = eth->fq_ring.scratch_ring + (j * MTK_FQ_DMA_LENGTH + i) * soc->tx.desc_size;
 			txd->txd1 = dma_addr + i * MTK_QDMA_PAGE_SIZE;
 			if (j * MTK_FQ_DMA_LENGTH + i < cnt)
-				txd->txd2 = eth->phy_scratch_ring +
+				txd->txd2 = eth->fq_ring.phy_scratch_ring +
 					    (j * MTK_FQ_DMA_LENGTH + i + 1) * soc->tx.desc_size;
 
 			txd->txd3 = TX_DMA_PLEN0(MTK_QDMA_PAGE_SIZE);
@@ -1495,7 +1510,7 @@ static int mtk_init_fq_dma(struct mtk_eth *eth)
 		}
 	}
 
-	mtk_w32(eth, eth->phy_scratch_ring, soc->reg_map->qdma.fq_head);
+	mtk_w32(eth, eth->fq_ring.phy_scratch_ring, soc->reg_map->qdma.fq_head);
 	mtk_w32(eth, phy_ring_tail, soc->reg_map->qdma.fq_tail);
 	mtk_w32(eth, (cnt << 16) | cnt, soc->reg_map->qdma.fq_count);
 	mtk_w32(eth, MTK_QDMA_PAGE_SIZE << 16, soc->reg_map->qdma.fq_blen);
@@ -2783,9 +2798,12 @@ static int mtk_tx_alloc(struct mtk_eth *eth)
 	if (!ring->buf)
 		goto no_tx_mem;
 
-	if (MTK_HAS_CAPS(soc->caps, MTK_SRAM)) {
+	if (MTK_HAS_CAPS(soc->caps, MTK_SRAM) &&
+	    mtk_validate_sram_range(eth, (u64)eth->sram_base + soc->tx.fq_dma_size * sz,
+				    ring_size * sz)) {
 		ring->dma = eth->sram_base + soc->tx.fq_dma_size * sz;
-		ring->phys = eth->phy_scratch_ring + soc->tx.fq_dma_size * (dma_addr_t)sz;
+		ring->phys = eth->fq_ring.phy_scratch_ring + soc->tx.fq_dma_size * (dma_addr_t)sz;
+		ring->in_sram = true;
 	} else {
 		ring->dma = dma_alloc_coherent(eth->dma_dev, ring_size * sz,
 					       &ring->phys, GFP_KERNEL);
@@ -2895,7 +2913,7 @@ static void mtk_tx_clean(struct mtk_eth *eth)
 		kfree(ring->buf);
 		ring->buf = NULL;
 	}
-	if (!MTK_HAS_CAPS(soc->caps, MTK_SRAM) && ring->dma) {
+	if (!ring->in_sram && ring->dma) {
 		dma_free_coherent(eth->dma_dev,
 				  ring->dma_size * soc->tx.desc_size,
 				  ring->dma, ring->phys);
@@ -2914,6 +2932,7 @@ static int mtk_rx_alloc(struct mtk_eth *eth, int ring_no, int rx_flag)
 {
 	const struct mtk_reg_map *reg_map = eth->soc->reg_map;
 	const struct mtk_soc_data *soc = eth->soc;
+	struct mtk_tx_ring *tx_ring = &eth->tx_ring;
 	struct mtk_rx_ring *ring;
 	dma_addr_t offset;
 	int rx_data_len, rx_dma_size, tx_ring_size;
@@ -2955,16 +2974,17 @@ static int mtk_rx_alloc(struct mtk_eth *eth, int ring_no, int rx_flag)
 		ring->page_pool = pp;
 	}
 
+	offset = tx_ring_size * (dma_addr_t)eth->soc->tx.desc_size +
+		 eth->soc->rx.dma_size * (dma_addr_t)eth->soc->rx.desc_size * ring_no;
+
 	if (!MTK_HAS_CAPS(eth->soc->caps, MTK_SRAM) ||
-	    rx_flag != MTK_RX_FLAGS_NORMAL) {
+	    rx_flag != MTK_RX_FLAGS_NORMAL ||
+	    !mtk_validate_sram_range(eth, (u64)tx_ring->dma + offset,
+				     rx_dma_size * eth->soc->rx.desc_size)) {
 		ring->dma = dma_alloc_coherent(eth->dma_dev,
 				rx_dma_size * eth->soc->rx.desc_size,
 				&ring->phys, GFP_KERNEL);
 	} else {
-		struct mtk_tx_ring *tx_ring = &eth->tx_ring;
-
-		offset = tx_ring_size * (dma_addr_t)eth->soc->tx.desc_size +
-			 eth->soc->rx.dma_size * (dma_addr_t)eth->soc->rx.desc_size * ring_no;
 		ring->dma = tx_ring->dma + offset;
 		ring->phys = tx_ring->phys + offset;
 		ring->in_sram = true;
@@ -3807,15 +3827,15 @@ static void mtk_dma_free(struct mtk_eth *eth)
 			netdev_tx_reset_queue(netdev_get_tx_queue(eth->netdev[i], j));
 	}
 
-	if (!MTK_HAS_CAPS(soc->caps, MTK_SRAM) && eth->scratch_ring) {
+	if (!eth->fq_ring.in_sram && eth->fq_ring.scratch_ring) {
 		dma_free_coherent(eth->dma_dev,
 				  soc->tx.fq_dma_size * soc->tx.desc_size,
-				  eth->scratch_ring, eth->phy_scratch_ring);
-		eth->scratch_ring = NULL;
-		eth->phy_scratch_ring = 0;
+				  eth->fq_ring.scratch_ring, eth->fq_ring.phy_scratch_ring);
+		eth->fq_ring.scratch_ring = NULL;
+		eth->fq_ring.phy_scratch_ring = 0;
 	}
 	mtk_tx_clean(eth);
-	mtk_rx_clean(eth, &eth->rx_ring[0], MTK_HAS_CAPS(soc->caps, MTK_SRAM));
+	mtk_rx_clean(eth, &eth->rx_ring[0], eth->rx_ring[0].in_sram);
 	mtk_rx_clean(eth, &eth->rx_ring_qdma, false);
 
 	if (eth->hwlro) {
@@ -3832,8 +3852,8 @@ static void mtk_dma_free(struct mtk_eth *eth)
 	}
 
 	for (i = 0; i < DIV_ROUND_UP(soc->tx.fq_dma_size, MTK_FQ_DMA_LENGTH); i++) {
-		kfree(eth->scratch_head[i]);
-		eth->scratch_head[i] = NULL;
+		kfree(eth->fq_ring.scratch_head[i]);
+		eth->fq_ring.scratch_head[i] = NULL;
 	}
 }
 
@@ -6276,11 +6296,18 @@ static int mtk_probe(struct platform_device *pdev)
 		 * functions to read from or write to SRAM.
 		 */
 		if (mtk_is_netsys_v3_or_greater(eth)) {
+			const struct resource *res;
+
 			eth->sram_base = (void __force *)devm_platform_ioremap_resource(pdev, 1);
 			if (IS_ERR(eth->sram_base))
 				return PTR_ERR(eth->sram_base);
+
+			res = platform_get_resource(pdev, IORESOURCE_MEM, 1);
+			if (res)
+				eth->sram_size = resource_size(res);
 		} else {
 			eth->sram_base = (void __force *)eth->base + MTK_ETH_SRAM_OFFSET;
+			eth->sram_size = SZ_256K;
 		}
 	}
 
@@ -6367,9 +6394,9 @@ static int mtk_probe(struct platform_device *pdev)
 					err = -EINVAL;
 					goto err_destroy_sgmii;
 				}
-				eth->phy_scratch_ring = res_sram->start;
+				eth->fq_ring.phy_scratch_ring = res_sram->start;
 			} else {
-				eth->phy_scratch_ring = res->start + MTK_ETH_SRAM_OFFSET;
+				eth->fq_ring.phy_scratch_ring = res->start + MTK_ETH_SRAM_OFFSET;
 			}
 		}
 	}
diff --git a/drivers/net/ethernet/mediatek/mtk_eth_soc.h b/drivers/net/ethernet/mediatek/mtk_eth_soc.h
index 83b689b..d9934bb 100644
--- a/drivers/net/ethernet/mediatek/mtk_eth_soc.h
+++ b/drivers/net/ethernet/mediatek/mtk_eth_soc.h
@@ -1196,6 +1196,19 @@ struct mtk_rx_ring {
 	struct xdp_rxq_info xdp_q;
 };
 
+/* struct mtk_fq_ring -	This struct holds info describing a FQ ring
+ * @scratch_ring:	Newer SoCs need memory for a second HW managed TX ring
+ * @phy_scratch_ring:	physical address of scratch_ring
+ * @scratch_head:	The scratch memory that scratch_ring points to.
+ * @in_sram:		The scratch ring in the SRAM
+ */
+struct mtk_fq_ring {
+	dma_addr_t	phy_scratch_ring;
+	void		*scratch_ring;
+	void		*scratch_head[MTK_FQ_DMA_HEAD];
+	bool		in_sram;
+};
+
 /* struct mtk_rss_params -	This is the structure holding parameters
 				for the RSS ring
  * @hash_key			The element is used to record the
@@ -1423,7 +1436,7 @@ enum mkt_eth_capabilities {
 #define MT7987_CAPS  (MTK_36BIT_DMA | MTK_GMAC1_SGMII | \
 		      MTK_GMAC2_2P5GPHY_V2 | MTK_GMAC2_SGMII | MTK_GMAC3_SGMII | \
 		      MTK_MUX_GMAC123_TO_GEPHY_SGMII | MTK_MUX_GMAC2_TO_2P5GPHY | \
-		      MTK_MUX_U3_GMAC23_TO_QPHY | MTK_U3_COPHY_V2 | \
+		      MTK_MUX_U3_GMAC23_TO_QPHY | MTK_U3_COPHY_V2 | MTK_SRAM | \
 		      MTK_QDMA | MTK_PDMA_INT | MTK_RSS | MTK_HWLRO | \
 		      MTK_GLO_MEM_ACCESS | MTK_RSTCTRL_PPE1)
 
@@ -1604,9 +1617,6 @@ struct mtk_soc_data {
  * @tx_packets:		Net DIM TX packet counter
  * @tx_bytes:		Net DIM TX byte counter
  * @tx_dim:		Net DIM TX context
- * @scratch_ring:	Newer SoCs need memory for a second HW managed TX ring
- * @phy_scratch_ring:	physical address of scratch_ring
- * @scratch_head:	The scratch memory that scratch_ring points to.
  * @clks:		clock array for all clocks required
  * @mii_bus:		If there is a bus we need to create an instance for it
  * @pending_work:	The workqueue used to reset the dma ring
@@ -1620,6 +1630,7 @@ struct mtk_eth {
 	void __iomem			*base;
 	void __iomem			*esw_base;
 	void				*sram_base;
+	u64				sram_size;
 	spinlock_t			page_lock;
 	spinlock_t			tx_irq_lock;
 	spinlock_t			rx_irq_lock;
@@ -1641,13 +1652,11 @@ struct mtk_eth {
 	struct mtk_tx_ring		tx_ring;
 	struct mtk_rx_ring		rx_ring[MTK_MAX_RX_RING_NUM];
 	struct mtk_rx_ring		rx_ring_qdma;
+	struct mtk_fq_ring		fq_ring;
 	struct napi_struct		tx_napi;
 	struct mtk_napi			rx_napi[MTK_RX_NAPI_NUM];
 	struct mtk_rss_params		rss_params;
 	struct mtk_qdma_params		qdma_params;
-	void				*scratch_ring;
-	dma_addr_t			phy_scratch_ring;
-	void				*scratch_head[MTK_FQ_DMA_HEAD];
 	struct clk			*clks[MTK_CLK_MAX];
 
 	struct mii_bus			*mii_bus;
-- 
2.45.2

