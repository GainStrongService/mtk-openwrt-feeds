From 32fb44819ef920ac453d7b3e40cf06ef1c6d5875 Mon Sep 17 00:00:00 2001
From: Peter Chiu <chui-hao.chiu@mediatek.com>
Date: Thu, 23 May 2024 02:33:47 +0800
Subject: [PATCH 148/277] mtk: mt76: add debugfs for rx drop counters

Signed-off-by: Peter Chiu <chui-hao.chiu@mediatek.com>
---
 agg-rx.c             |  9 +++++
 dma.c                | 36 +++++++++++++++-----
 dma.h                | 14 ++++----
 mac80211.c           | 15 ++++++++-
 mt76.h               | 38 +++++++++++++++++++++
 mt7996/mac.c         | 13 ++++++++
 mt7996/mtk_debugfs.c | 79 ++++++++++++++++++++++++++++++++++++++++++++
 7 files changed, 189 insertions(+), 15 deletions(-)

diff --git a/agg-rx.c b/agg-rx.c
index b48943c4..9875baa8 100644
--- a/agg-rx.c
+++ b/agg-rx.c
@@ -152,6 +152,7 @@ void mt76_rx_aggr_reorder(struct sk_buff *skb, struct sk_buff_head *frames)
 	struct mt76_wcid *wcid = status->wcid;
 	struct ieee80211_sta *sta;
 	struct mt76_rx_tid *tid;
+	struct mt76_phy *phy;
 	bool sn_less;
 	u16 seqno, head, size, idx;
 	u8 tidno = status->qos_ctl & IEEE80211_QOS_CTL_TID_MASK;
@@ -178,6 +179,8 @@ void mt76_rx_aggr_reorder(struct sk_buff *skb, struct sk_buff_head *frames)
 	if (!tid)
 		return;
 
+	phy = mt76_dev_phy(tid->dev, wcid->phy_idx);
+
 	status->flag |= RX_FLAG_DUP_VALIDATED;
 	spin_lock_bh(&tid->lock);
 
@@ -200,6 +203,9 @@ void mt76_rx_aggr_reorder(struct sk_buff *skb, struct sk_buff_head *frames)
 	if (sn_less) {
 		__skb_unlink(skb, frames);
 		dev_kfree_skb(skb);
+		spin_lock_bh(&phy->rx_dbg_stats.lock);
+		phy->rx_dbg_stats.rx_drop[MT_RX_DROP_AGG_SN_LESS]++;
+		spin_unlock_bh(&phy->rx_dbg_stats.lock);
 		goto out;
 	}
 
@@ -226,6 +232,9 @@ void mt76_rx_aggr_reorder(struct sk_buff *skb, struct sk_buff_head *frames)
 	/* Discard if the current slot is already in use */
 	if (tid->reorder_buf[idx]) {
 		dev_kfree_skb(skb);
+		spin_lock_bh(&phy->rx_dbg_stats.lock);
+		phy->rx_dbg_stats.rx_drop[MT_RX_DROP_AGG_DUP]++;
+		spin_unlock_bh(&phy->rx_dbg_stats.lock);
 		goto out;
 	}
 
diff --git a/dma.c b/dma.c
index f9acdd75..9fadc8bb 100644
--- a/dma.c
+++ b/dma.c
@@ -251,13 +251,16 @@ mt76_dma_add_rx_buf(struct mt76_dev *dev, struct mt76_queue *q,
 	if (mt76_queue_is_wed_rx(q)) {
 		if (!rxwi) {
 			rxwi = mt76_get_rxwi(dev);
-			if (!rxwi)
+			if (!rxwi) {
+				q->rx_drop[MT_RX_DROP_DMAD_GET_RXWI_FAIL]++;
 				return -ENOMEM;
+			}
 		}
 
 		rx_token = mt76_rx_token_consume(dev, data, rxwi, buf->addr);
 		if (rx_token < 0) {
 			mt76_put_rxwi(dev, rxwi);
+			q->rx_drop[MT_RX_DROP_DMAD_GET_TOKEN_FAIL]++;
 			return -ENOMEM;
 		}
 
@@ -428,6 +431,7 @@ mt76_dma_get_buf(struct mt76_dev *dev, struct mt76_queue *q, int idx,
 	struct mt76_desc *desc = &q->desc[idx];
 	u32 ctrl, desc_info, buf1;
 	void *buf = e->buf;
+	int reason;
 
 	if (mt76_queue_is_wed_rro_ind(q))
 		goto done;
@@ -443,7 +447,9 @@ mt76_dma_get_buf(struct mt76_dev *dev, struct mt76_queue *q, int idx,
 		*info = desc_info;
 
 	buf1 = le32_to_cpu(desc->buf1);
-	mt76_dma_should_drop_buf(drop, ctrl, buf1, desc_info);
+	reason = mt76_dma_should_drop_buf(drop, ctrl, buf1, desc_info);
+	if (drop && *drop && reason >= 0)
+		q->rx_drop[reason]++;
 
 	if (mt76_queue_is_wed_rx(q)) {
 		u32 id, find = 0;
@@ -467,13 +473,17 @@ mt76_dma_get_buf(struct mt76_dev *dev, struct mt76_queue *q, int idx,
 			}
 
 			spin_unlock_bh(&dev->rx_token_lock);
-			if (!find)
+			if (!find) {
+				q->rx_drop[MT_RX_DROP_DMAD_ADDR_NOT_FOUND]++;
 				return NULL;
+			}
 		}
 
 		r = mt76_rx_token_release(dev, token);
-		if (!r)
+		if (!r) {
+			q->rx_drop[MT_RX_DROP_DMAD_TOKEN_NOT_FOUND]++;
 			return NULL;
+		}
 
 		dma_sync_single_for_cpu(dev->dma_dev, r->dma_addr,
 				SKB_WITH_OVERHEAD(q->buf_size),
@@ -493,6 +503,7 @@ mt76_dma_get_buf(struct mt76_dev *dev, struct mt76_queue *q, int idx,
 
 			buf = mt76_get_page_pool_buf(q, &offset, q->buf_size);
 			if (!buf) {
+				q->rx_drop[MT_RX_DROP_DMAD_NOMEM]++;
 				mt76_put_rxwi(dev, r);
 				return NULL;
 			}
@@ -514,8 +525,11 @@ mt76_dma_get_buf(struct mt76_dev *dev, struct mt76_queue *q, int idx,
 			}
 		}
 
-		if (drop)
+		if (drop) {
 			*drop |= !!(buf1 & MT_DMA_CTL_WO_DROP);
+			if (buf1 & MT_DMA_CTL_WO_DROP)
+				q->rx_drop[MT_RX_DROP_DMAD_WO_FRAG]++;
+		}
 	} else {
 		dma_sync_single_for_cpu(dev->dma_dev, e->dma_addr[0],
 				SKB_WITH_OVERHEAD(q->buf_size),
@@ -885,16 +899,19 @@ mt76_add_fragment(struct mt76_dev *dev, struct mt76_queue *q, void *data,
 		skb_add_rx_frag(skb, nr_frags, page, offset, len, q->buf_size);
 	} else {
 		mt76_put_page_pool_buf(data, allow_direct);
+		q->rx_drop[MT_RX_DROP_FRAG]++;
 	}
 
 	if (more)
 		return;
 
 	q->rx_head = NULL;
-	if (nr_frags < ARRAY_SIZE(shinfo->frags))
+	if (nr_frags < ARRAY_SIZE(shinfo->frags)) {
 		dev->drv->rx_skb(dev, q - dev->q_rx, skb, &info);
-	else
+	} else {
+		q->rx_drop[MT_RX_DROP_FRAG]++;
 		dev_kfree_skb(skb);
+	}
 }
 
 static int
@@ -939,6 +956,7 @@ mt76_dma_rx_process(struct mt76_dev *dev, struct mt76_queue *q, int budget)
 			data_len = SKB_WITH_OVERHEAD(q->buf_size);
 
 		if (data_len < len + q->buf_offset) {
+			q->rx_drop[MT_RX_DROP_FRAG]++;
 			dev_kfree_skb(q->rx_head);
 			q->rx_head = NULL;
 			goto free_frag;
@@ -955,8 +973,10 @@ mt76_dma_rx_process(struct mt76_dev *dev, struct mt76_queue *q, int budget)
 			goto free_frag;
 
 		skb = napi_build_skb(data, q->buf_size);
-		if (!skb)
+		if (!skb) {
+			q->rx_drop[MT_RX_DROP_BUILD_SKB_FAIL]++;
 			goto free_frag;
+		}
 
 		skb_reserve(skb, q->buf_offset);
 		skb_mark_for_recycle(skb);
diff --git a/dma.h b/dma.h
index 3a8c2e55..718122d5 100644
--- a/dma.h
+++ b/dma.h
@@ -93,27 +93,29 @@ mt76_dma_reset_tx_queue(struct mt76_dev *dev, struct mt76_queue *q)
 		mt76_wed_dma_setup(dev, q, true);
 }
 
-static inline void
+static inline int
 mt76_dma_should_drop_buf(bool *drop, u32 ctrl, u32 buf1, u32 info)
 {
 	if (!drop)
-		return;
+		return -1;
 
 	*drop = !!(ctrl & (MT_DMA_CTL_TO_HOST_A | MT_DMA_CTL_DROP));
 	if (!(ctrl & MT_DMA_CTL_VER_MASK))
-		return;
+		return MT_RX_DROP_DMAD_WO_DROP;
 
 	switch (FIELD_GET(MT_DMA_WED_IND_REASON, buf1)) {
 	case MT_DMA_WED_IND_REASON_REPEAT:
 		*drop = true;
-		break;
+		return MT_RX_DROP_DMAD_RRO_REPEAT;
 	case MT_DMA_WED_IND_REASON_OLDPKT:
 		*drop = !(info & MT_DMA_INFO_DMA_FRAG);
-		break;
+		return MT_RX_DROP_DMAD_RRO_OLDPKT;
 	default:
 		*drop = !!(ctrl & MT_DMA_CTL_PN_CHK_FAIL);
-		break;
+		return MT_RX_DROP_DMAD_RRO_PN_CHK_FAIL;
 	}
+
+	return -1;
 }
 
 #endif
diff --git a/mac80211.c b/mac80211.c
index 17270227..4eb0ccc6 100644
--- a/mac80211.c
+++ b/mac80211.c
@@ -418,6 +418,7 @@ mt76_phy_init(struct mt76_phy *phy, struct ieee80211_hw *hw)
 	INIT_LIST_HEAD(&phy->tx_list);
 	spin_lock_init(&phy->tx_lock);
 	spin_lock_init(&phy->tx_dbg_stats.lock);
+	spin_lock_init(&phy->rx_dbg_stats.lock);
 
 	SET_IEEE80211_DEV(hw, dev->dev);
 	SET_IEEE80211_PERM_ADDR(hw, phy->macaddr);
@@ -796,6 +797,9 @@ static void mt76_rx_release_amsdu(struct mt76_phy *phy, enum mt76_rxq_id q)
 		}
 
 		if (ether_addr_equal(skb->data + offset, rfc1042_header)) {
+			spin_lock_bh(&phy->rx_dbg_stats.lock);
+			phy->rx_dbg_stats.rx_drop[MT_RX_DROP_RFC_PKT]++;
+			spin_unlock_bh(&phy->rx_dbg_stats.lock);
 			dev_kfree_skb(skb);
 			return;
 		}
@@ -833,6 +837,9 @@ void mt76_rx(struct mt76_dev *dev, enum mt76_rxq_id q, struct sk_buff *skb)
 
 	if (!test_bit(MT76_STATE_RUNNING, &phy->state)) {
 		dev_kfree_skb(skb);
+		spin_lock_bh(&phy->rx_dbg_stats.lock);
+		phy->rx_dbg_stats.rx_drop[MT_RX_DROP_STATE_ERR]++;
+		spin_unlock_bh(&phy->rx_dbg_stats.lock);
 		return;
 	}
 
@@ -1121,6 +1128,7 @@ mt76_rx_convert(struct mt76_dev *dev, struct sk_buff *skb,
 {
 	struct ieee80211_rx_status *status = IEEE80211_SKB_RXCB(skb);
 	struct ieee80211_hdr *hdr = mt76_skb_get_hdr(skb);
+	struct mt76_phy *phy;
 	struct mt76_rx_status mstat;
 
 	mstat = *((struct mt76_rx_status *)skb->cb);
@@ -1167,7 +1175,12 @@ mt76_rx_convert(struct mt76_dev *dev, struct sk_buff *skb,
 	}
 
 	*sta = wcid_to_sta(mstat.wcid);
-	*hw = mt76_main_hw(dev->phys[mstat.phy_idx]);
+	*hw = mt76_phy_hw(dev, mstat.phy_idx);
+
+	phy = mt76_dev_phy(dev, mstat.phy_idx);
+	spin_lock_bh(&phy->rx_dbg_stats.lock);
+	phy->rx_dbg_stats.rx_to_mac80211++;
+	spin_unlock_bh(&phy->rx_dbg_stats.lock);
 }
 
 static void
diff --git a/mt76.h b/mt76.h
index fb197eea..3ffb9b11 100644
--- a/mt76.h
+++ b/mt76.h
@@ -188,6 +188,34 @@ enum mt76_dfs_state {
 	MT_DFS_STATE_ACTIVE,
 };
 
+enum {
+	/* Per dev counters*/
+	MT_RX_DROP_DMAD_RRO_REPEAT,
+	MT_RX_DROP_DMAD_RRO_OLDPKT,
+	MT_RX_DROP_DMAD_RRO_PN_CHK_FAIL,
+	MT_RX_DROP_DMAD_WO_FRAG,
+	MT_RX_DROP_DMAD_WO_DROP,
+	MT_RX_DROP_DMAD_ADDR_NOT_FOUND,
+	MT_RX_DROP_DMAD_TOKEN_NOT_FOUND,
+	MT_RX_DROP_DMAD_GET_TOKEN_FAIL,
+	MT_RX_DROP_DMAD_GET_RXWI_FAIL,
+	MT_RX_DROP_DMAD_NOMEM,
+	MT_RX_DROP_DMAD_DMA_MAPPING_FAIL,
+	MT_RX_DROP_FRAG,
+	MT_RX_DROP_BUILD_SKB_FAIL,
+
+	MT_RX_DROP_PER_Q_MAX,
+
+	/* Per phy counters */
+	MT_RX_DROP_RXD_ERR = 0,
+	MT_RX_DROP_STATE_ERR,
+	MT_RX_DROP_RFC_PKT,
+	MT_RX_DROP_AGG_SN_LESS,
+	MT_RX_DROP_AGG_DUP,
+
+	MT_RX_DROP_PER_PHY_MAX,
+};
+
 struct mt76_queue_buf {
 	dma_addr_t addr;
 	u16 len:15,
@@ -256,6 +284,7 @@ struct mt76_queue {
 	dma_addr_t desc_dma;
 	struct sk_buff *rx_head;
 	struct page_pool *page_pool;
+	u32 rx_drop[MT_RX_DROP_PER_Q_MAX];
 };
 
 struct mt76_mcu_ops {
@@ -874,6 +903,14 @@ struct mt76_tx_debug {
 	spinlock_t lock;
 };
 
+struct mt76_rx_debug {
+	u32 rx_from_hw;
+	u32 rx_to_mac80211;
+
+	u32 rx_drop[MT_RX_DROP_PER_PHY_MAX];
+	spinlock_t lock;
+};
+
 struct mt76_phy {
 	struct ieee80211_hw *hw;
 	struct ieee80211_hw *ori_hw;
@@ -932,6 +969,7 @@ struct mt76_phy {
 		u8 pin;
 	} leds;
 	struct mt76_tx_debug tx_dbg_stats;
+	struct mt76_rx_debug rx_dbg_stats;
 };
 
 struct mt76_dev {
diff --git a/mt7996/mac.c b/mt7996/mac.c
index a78ebefc..ee505a55 100644
--- a/mt7996/mac.c
+++ b/mt7996/mac.c
@@ -1422,9 +1422,11 @@ void mt7996_queue_rx_skb(struct mt76_dev *mdev, enum mt76_rxq_id q,
 			 struct sk_buff *skb, u32 *info)
 {
 	struct mt7996_dev *dev = container_of(mdev, struct mt7996_dev, mt76);
+	struct mt76_phy *phy;
 	__le32 *rxd = (__le32 *)skb->data;
 	__le32 *end = (__le32 *)&skb->data[skb->len];
 	enum rx_pkt_type type;
+	u8 band_idx;
 
 	type = le32_get_bits(rxd[0], MT_RXD0_PKT_TYPE);
 	if (type != PKT_TYPE_NORMAL) {
@@ -1459,12 +1461,23 @@ void mt7996_queue_rx_skb(struct mt76_dev *mdev, enum mt76_rxq_id q,
 		dev_kfree_skb(skb);
 		break;
 	case PKT_TYPE_NORMAL:
+		band_idx = le32_get_bits(rxd[1], MT_RXD1_NORMAL_BAND_IDX);
+		phy = mt76_dev_phy(mdev, band_idx);
+		spin_lock_bh(&phy->rx_dbg_stats.lock);
+		phy->rx_dbg_stats.rx_from_hw++;
+		spin_unlock_bh(&phy->rx_dbg_stats.lock);
+
 		if (!mt7996_mac_fill_rx(dev, q, skb, info)) {
 			mt76_rx(&dev->mt76, q, skb);
 			return;
 		}
 		fallthrough;
 	default:
+		band_idx = le32_get_bits(rxd[1], MT_RXD1_NORMAL_BAND_IDX);
+		phy = mt76_dev_phy(mdev, band_idx);
+		spin_lock_bh(&phy->rx_dbg_stats.lock);
+		phy->rx_dbg_stats.rx_drop[MT_RX_DROP_RXD_ERR]++;
+		spin_unlock_bh(&phy->rx_dbg_stats.lock);
 		dev_kfree_skb(skb);
 		break;
 	}
diff --git a/mt7996/mtk_debugfs.c b/mt7996/mtk_debugfs.c
index 1a468705..2de3beac 100644
--- a/mt7996/mtk_debugfs.c
+++ b/mt7996/mtk_debugfs.c
@@ -4239,6 +4239,84 @@ out:
 }
 DEFINE_SHOW_ATTRIBUTE(mt7996_tx_drop);
 
+static int
+mt7996_rx_drop_show(struct seq_file *s, void *data)
+{
+	struct mt7996_dev *dev = s->private;
+	struct mt76_dev *mdev = &dev->mt76;
+	struct mt76_rx_debug *stats[__MT_MAX_BAND];
+	struct mt76_queue *q[2];
+	int i = 0;
+
+	q[0] = &mdev->q_rx[MT_RXQ_MAIN];
+	q[1] = is_mt7996(mdev) ? &mdev->q_rx[MT_RXQ_BAND2] :
+				 &mdev->q_rx[MT_RXQ_BAND1];
+
+	seq_printf(s, "\t\t\t\t   ");
+	for (i = 0; i < 2; i++) {
+		seq_printf(s, "        RXQ%d", q[i]->hw_idx);
+	}
+	seq_printf(s, "\n");
+
+#define __pr(t) seq_printf(s, "Drop due to %-22s%12d%12d\n", #t, \
+			   q[0]->rx_drop[MT_RX_DROP_##t],	\
+			   q[1]->rx_drop[MT_RX_DROP_##t]);
+	__pr(DMAD_RRO_REPEAT);
+	__pr(DMAD_RRO_OLDPKT);
+	__pr(DMAD_RRO_PN_CHK_FAIL);
+	__pr(DMAD_WO_FRAG);
+	__pr(DMAD_WO_DROP);
+	__pr(DMAD_ADDR_NOT_FOUND);
+	__pr(DMAD_TOKEN_NOT_FOUND);
+	__pr(DMAD_GET_TOKEN_FAIL);
+	__pr(DMAD_GET_RXWI_FAIL);
+	__pr(DMAD_NOMEM);
+	__pr(DMAD_DMA_MAPPING_FAIL);
+	__pr(FRAG);
+	__pr(BUILD_SKB_FAIL);
+#undef __pr
+
+	seq_printf(s, "\n\t\t\t\t   ");
+	for (i = 0; i < __MT_MAX_BAND; i++) {
+		seq_printf(s, "       Band%d", i);
+		if (mdev->phys[i]) {
+			stats[i] = &mdev->phys[i]->rx_dbg_stats;
+		} else {
+			stats[i] = kzalloc(sizeof(struct mt76_rx_debug),
+					       GFP_KERNEL);
+			if (!stats[i])
+				goto out;
+		}
+	}
+	seq_printf(s, "\n");
+	seq_printf(s, "%-35s%12d%12d%12d\n", "Receive from hw",
+		       stats[MT_BAND0]->rx_from_hw,
+		       stats[MT_BAND1]->rx_from_hw,
+		       stats[MT_BAND2]->rx_from_hw);
+	seq_printf(s, "%-35s%12d%12d%12d\n\n", "Send to mac80211",
+		       stats[MT_BAND0]->rx_to_mac80211,
+		       stats[MT_BAND1]->rx_to_mac80211,
+		       stats[MT_BAND2]->rx_to_mac80211);
+#define __pr(t) seq_printf(s, "Drop due to %-22s%12d%12d%12d\n", #t, \
+			   stats[MT_BAND0]->rx_drop[MT_RX_DROP_##t],	\
+			   stats[MT_BAND1]->rx_drop[MT_RX_DROP_##t],	\
+			   stats[MT_BAND2]->rx_drop[MT_RX_DROP_##t])
+	__pr(RXD_ERR);
+	__pr(STATE_ERR);
+	__pr(RFC_PKT);
+	__pr(AGG_SN_LESS);
+	__pr(AGG_DUP);
+#undef __pr
+
+out:
+	for (i = 0; i < __MT_MAX_BAND; i++) {
+		if (!mdev->phys[i] && stats[i])
+			kfree(stats[i]);
+	}
+
+	return 0;
+}
+DEFINE_SHOW_ATTRIBUTE(mt7996_rx_drop);
 /* DRR */
 static int
 mt7996_drr_info(struct seq_file *s, void *data)
@@ -4367,6 +4445,7 @@ void mt7996_mtk_init_dev_debugfs(struct mt7996_dev *dev, struct dentry *dir)
 
 	/* Drop counters */
 	debugfs_create_file("tx_drop_stats", 0400, dir, dev, &mt7996_tx_drop_fops);
+	debugfs_create_file("rx_drop_stats", 0400, dir, dev, &mt7996_rx_drop_fops);
 }
 
 #endif
-- 
2.45.2

