From d72e87d8eacba243a196b8ed44b5e0bf11a33f0c Mon Sep 17 00:00:00 2001
From: Rex Lu <rex.lu@mediatek.com>
Date: Tue, 17 Dec 2024 14:35:26 +0800
Subject: [PATCH] mtk: mt76: mt7990: add mt7987 wed hw path support

1. mt7996_mmio_wed_init support to check wed hw version for init setup
2. add rro3.1 hw init function
3. add wed rx data ring for wed3.1 to host
4. change has_rro to hwrro_mode for support rro3.1 and rro 3.0. and move from mt7996_dev to mt76.

CR-Id: WCNCR00259516
Signed-off-by: Rex Lu <rex.lu@mediatek.com>
Change-Id: I84b03bd27340ada7b630b491c4d9778935315f47
---
 dma.c                |  15 +++--
 mt76.h               |  27 ++++++++
 mt7996/dma.c         | 112 ++++++++++++++++++++++++----------
 mt7996/init.c        | 142 ++++++++++++++++++++++++-------------------
 mt7996/mac.c         |   7 ++-
 mt7996/mcu.c         |   4 +-
 mt7996/mmio.c        | 124 ++++++++++++++++++++++++++-----------
 mt7996/mt7996.h      |  12 +++-
 mt7996/mtk_debug.h   |   3 +-
 mt7996/mtk_debugfs.c |  11 +++-
 mt7996/pci.c         |  18 +++++-
 mt7996/regs.h        |   9 +++
 wed.c                |   5 ++
 13 files changed, 344 insertions(+), 145 deletions(-)

diff --git a/dma.c b/dma.c
index 9004aa1b5..e4cac4044 100644
--- a/dma.c
+++ b/dma.c
@@ -203,7 +203,7 @@ void __mt76_dma_queue_reset(struct mt76_dev *dev, struct mt76_queue *q,
 	if (!q || !q->ndesc)
 		return;
 
-	if (!mt76_queue_is_wed_rro_ind(q)) {
+	if (!mt76_queue_is_wed_rro_ind(q) && !mt76_queue_is_wed_rro_rxdmad_c(q)) {
 		int i;
 
 		/* clear descriptors */
@@ -240,6 +240,9 @@ mt76_dma_add_rx_buf(struct mt76_dev *dev, struct mt76_queue *q,
 		rro_desc = (struct mt76_wed_rro_desc *)q->desc;
 		data = &rro_desc[q->head];
 		goto done;
+	} else if (mt76_queue_is_wed_rro_rxdmad_c(q)) {
+		data = &q->desc[q->head];
+		goto done;
 	}
 
 	desc = &q->desc[q->head];
@@ -544,7 +547,8 @@ mt76_dma_get_buf(struct mt76_dev *dev, struct mt76_queue *q, int idx,
 			}
 		}
 #endif
-		if (drop) {
+		/* only wed v2 rx path handle by wo */
+		if (drop && dev->mmio.wed.version == MTK_WED_HW_V2) {
 			*drop |= !!(buf1 & MT_DMA_CTL_WO_DROP);
 			if (buf1 & MT_DMA_CTL_WO_DROP)
 				q->rx_drop[MT_RX_DROP_DMAD_WO_FRAG]++;
@@ -571,7 +575,8 @@ mt76_dma_dequeue(struct mt76_dev *dev, struct mt76_queue *q, bool flush,
 		return NULL;
 
 	if (mt76_queue_is_wed_rro_data(q) ||
-	    mt76_queue_is_wed_rro_msdu_pg(q))
+	    mt76_queue_is_wed_rro_msdu_pg(q) ||
+	    mt76_queue_is_wed_rro_rxdmad_c(q))
 		goto done;
 
 	if (mt76_queue_is_wed_rro_ind(q)) {
@@ -764,7 +769,7 @@ int mt76_dma_rx_fill_buf(struct mt76_dev *dev, struct mt76_queue *q,
 		int offset;
 		void *buf = NULL;
 
-		if (mt76_queue_is_wed_rro_ind(q))
+		if (mt76_queue_is_wed_rro_ind(q) || mt76_queue_is_wed_rro_rxdmad_c(q))
 			goto done;
 
 		buf = mt76_get_page_pool_buf(q, &offset, q->buf_size);
@@ -904,7 +909,7 @@ mt76_dma_rx_reset(struct mt76_dev *dev, enum mt76_rxq_id qid)
 	if (!q->ndesc)
 		return;
 
-	if (!mt76_queue_is_wed_rro_ind(q)) {
+	if (!mt76_queue_is_wed_rro_ind(q) && !mt76_queue_is_wed_rro_rxdmad_c(q)) {
 		int i;
 
 		for (i = 0; i < q->ndesc; i++)
diff --git a/mt76.h b/mt76.h
index df50450a7..aa49f9fb0 100644
--- a/mt76.h
+++ b/mt76.h
@@ -74,6 +74,7 @@
 #define MT_WED_RRO_Q_DATA(_n)	__MT_WED_RRO_Q(MT76_WED_RRO_Q_DATA, _n)
 #define MT_WED_RRO_Q_MSDU_PG(_n)	__MT_WED_RRO_Q(MT76_WED_RRO_Q_MSDU_PG, _n)
 #define MT_WED_RRO_Q_IND	__MT_WED_RRO_Q(MT76_WED_RRO_Q_IND, 0)
+#define MT_WED_RRO_Q_RXDMAD_C	__MT_WED_RRO_Q(MT76_WED_RRO_Q_RXDMAD_C, 0)
 
 #define AMPDU_ADDBA_SUCC_SHFT IEEE80211_NUM_TIDS
 
@@ -102,6 +103,13 @@ enum mt76_wed_type {
 	MT76_WED_RRO_Q_DATA,
 	MT76_WED_RRO_Q_MSDU_PG,
 	MT76_WED_RRO_Q_IND,
+	MT76_WED_RRO_Q_RXDMAD_C,
+};
+
+enum mt76_hwrro_mode {
+	MT76_HWRRO_DISABLE,
+	MT76_HWRRO_V3,
+	MT76_HWRRO_V3_1,
 };
 
 struct mt76_bus_ops {
@@ -160,6 +168,8 @@ enum mt76_rxq_id {
 	MT_RXQ_TXFREE_BAND1,
 	MT_RXQ_TXFREE_BAND2,
 	MT_RXQ_RRO_IND,
+	MT_RXQ_RRO_RXDMAD_C,
+	MT_RXQ_WED_RX_DATA,
 	__MT_RXQ_MAX
 };
 
@@ -1107,6 +1117,7 @@ struct mt76_dev {
 	struct mt76_queue q_rx[__MT_RXQ_MAX];
 	const struct mt76_queue_ops *queue_ops;
 	int tx_dma_idx[4];
+	enum mt76_hwrro_mode hwrro_mode;
 
 	struct mt76_worker tx_worker;
 	struct napi_struct tx_napi;
@@ -1417,6 +1428,11 @@ int mt76_wed_net_setup_tc(struct ieee80211_hw *hw, struct ieee80211_vif *vif,
 u32 mt76_wed_init_rx_buf(struct mtk_wed_device *wed, int size);
 int mt76_wed_offload_enable(struct mtk_wed_device *wed);
 int mt76_wed_dma_setup(struct mt76_dev *dev, struct mt76_queue *q, bool reset);
+
+static inline bool mt76_wed_check_rx_cap(struct mtk_wed_device *wed)
+{
+	return mtk_wed_device_active(wed) && mtk_wed_get_rx_capa(wed);
+}
 #else
 static inline u32 mt76_wed_init_rx_buf(struct mtk_wed_device *wed, int size)
 {
@@ -1433,6 +1449,11 @@ static inline int mt76_wed_dma_setup(struct mt76_dev *dev, struct mt76_queue *q,
 {
 	return 0;
 }
+
+static inline bool mt76_wed_check_rx_cap(struct mtk_wed_device *wed)
+{
+	return false;
+}
 #endif /* CONFIG_NET_MEDIATEK_SOC_WED */
 
 #define mt76xx_chip(dev) mt76_chip(&((dev)->mt76))
@@ -2034,6 +2055,12 @@ static inline bool mt76_queue_is_wed_rro_ind(struct mt76_queue *q)
 	       FIELD_GET(MT_QFLAG_WED_TYPE, q->flags) == MT76_WED_RRO_Q_IND;
 }
 
+static inline bool mt76_queue_is_wed_rro_rxdmad_c(struct mt76_queue *q)
+{
+	return mt76_queue_is_wed_rro(q) &&
+	       FIELD_GET(MT_QFLAG_WED_TYPE, q->flags) == MT76_WED_RRO_Q_RXDMAD_C;
+}
+
 static inline bool mt76_queue_is_wed_rro_data(struct mt76_queue *q)
 {
 	return mt76_queue_is_wed_rro(q) &&
diff --git a/mt7996/dma.c b/mt7996/dma.c
index 61e5da758..26ebf9815 100644
--- a/mt7996/dma.c
+++ b/mt7996/dma.c
@@ -50,6 +50,8 @@ static int mt7996_poll_tx(struct napi_struct *napi, int budget)
 
 static void mt7996_dma_config(struct mt7996_dev *dev)
 {
+	struct mtk_wed_device *wed = &dev->mt76.mmio.wed;
+
 #define Q_CONFIG(q, wfdma, int, id) do {		\
 	if (wfdma)					\
 		dev->q_wfdma_mask |= (1 << (q));	\
@@ -90,12 +92,13 @@ static void mt7996_dma_config(struct mt7996_dev *dev)
 		break;
 	}
 
-	if (dev->has_rro) {
+	if (mt7996_has_hwrro(dev)) {
 		/* band0 */
 		RXQ_CONFIG(MT_RXQ_RRO_BAND0, WFDMA0, MT_INT_RX_DONE_RRO_BAND0,
 			   MT7996_RXQ_RRO_BAND0);
-		RXQ_CONFIG(MT_RXQ_MSDU_PAGE_BAND0, WFDMA0, MT_INT_RX_DONE_MSDU_PG_BAND0,
-			   MT7996_RXQ_MSDU_PG_BAND0);
+		if (dev->mt76.hwrro_mode == MT76_HWRRO_V3)
+			RXQ_CONFIG(MT_RXQ_MSDU_PAGE_BAND0, WFDMA0, MT_INT_RX_DONE_MSDU_PG_BAND0,
+				   MT7996_RXQ_MSDU_PG_BAND0);
 		if (is_mt7996(&dev->mt76)) {
 			RXQ_CONFIG(MT_RXQ_TXFREE_BAND0, WFDMA0, MT_INT_RX_TXFREE_MAIN,
 				   MT7996_RXQ_TXFREE0);
@@ -114,8 +117,23 @@ static void mt7996_dma_config(struct mt7996_dev *dev)
 				   MT7996_RXQ_RRO_BAND1);
 		}
 
-		RXQ_CONFIG(MT_RXQ_RRO_IND, WFDMA0, MT_INT_RX_DONE_RRO_IND,
-			   MT7996_RXQ_RRO_IND);
+		if (dev->mt76.hwrro_mode == MT76_HWRRO_V3)
+			RXQ_CONFIG(MT_RXQ_RRO_IND, WFDMA0, MT_INT_RX_DONE_RRO_IND,
+				   MT7996_RXQ_RRO_IND);
+		else
+			RXQ_CONFIG(MT_RXQ_RRO_RXDMAD_C, WFDMA0, MT_INT_RX_DONE_RRO_RXDMAD_C,
+				   MT7996_RXQ_RRO_RXDMAD_C);
+	}
+
+	/* wed rx queue */
+	if (mtk_wed_device_active(wed) && wed->version == MTK_WED_HW_V3_1) {
+		/* For mt7996, there is not enough Rx ring and thus we use Tx ring instead */
+		if (is_mt7996(&dev->mt76))
+			RXQ_CONFIG(MT_RXQ_WED_RX_DATA, WFDMA0, MT_INT_RX_DONE_WED_RX_DATA_MT7996,
+				   MT7996_TXQ_WED_RX);
+		else
+			RXQ_CONFIG(MT_RXQ_WED_RX_DATA, WFDMA0, MT_INT_RX_DONE_WED_RX_DATA,
+				   MT7996_RXQ_WED_RX_DATA);
 	}
 
 	/* data tx queue */
@@ -198,11 +216,11 @@ static void __mt7996_dma_prefetch(struct mt7996_dev *dev, u32 ofs)
 
 	/* Rx TxFreeDone From MAC Rings */
 	val = is_mt7996(&dev->mt76) ? 4 : 8;
-	if (is_mt7990(&dev->mt76) || (is_mt7996(&dev->mt76) && dev->has_rro))
+	if (is_mt7990(&dev->mt76) || (is_mt7996(&dev->mt76) && mt7996_has_hwrro(dev)))
 		mt76_wr(dev, MT_RXQ_EXT_CTRL(MT_RXQ_TXFREE_BAND0) + ofs, PREFETCH(val));
 	if (is_mt7990(&dev->mt76) && dev->hif2)
 		mt76_wr(dev, MT_RXQ_EXT_CTRL(MT_RXQ_TXFREE_BAND1) + ofs, PREFETCH(val));
-	else if (is_mt7996(&dev->mt76) && dev->has_rro)
+	else if (is_mt7996(&dev->mt76) && mt7996_has_hwrro(dev))
 		mt76_wr(dev, MT_RXQ_EXT_CTRL(MT_RXQ_TXFREE_BAND2) + ofs, PREFETCH(val));
 
 	/* Rx Data Rings */
@@ -211,7 +229,7 @@ static void __mt7996_dma_prefetch(struct mt7996_dev *dev, u32 ofs)
 	mt76_wr(dev, MT_RXQ_EXT_CTRL(queue) + ofs, PREFETCH(0x10));
 
 	/* Rx RRO Rings */
-	if (dev->has_rro) {
+	if (mt7996_has_hwrro(dev)) {
 		mt76_wr(dev, MT_RXQ_EXT_CTRL(MT_RXQ_RRO_BAND0) + ofs, PREFETCH(0x10));
 		queue = is_mt7996(&dev->mt76) ? MT_RXQ_RRO_BAND2 : MT_RXQ_RRO_BAND1;
 		mt76_wr(dev, MT_RXQ_EXT_CTRL(queue) + ofs, PREFETCH(0x10));
@@ -292,7 +310,7 @@ void mt7996_dma_start(struct mt7996_dev *dev, bool reset, bool wed_reset)
 
 	/* enable WFDMA Tx/Rx */
 	if (!reset) {
-		if (mtk_wed_device_active(wed) && mtk_wed_get_rx_capa(wed))
+		if (mt76_wed_check_rx_cap(wed))
 			mt76_set(dev, MT_WFDMA0_GLO_CFG,
 				 MT_WFDMA0_GLO_CFG_TX_DMA_EN |
 				 MT_WFDMA0_GLO_CFG_OMIT_TX_INFO |
@@ -326,13 +344,16 @@ void mt7996_dma_start(struct mt7996_dev *dev, bool reset, bool wed_reset)
 			irq_mask |= MT_INT_RX_TXFREE_BAND1_EXT;
 	}
 
+	irq_mask |=  MT_INT_TX_DONE_BAND0 | MT_INT_TX_DONE_BAND1;
 	if (mt7996_band_valid(dev, MT_BAND2))
 		irq_mask |= MT_INT_BAND2_RX_DONE | MT_INT_TX_RX_DONE_EXT;
 
 	if (mtk_wed_device_active(wed) && wed_reset) {
 		u32 wed_irq_mask = irq_mask;
 
-		wed_irq_mask |= MT_INT_TX_DONE_BAND0 | MT_INT_TX_DONE_BAND1;
+		if (wed->version == MTK_WED_HW_V3_1)
+			wed_irq_mask |= MT_INT_RX(MT_RXQ_WED_RX_DATA);
+
 		mt76_wr(dev, MT_INT_MASK_CSR, wed_irq_mask);
 		mtk_wed_device_start(wed, wed_irq_mask);
 	}
@@ -479,7 +500,7 @@ static void mt7996_dma_enable(struct mt7996_dev *dev, bool reset)
 		 * so, redirect pcie0 rx ring3 interrupt to pcie1
 		 */
 		if (mtk_wed_device_active(&dev->mt76.mmio.wed) &&
-		    dev->has_rro) {
+		    mt7996_has_hwrro(dev)) {
 			u32 intr = is_mt7996(&dev->mt76) ?
 				   MT_WFDMA0_RX_INT_SEL_RING6 :
 				   MT_WFDMA0_RX_INT_SEL_RING9 |
@@ -501,10 +522,24 @@ int mt7996_dma_rro_init(struct mt7996_dev *dev)
 	u32 irq_mask;
 	int ret;
 
+	if (dev->mt76.hwrro_mode == MT76_HWRRO_V3_1) {
+		/* rxdmad_c */
+		mdev->q_rx[MT_RXQ_RRO_RXDMAD_C].flags = MT_WED_RRO_Q_RXDMAD_C;
+		if (mt76_wed_check_rx_cap(&mdev->mmio.wed))
+			mdev->q_rx[MT_RXQ_RRO_RXDMAD_C].wed = &mdev->mmio.wed;
+		ret = mt76_queue_alloc(dev, &mdev->q_rx[MT_RXQ_RRO_RXDMAD_C],
+				       MT_RXQ_ID(MT_RXQ_RRO_RXDMAD_C),
+				       MT7996_RX_RING_SIZE,
+				       MT7996_RX_BUF_SIZE,
+				       MT_RXQ_RRO_AP_RING_BASE);
+		if (ret)
+			return ret;
+
+		goto start_hw_rro;
+	}
 	/* ind cmd */
 	mdev->q_rx[MT_RXQ_RRO_IND].flags = MT_WED_RRO_Q_IND;
-	if (mtk_wed_device_active(&mdev->mmio.wed) &&
-	    mtk_wed_get_rx_capa(&mdev->mmio.wed))
+	if (mt76_wed_check_rx_cap(&mdev->mmio.wed))
 		mdev->q_rx[MT_RXQ_RRO_IND].wed = &mdev->mmio.wed;
 	ret = mt76_queue_alloc(dev, &mdev->q_rx[MT_RXQ_RRO_IND],
 			       MT_RXQ_ID(MT_RXQ_RRO_IND),
@@ -516,8 +551,7 @@ int mt7996_dma_rro_init(struct mt7996_dev *dev)
 	/* rx msdu page queue for band0 */
 	mdev->q_rx[MT_RXQ_MSDU_PAGE_BAND0].flags =
 		MT_WED_RRO_Q_MSDU_PG(0) | MT_QFLAG_WED_RRO_EN;
-	if (mtk_wed_device_active(&mdev->mmio.wed) &&
-	    mtk_wed_get_rx_capa(&mdev->mmio.wed))
+	if (mt76_wed_check_rx_cap(&mdev->mmio.wed))
 		mdev->q_rx[MT_RXQ_MSDU_PAGE_BAND0].wed = &mdev->mmio.wed;
 	ret = mt76_queue_alloc(dev, &mdev->q_rx[MT_RXQ_MSDU_PAGE_BAND0],
 			       MT_RXQ_ID(MT_RXQ_MSDU_PAGE_BAND0),
@@ -531,8 +565,7 @@ int mt7996_dma_rro_init(struct mt7996_dev *dev)
 		/* rx msdu page queue for band1 */
 		mdev->q_rx[MT_RXQ_MSDU_PAGE_BAND1].flags =
 			MT_WED_RRO_Q_MSDU_PG(1) | MT_QFLAG_WED_RRO_EN;
-		if (mtk_wed_device_active(&mdev->mmio.wed) &&
-		    mtk_wed_get_rx_capa(&mdev->mmio.wed))
+		if (mt76_wed_check_rx_cap(&mdev->mmio.wed))
 			mdev->q_rx[MT_RXQ_MSDU_PAGE_BAND1].wed = &mdev->mmio.wed;
 		ret = mt76_queue_alloc(dev, &mdev->q_rx[MT_RXQ_MSDU_PAGE_BAND1],
 				       MT_RXQ_ID(MT_RXQ_MSDU_PAGE_BAND1),
@@ -547,8 +580,7 @@ int mt7996_dma_rro_init(struct mt7996_dev *dev)
 		/* rx msdu page queue for band2 */
 		mdev->q_rx[MT_RXQ_MSDU_PAGE_BAND2].flags =
 			MT_WED_RRO_Q_MSDU_PG(2) | MT_QFLAG_WED_RRO_EN;
-		if (mtk_wed_device_active(&mdev->mmio.wed) &&
-		    mtk_wed_get_rx_capa(&mdev->mmio.wed))
+		if (mt76_wed_check_rx_cap(&mdev->mmio.wed))
 			mdev->q_rx[MT_RXQ_MSDU_PAGE_BAND2].wed = &mdev->mmio.wed;
 		ret = mt76_queue_alloc(dev, &mdev->q_rx[MT_RXQ_MSDU_PAGE_BAND2],
 				       MT_RXQ_ID(MT_RXQ_MSDU_PAGE_BAND2),
@@ -559,12 +591,14 @@ int mt7996_dma_rro_init(struct mt7996_dev *dev)
 			return ret;
 	}
 
-
-
+start_hw_rro:
 	if (mtk_wed_device_active(&mdev->mmio.wed)) {
 		irq_mask = mdev->mmio.irqmask |
 			   MT_INT_TX_DONE_BAND2;
 
+		if (mdev->mmio.wed.version == MTK_WED_HW_V3_1)
+			irq_mask |= MT_INT_RX(MT_RXQ_WED_RX_DATA);
+
 		mt76_wr(dev, MT_INT_MASK_CSR, irq_mask);
 		mtk_wed_device_start_hw_rro(&mdev->mmio.wed, irq_mask, false);
 		mt7996_irq_enable(dev, irq_mask);
@@ -659,7 +693,7 @@ int mt7996_dma_init(struct mt7996_dev *dev)
 		return ret;
 
 	/* rx data queue for band0 and mt7996 band1 */
-	if (mtk_wed_device_active(wed) && mtk_wed_get_rx_capa(wed)) {
+	if (mt76_wed_check_rx_cap(wed) && wed->version != MTK_WED_HW_V3_1) {
 		dev->mt76.q_rx[MT_RXQ_MAIN].flags = MT_WED_Q_RX(0);
 		dev->mt76.q_rx[MT_RXQ_MAIN].wed = wed;
 	}
@@ -672,9 +706,22 @@ int mt7996_dma_init(struct mt7996_dev *dev)
 	if (ret)
 		return ret;
 
+	if (mt76_wed_check_rx_cap(wed) && wed->version == MTK_WED_HW_V3_1) {
+		dev->mt76.q_rx[MT_RXQ_WED_RX_DATA].flags = MT_WED_Q_RX(0);
+		dev->mt76.q_rx[MT_RXQ_WED_RX_DATA].wed = wed;
+		rx_base = is_mt7996(&dev->mt76) ? MT_MCUQ_RING_BASE(MT_RXQ_WED_RX_DATA) :
+			  MT_RXQ_RING_BASE(MT_RXQ_WED_RX_DATA);
+		ret = mt76_queue_alloc(dev, &dev->mt76.q_rx[MT_RXQ_WED_RX_DATA],
+				       MT_RXQ_ID(MT_RXQ_WED_RX_DATA),
+				       MT7996_RX_RING_SIZE,
+				       MT_RX_BUF_SIZE,
+				       rx_base);
+		if (ret)
+			return ret;
+	}
 	/* tx free notify event from WA for band0 */
 	if (mtk_wed_device_active(wed) &&
-	    ((is_mt7996(&dev->mt76) && !dev->has_rro) ||
+	    ((is_mt7996(&dev->mt76) && !mt7996_has_hwrro(dev)) ||
 	     (is_mt7992(&dev->mt76)))) {
 		dev->mt76.q_rx[MT_RXQ_MAIN_WA].flags = MT_WED_Q_TXFREE;
 		dev->mt76.q_rx[MT_RXQ_MAIN_WA].wed = wed;
@@ -719,7 +766,7 @@ int mt7996_dma_init(struct mt7996_dev *dev)
 	if (mt7996_band_valid(dev, MT_BAND2)) {
 		/* rx data queue for mt7996 band2 */
 		rx_base = MT_RXQ_RING_BASE(MT_RXQ_BAND2) + hif1_ofs;
-		if (mtk_wed_device_active(wed_hif2) && mtk_wed_get_rx_capa(wed_hif2)) {
+		if (mt76_wed_check_rx_cap(wed_hif2)) {
 			dev->mt76.q_rx[MT_RXQ_BAND2].flags = MT_WED_Q_RX(0);
 			dev->mt76.q_rx[MT_RXQ_BAND2].wed = wed_hif2;
 		}
@@ -734,7 +781,7 @@ int mt7996_dma_init(struct mt7996_dev *dev)
 		/* tx free notify event from WA for mt7996 band2
 		 * use pcie0's rx ring3, but, redirect pcie0 rx ring3 interrupt to pcie1
 		 */
-		if (mtk_wed_device_active(wed_hif2) && !dev->has_rro) {
+		if (mtk_wed_device_active(wed_hif2) && !mt7996_has_hwrro(dev)) {
 			dev->mt76.q_rx[MT_RXQ_BAND2_WA].flags = MT_WED_Q_TXFREE;
 			dev->mt76.q_rx[MT_RXQ_BAND2_WA].wed = wed_hif2;
 		}
@@ -749,7 +796,7 @@ int mt7996_dma_init(struct mt7996_dev *dev)
 	} else if (mt7996_band_valid(dev, MT_BAND1)) {
 		/* rx data queue for mt7992 band1 */
 		rx_base = MT_RXQ_RING_BASE(MT_RXQ_BAND1) + hif1_ofs;
-		if (mtk_wed_device_active(wed) && mtk_wed_get_rx_capa(wed)) {
+		if (mt76_wed_check_rx_cap(wed) && wed->version != MTK_WED_HW_V3_1) {
 			dev->mt76.q_rx[MT_RXQ_BAND1].flags = MT_WED_Q_RX(1);
 			dev->mt76.q_rx[MT_RXQ_BAND1].wed = wed;
 		}
@@ -779,11 +826,11 @@ int mt7996_dma_init(struct mt7996_dev *dev)
 		}
 	}
 
-	if (dev->has_rro) {
+	if (mt7996_has_hwrro(dev)) {
 		/* rx rro data queue for band0 */
 		dev->mt76.q_rx[MT_RXQ_RRO_BAND0].flags =
 			MT_WED_RRO_Q_DATA(0) | MT_QFLAG_WED_RRO_EN;
-		if (mtk_wed_device_active(wed) && mtk_wed_get_rx_capa(wed))
+		if (mt76_wed_check_rx_cap(wed))
 			dev->mt76.q_rx[MT_RXQ_RRO_BAND0].wed = wed;
 		ret = mt76_queue_alloc(dev, &dev->mt76.q_rx[MT_RXQ_RRO_BAND0],
 				       MT_RXQ_ID(MT_RXQ_RRO_BAND0),
@@ -796,7 +843,7 @@ int mt7996_dma_init(struct mt7996_dev *dev)
 		if (!is_mt7996(&dev->mt76)) {
 			dev->mt76.q_rx[MT_RXQ_RRO_BAND1].flags =
 				MT_WED_RRO_Q_DATA(1) | MT_QFLAG_WED_RRO_EN;
-			if (mtk_wed_device_active(wed) && mtk_wed_get_rx_capa(wed))
+			if (mt76_wed_check_rx_cap(wed))
 				dev->mt76.q_rx[MT_RXQ_RRO_BAND1].wed = wed;
 			ret = mt76_queue_alloc(dev, &dev->mt76.q_rx[MT_RXQ_RRO_BAND1],
 					       MT_RXQ_ID(MT_RXQ_RRO_BAND1),
@@ -824,7 +871,7 @@ int mt7996_dma_init(struct mt7996_dev *dev)
 			/* rx rro data queue for band2 */
 			dev->mt76.q_rx[MT_RXQ_RRO_BAND2].flags =
 				MT_WED_RRO_Q_DATA(1) | MT_QFLAG_WED_RRO_EN;
-			if (mtk_wed_device_active(wed) && mtk_wed_get_rx_capa(wed))
+			if (mt76_wed_check_rx_cap(wed))
 				dev->mt76.q_rx[MT_RXQ_RRO_BAND2].wed = wed;
 			ret = mt76_queue_alloc(dev, &dev->mt76.q_rx[MT_RXQ_RRO_BAND2],
 					       MT_RXQ_ID(MT_RXQ_RRO_BAND2),
@@ -902,7 +949,7 @@ void mt7996_dma_reset(struct mt7996_dev *dev, bool force)
 		dev_info(dev->mt76.dev,"%s L1 SER rx queue clean up done.",
 			 wiphy_name(dev->mt76.hw->wiphy));
 
-	if (dev->has_rro && !mtk_wed_device_active(&dev->mt76.mmio.wed)) {
+	if (mt7996_has_hwrro(dev) && !mtk_wed_device_active(&dev->mt76.mmio.wed)) {
 		mt7996_rro_msdu_pg_free(dev);
 		mt7996_rx_token_put(dev);
 	}
@@ -975,7 +1022,8 @@ void mt7996_dma_reset(struct mt7996_dev *dev, bool force)
 	mt76_for_each_q_rx(&dev->mt76, i) {
 		if (mtk_wed_device_active(&dev->mt76.mmio.wed) && force &&
 		    (mt76_queue_is_wed_rro_ind(&dev->mt76.q_rx[i]) ||
-		     mt76_queue_is_wed_rro_msdu_pg(&dev->mt76.q_rx[i])))
+		     mt76_queue_is_wed_rro_msdu_pg(&dev->mt76.q_rx[i]) ||
+		     mt76_queue_is_wed_rro_rxdmad_c(&dev->mt76.q_rx[i])))
 			continue;
 
 		mt76_queue_rx_reset(dev, i);
diff --git a/mt7996/init.c b/mt7996/init.c
index 29eae3e71..fec19ab66 100644
--- a/mt7996/init.c
+++ b/mt7996/init.c
@@ -713,11 +713,11 @@ void mt7996_mac_init(struct mt7996_dev *dev)
 	}
 
 	/* griffin does not have WA */
-	if (!dev->has_rro && mt7996_has_wa(dev))
+	if (!mt7996_has_hwrro(dev) && mt7996_has_wa(dev))
 		txfree_path = MT7996_TXFREE_FROM_WA;
 
 	rx_path_type = dev->hif2 ? rx_path_type : 0;
-	rro_bypass = dev->has_rro ? rro_bypass : MT7996_RRO_ALL_BYPASS;
+	rro_bypass = mt7996_has_hwrro(dev) ? rro_bypass : MT7996_RRO_ALL_BYPASS;
 
 	mt7996_mcu_set_rro(dev, UNI_RRO_SET_PLATFORM_TYPE, rx_path_type);
 	mt7996_mcu_set_rro(dev, UNI_RRO_SET_BYPASS_MODE, rro_bypass);
@@ -727,7 +727,7 @@ void mt7996_mac_init(struct mt7996_dev *dev)
 		"Platform_type = %d, bypass_rro = %d, txfree_path = %d\n",
 		rx_path_type, rro_bypass, txfree_path);
 
-	if (dev->has_rro) {
+	if (mt7996_has_hwrro(dev)) {
 		u16 timeout;
 
 		timeout = mt76_rr(dev, MT_HW_REV) == MT_HW_REV1 ? 512 : 128;
@@ -943,51 +943,14 @@ void mt7996_wfsys_reset(struct mt7996_dev *dev)
 	msleep(20);
 }
 
-void mt7996_rro_hw_init(struct mt7996_dev *dev)
+static void mt7996_rro_v3_hw_init(struct mt7996_dev *dev)
 {
 	struct mtk_wed_device *wed = &dev->mt76.mmio.wed;
-	u32 reg = MT_RRO_ADDR_ELEM_SEG_ADDR0;
-	int i;
 
-	if (!dev->has_rro)
+	if (dev->mt76.hwrro_mode == MT76_HWRRO_V3_1)
 		return;
 
-	INIT_LIST_HEAD(&dev->wed_rro.pg_addr_cache);
-	for (i = 0; i < MT7996_RRO_MSDU_PG_HASH_SIZE; i++)
-		INIT_LIST_HEAD(&dev->wed_rro.pg_hash_head[i]);
-
-	if (!is_mt7996(&dev->mt76)) {
-		/* set emul 3.0 function */
-		mt76_wr(dev, MT_RRO_3_0_EMU_CONF,
-			MT_RRO_3_0_EMU_CONF_EN_MASK);
-
-		mt76_wr(dev, MT_RRO_ADDR_ARRAY_BASE0,
-			dev->wed_rro.addr_elem[0].phy_addr);
-	} else {
-
-		/* TODO: remove line after WM has set */
-		mt76_clear(dev, WF_RRO_AXI_MST_CFG, WF_RRO_AXI_MST_CFG_DIDX_OK);
-
-		/* setup BA bitmap cache address */
-		mt76_wr(dev, MT_RRO_BA_BITMAP_BASE0,
-			dev->wed_rro.ba_bitmap[0].phy_addr);
-		mt76_wr(dev, MT_RRO_BA_BITMAP_BASE1, 0);
-		mt76_wr(dev, MT_RRO_BA_BITMAP_BASE_EXT0,
-			dev->wed_rro.ba_bitmap[1].phy_addr);
-		mt76_wr(dev, MT_RRO_BA_BITMAP_BASE_EXT1, 0);
-
-		/* setup Address element address */
-		for (i = 0; i < ARRAY_SIZE(dev->wed_rro.addr_elem); i++) {
-			mt76_wr(dev, reg, dev->wed_rro.addr_elem[i].phy_addr >> 4);
-			reg += 4;
-		}
-
-		/* setup Address element address - separate address segment mode */
-		mt76_wr(dev, MT_RRO_ADDR_ARRAY_BASE1,
-			MT_RRO_ADDR_ARRAY_ELEM_ADDR_SEG_MODE);
-	}
-
-	if (mtk_wed_device_active(wed) && mtk_wed_get_rx_capa(wed)) {
+	if (mt76_wed_check_rx_cap(wed)) {
 		wed->wlan.ind_cmd.win_size = ffs(MT7996_RRO_WINDOW_MAX_LEN) - 6;
 		if (is_mt7996(&dev->mt76))
 			wed->wlan.ind_cmd.particular_sid = MT7996_RRO_MAX_SESSION;
@@ -1009,9 +972,46 @@ void mt7996_rro_hw_init(struct mt7996_dev *dev)
 	/* use max session idx + 1 as particular session id */
 	mt76_wr(dev, MT_RRO_PARTICULAR_CFG0, dev->wed_rro.session.phy_addr);
 
+	if (!is_mt7996(&dev->mt76))
+		mt76_wr(dev, MT_RRO_PARTICULAR_CFG1,
+			MT_RRO_PARTICULAR_CONFG_EN |
+			FIELD_PREP(MT_RRO_PARTICULAR_SID, 1));
+	else
+		mt76_wr(dev, MT_RRO_PARTICULAR_CFG1,
+			MT_RRO_PARTICULAR_CONFG_EN |
+			FIELD_PREP(MT_RRO_PARTICULAR_SID, MT7996_RRO_MAX_SESSION));
+
+}
+
+void mt7996_rro_hw_init(struct mt7996_dev *dev)
+{
+	u32 reg = MT_RRO_ADDR_ELEM_SEG_ADDR0;
+	int i;
+
+	if (!mt7996_has_hwrro(dev))
+		return;
+
+	INIT_LIST_HEAD(&dev->wed_rro.pg_addr_cache);
+	for (i = 0; i < MT7996_RRO_MSDU_PG_HASH_SIZE; i++)
+		INIT_LIST_HEAD(&dev->wed_rro.pg_hash_head[i]);
+
 	if (!is_mt7996(&dev->mt76)) {
 		reg = MT_RRO_MSDU_PG_SEG_ADDR0;
 
+		if (dev->mt76.hwrro_mode == MT76_HWRRO_V3_1) {
+			mt76_clear(dev, MT_RRO_3_0_EMU_CONF,
+				   MT_RRO_3_0_EMU_CONF_EN_MASK);
+			mt76_set(dev, MT_RRO_3_1_GLOBAL_CONFIG,
+				 MT_RRO_3_1_GLOBAL_CONFIG_RXDMAD_SEL);
+		} else {
+			/* set emul 3.0 function */
+			mt76_wr(dev, MT_RRO_3_0_EMU_CONF,
+				MT_RRO_3_0_EMU_CONF_EN_MASK);
+
+			mt76_wr(dev, MT_RRO_ADDR_ARRAY_BASE0,
+				dev->wed_rro.addr_elem[0].phy_addr);
+		}
+
 		mt76_set(dev, MT_RRO_3_1_GLOBAL_CONFIG,
 			 MT_RRO_3_1_GLOBAL_CONFIG_INTERLEAVE_EN);
 
@@ -1020,14 +1020,32 @@ void mt7996_rro_hw_init(struct mt7996_dev *dev)
 			mt76_wr(dev, reg, dev->wed_rro.msdu_pg[i].phy_addr >> 4);
 			reg += 4;
 		}
-		mt76_wr(dev, MT_RRO_PARTICULAR_CFG1,
-			MT_RRO_PARTICULAR_CONFG_EN |
-			FIELD_PREP(MT_RRO_PARTICULAR_SID, 1));
 	} else {
-		mt76_wr(dev, MT_RRO_PARTICULAR_CFG1,
-			MT_RRO_PARTICULAR_CONFG_EN |
-			FIELD_PREP(MT_RRO_PARTICULAR_SID, MT7996_RRO_MAX_SESSION));
+
+		/* TODO: remove line after WM has set */
+		mt76_clear(dev, WF_RRO_AXI_MST_CFG, WF_RRO_AXI_MST_CFG_DIDX_OK);
+
+		/* setup BA bitmap cache address */
+		mt76_wr(dev, MT_RRO_BA_BITMAP_BASE0,
+			dev->wed_rro.ba_bitmap[0].phy_addr);
+		mt76_wr(dev, MT_RRO_BA_BITMAP_BASE1, 0);
+		mt76_wr(dev, MT_RRO_BA_BITMAP_BASE_EXT0,
+			dev->wed_rro.ba_bitmap[1].phy_addr);
+		mt76_wr(dev, MT_RRO_BA_BITMAP_BASE_EXT1, 0);
+
+		/* setup Address element address */
+		for (i = 0; i < ARRAY_SIZE(dev->wed_rro.addr_elem); i++) {
+			mt76_wr(dev, reg, dev->wed_rro.addr_elem[i].phy_addr >> 4);
+			reg += 4;
+		}
+
+		/* setup Address element address - separate address segment mode */
+		mt76_wr(dev, MT_RRO_ADDR_ARRAY_BASE1,
+			MT_RRO_ADDR_ARRAY_ELEM_ADDR_SEG_MODE);
 	}
+
+	mt7996_rro_v3_hw_init(dev);
+
 	/* interrupt enable */
 	mt76_wr(dev, MT_RRO_HOST_INT_ENA,
 		MT_RRO_HOST_INT_ENA_HOST_RRO_DONE_ENA);
@@ -1041,18 +1059,20 @@ static int mt7996_wed_rro_init(struct mt7996_dev *dev)
 	void *ptr;
 	int i;
 
-	if (!dev->has_rro)
+	if (!mt7996_has_hwrro(dev))
 		return 0;
 
-	for (i = 0; i < ARRAY_SIZE(dev->wed_rro.ba_bitmap); i++) {
-		ptr = dmam_alloc_coherent(dev->mt76.dma_dev,
-					  MT7996_RRO_BA_BITMAP_CR_SIZE,
-					  &dev->wed_rro.ba_bitmap[i].phy_addr,
-					  GFP_KERNEL);
-		if (!ptr)
-			return -ENOMEM;
+	if (dev->mt76.hwrro_mode == MT76_HWRRO_V3) {
+		for (i = 0; i < ARRAY_SIZE(dev->wed_rro.ba_bitmap); i++) {
+			ptr = dmam_alloc_coherent(dev->mt76.dma_dev,
+						  MT7996_RRO_BA_BITMAP_CR_SIZE,
+						  &dev->wed_rro.ba_bitmap[i].phy_addr,
+						  GFP_KERNEL);
+			if (!ptr)
+				return -ENOMEM;
 
-		dev->wed_rro.ba_bitmap[i].ptr = ptr;
+			dev->wed_rro.ba_bitmap[i].ptr = ptr;
+		}
 	}
 
 	for (i = 0; i < ARRAY_SIZE(dev->wed_rro.addr_elem); i++) {
@@ -1074,7 +1094,7 @@ static int mt7996_wed_rro_init(struct mt7996_dev *dev)
 			addr->signature = 0xff;
 			addr++;
 		}
-		if (mtk_wed_device_active(wed) && mtk_wed_get_rx_capa(wed))
+		if (mt76_wed_check_rx_cap(wed))
 			wed->wlan.ind_cmd.addr_elem_phys[i] = dev->wed_rro.addr_elem[i].phy_addr;
 	}
 
@@ -1113,7 +1133,7 @@ static void mt7996_wed_rro_free(struct mt7996_dev *dev)
 {
 	int i;
 
-	if (!dev->has_rro)
+	if (!mt7996_has_hwrro(dev))
 		return;
 
 	for (i = 0; i < ARRAY_SIZE(dev->wed_rro.ba_bitmap); i++) {
@@ -1867,7 +1887,7 @@ void mt7996_unregister_device(struct mt7996_dev *dev)
 	mt7996_mcu_exit(dev);
 	mt7996_tx_token_put(dev);
 	mt7996_dma_cleanup(dev);
-	if (dev->has_rro && !mtk_wed_device_active(&dev->mt76.mmio.wed)) {
+	if (mt7996_has_hwrro(dev) && !mtk_wed_device_active(&dev->mt76.mmio.wed)) {
 		mt7996_rro_msdu_pg_free(dev);
 		mt7996_rx_token_put(dev);
 	}
diff --git a/mt7996/mac.c b/mt7996/mac.c
index 5d95b5989..3f76023d5 100644
--- a/mt7996/mac.c
+++ b/mt7996/mac.c
@@ -2229,14 +2229,15 @@ mt7996_mac_restart(struct mt7996_dev *dev)
 	if (ret)
 		goto out;
 
-	if (mtk_wed_device_active(&dev->mt76.mmio.wed) && dev->has_rro) {
+	if (mtk_wed_device_active(&dev->mt76.mmio.wed) && mt7996_has_hwrro(dev)) {
 		u32 wed_irq_mask = dev->mt76.mmio.irqmask |
 				   MT_INT_TX_DONE_BAND2;
 
 		mt7996_rro_hw_init(dev);
 		mt76_for_each_q_rx(&dev->mt76, i) {
 			if (mt76_queue_is_wed_rro_ind(&dev->mt76.q_rx[i]) ||
-			    mt76_queue_is_wed_rro_msdu_pg(&dev->mt76.q_rx[i]))
+			    mt76_queue_is_wed_rro_msdu_pg(&dev->mt76.q_rx[i]) ||
+			    mt76_queue_is_wed_rro_rxdmad_c(&dev->mt76.q_rx[i]))
 				mt76_queue_rx_reset(dev, i);
 		}
 
@@ -2482,7 +2483,7 @@ void mt7996_mac_reset_work(struct work_struct *work)
 	dev_info(dev->mt76.dev,"%s L1 SER dma start done.",
 		 wiphy_name(dev->mt76.hw->wiphy));
 
-	if (is_mt7992(&dev->mt76) && dev->has_rro)
+	if (!is_mt7996(&dev->mt76) && dev->mt76.hwrro_mode == MT76_HWRRO_V3)
 		mt76_wr(dev, MT_RRO_3_0_EMU_CONF, MT_RRO_3_0_EMU_CONF_EN_MASK);
 
 	if (mtk_wed_device_active(&dev->mt76.mmio.wed)) {
diff --git a/mt7996/mcu.c b/mt7996/mcu.c
index cce808b91..b79f959bb 100644
--- a/mt7996/mcu.c
+++ b/mt7996/mcu.c
@@ -1258,7 +1258,7 @@ mt7996_mcu_wed_rro_event(struct mt7996_dev *dev, struct sk_buff *skb)
 {
 	struct mt7996_mcu_wed_rro_event *event = (void *)skb->data;
 
-	if (!dev->has_rro)
+	if (!mt7996_has_hwrro(dev))
 		return;
 
 	skb_pull(skb, sizeof(struct mt7996_mcu_rxd) + 4);
@@ -2139,7 +2139,7 @@ mt7996_mcu_sta_ba(struct mt7996_dev *dev, struct mt76_vif_link *mvif,
 	ba->ba_en = enable << params->tid;
 	ba->amsdu = params->amsdu;
 	ba->tid = params->tid;
-	ba->ba_rdd_rro = !tx && enable && dev->has_rro;
+	ba->ba_rdd_rro = !tx && enable && mt7996_has_hwrro(dev);
 
 	return mt76_mcu_skb_send_msg(&dev->mt76, skb,
 				     MCU_WMWA_UNI_CMD(STA_REC_UPDATE), true);
diff --git a/mt7996/mmio.c b/mt7996/mmio.c
index b9f59a419..74cbcb53c 100644
--- a/mt7996/mmio.c
+++ b/mt7996/mmio.c
@@ -455,6 +455,16 @@ out:
 }
 #endif
 
+/*
+ *			MTK_WED_HW_V3	MTK_WED_HW_V3_1
+ *			(MT7988)	(MT7987)
+ * ----------------------------------------------------
+ * 	|MT7996		MT76_HWRRO_V3	MT76_HWRRO_V3
+ *	|
+ * RRO	|MT7992		MT76_HWRRO_V3	MT76_HWRRO_V3_1
+ * 	|
+ * 	|MT7990 	MT76_HWRRO_V3	MT76_HWRRO_V3_1
+ */
 int mt7996_mmio_wed_init(struct mt7996_dev *dev, void *pdev_ptr,
 			 bool hif2, int *irq)
 {
@@ -462,18 +472,39 @@ int mt7996_mmio_wed_init(struct mt7996_dev *dev, void *pdev_ptr,
 	struct mtk_wed_device *wed = &dev->mt76.mmio.wed;
 	struct pci_dev *pci_dev = pdev_ptr;
 	u32 hif1_ofs = 0, intr, ring;
+	u16 tx_token_size, rx_token_size;
+	int wed_hw_ver;
 
 	if (!wed_enable)
 		return 0;
 
-	dev->has_rro = true;
+	if (hif2)
+		wed = &dev->mt76.mmio.wed_hif2;
+
+	wed_hw_ver = mtk_wed_device_get_hw_version();
+
+	switch (wed_hw_ver) {
+	case MTK_WED_HW_V3:
+		dev->mt76.hwrro_mode = MT76_HWRRO_V3;
+		tx_token_size = MT7996_WED_TOKEN_SIZE_V3;
+		rx_token_size = dev->hif2 ? 32768 : 24576;
+		break;
+	case MTK_WED_HW_V3_1:
+		dev->mt76.hwrro_mode = is_mt7996(&dev->mt76) ?
+				       MT76_HWRRO_V3 : MT76_HWRRO_V3_1;
+		tx_token_size = MT7996_WED_TOKEN_SIZE_V3_1;
+		rx_token_size = 8192;
+		break;
+	default:
+		wed_enable = false;
+		dev->mt76.hwrro_mode = MT76_HWRRO_DISABLE;
+		dev_err(dev->mt76.dev, "wed version %d not support\n", wed_hw_ver);
+		return 0;
+	}
 
 	if (dev->hif2)
 		hif1_ofs = MT_WFDMA0_PCIE1(0) - MT_WFDMA0(0);
 
-	if (hif2)
-		wed = &dev->mt76.mmio.wed_hif2;
-
 	wed->wlan.pci_dev = pci_dev;
 	wed->wlan.bus_type = MTK_WED_BUS_PCIE;
 
@@ -487,10 +518,12 @@ int mt7996_mmio_wed_init(struct mt7996_dev *dev, void *pdev_ptr,
 				      MT_INT_PCIE1_SOURCE_CSR_EXT;
 		wed->wlan.wpdma_mask = wed->wlan.phy_base +
 				       MT_INT_PCIE1_MASK_CSR;
-		wed->wlan.wpdma_tx = wed->wlan.phy_base + hif1_ofs +
-					     MT_TXQ_RING_BASE(0) +
-					     MT7996_TXQ_BAND2 * MT_RING_SIZE;
-		if (dev->has_rro) {
+
+		wed->wlan.wpdma_tx[0] = wed->wlan.phy_base + hif1_ofs +
+					MT_TXQ_RING_BASE(0) +
+					MT7996_TXQ_BAND2 * MT_RING_SIZE;
+
+		if (mt7996_has_hwrro(dev)) {
 			switch (mt76_chip(&dev->mt76)) {
 			case MT7996_DEVICE_ID:
 				intr = MT_INT_RX_TXFREE_EXT;
@@ -519,25 +552,30 @@ int mt7996_mmio_wed_init(struct mt7996_dev *dev, void *pdev_ptr,
 		}
 
 		wed->wlan.wpdma_rx_glo = wed->wlan.phy_base + hif1_ofs + MT_WFDMA0_GLO_CFG;
-
-		wed->wlan.wpdma_rx[0] = wed->wlan.phy_base + hif1_ofs +
-				     MT_RXQ_RING_BASE(MT7996_RXQ_BAND2) +
-				     MT7996_RXQ_BAND2 * MT_RING_SIZE;
+		if (wed_hw_ver == MTK_WED_HW_V3)
+			wed->wlan.wpdma_rx[0] = wed->wlan.phy_base + hif1_ofs +
+						MT_RXQ_RING_BASE(MT7996_RXQ_BAND2) +
+						MT7996_RXQ_BAND2 * MT_RING_SIZE;
 
 		wed->wlan.id = MT7996_DEVICE_ID_2;
 		wed->wlan.tx_tbit[0] = ffs(MT_INT_TX_DONE_BAND2) - 1;
 	} else {
-		wed->wlan.hw_rro = dev->has_rro; /* default on */
+		wed->wlan.hw_rro = (enum mtk_wed_hwrro_mode )dev->mt76.hwrro_mode; /* default on */
 		wed->wlan.wpdma_int = wed->wlan.phy_base + MT_INT_SOURCE_CSR;
 		wed->wlan.wpdma_mask = wed->wlan.phy_base + MT_INT_MASK_CSR;
-		wed->wlan.wpdma_tx = wed->wlan.phy_base + MT_TXQ_RING_BASE(0) +
-				     MT7996_TXQ_BAND0 * MT_RING_SIZE;
+
+		wed->wlan.wpdma_tx[0] = wed->wlan.phy_base + MT_TXQ_RING_BASE(0) +
+					MT7996_TXQ_BAND0 * MT_RING_SIZE;
+
+		wed->wlan.wpdma_tx[1] = wed->wlan.phy_base + MT_TXQ_RING_BASE(1) +
+					MT7996_TXQ_BAND1 * MT_RING_SIZE;
 
 		wed->wlan.wpdma_rx_glo = wed->wlan.phy_base + MT_WFDMA0_GLO_CFG;
 
-		wed->wlan.wpdma_rx[0] = wed->wlan.phy_base +
-				     MT_RXQ_RING_BASE(MT7996_RXQ_BAND0) +
-				     MT7996_RXQ_BAND0 * MT_RING_SIZE;
+		if (wed_hw_ver == MTK_WED_HW_V3)
+			wed->wlan.wpdma_rx[0] = wed->wlan.phy_base +
+						MT_RXQ_RING_BASE(MT7996_RXQ_BAND0) +
+						MT7996_RXQ_BAND0 * MT_RING_SIZE;
 
 		wed->wlan.wpdma_rx_rro[0] = wed->wlan.phy_base +
 					    MT_RXQ_RING_BASE(MT7996_RXQ_RRO_BAND0) +
@@ -551,20 +589,34 @@ int mt7996_mmio_wed_init(struct mt7996_dev *dev, void *pdev_ptr,
 						    MT_RXQ_RING_BASE(MT7996_RXQ_RRO_BAND1) +
 						    MT7996_RXQ_RRO_BAND1 * MT_RING_SIZE;
 
-			wed->wlan.wpdma_rx[1] = wed->wlan.phy_base + hif1_ofs +
-						MT_RXQ_RING_BASE(MT7996_RXQ_BAND1) +
-						MT7996_RXQ_BAND1 * MT_RING_SIZE;
+			if (wed_hw_ver == MTK_WED_HW_V3)
+				wed->wlan.wpdma_rx[1] = wed->wlan.phy_base + hif1_ofs +
+							MT_RXQ_RING_BASE(MT7996_RXQ_BAND1) +
+							MT7996_RXQ_BAND1 * MT_RING_SIZE;
 		}
 
-		wed->wlan.wpdma_rx_pg = wed->wlan.phy_base +
-					MT_RXQ_RING_BASE(MT7996_RXQ_MSDU_PG_BAND0) +
-					MT7996_RXQ_MSDU_PG_BAND0 * MT_RING_SIZE;
+		if (dev->mt76.hwrro_mode == MT76_HWRRO_V3)
+			wed->wlan.wpdma_rx_pg = wed->wlan.phy_base +
+						MT_RXQ_RING_BASE(MT7996_RXQ_MSDU_PG_BAND0) +
+						MT7996_RXQ_MSDU_PG_BAND0 * MT_RING_SIZE;
+		else
+			wed->wlan.wpdma_rro_3_1_rx = wed->wlan.phy_base +
+						     MT_RXQ_RRO_AP_RING_BASE +
+						     MT_RXQ_ID(MT_RXQ_RRO_RXDMAD_C) * MT_RING_SIZE;
 
 		wed->wlan.rx_nbuf = 65536;
-		wed->wlan.rx_npkt = dev->hif2 ? 32768 : 24576;
+		wed->wlan.rx_npkt = rx_token_size;
 		wed->wlan.rx_size = SKB_WITH_OVERHEAD(MT_RX_BUF_SIZE);
 
-		wed->wlan.rx_tbit[0] = ffs(MT_INT_RX_DONE_BAND0) - 1;
+		if (wed_hw_ver == MTK_WED_HW_V3) {
+			wed->wlan.rx_tbit[0] = ffs(MT_INT_RX_DONE_BAND0) - 1;
+
+		} else {
+			if (dev->mt76.hwrro_mode == MT76_HWRRO_V3_1)
+				wed->wlan.rx_tbit[0] = ffs(MT_INT_RX_DONE_WED_RX_DATA) - 1;
+			else
+				wed->wlan.rx_tbit[0] = ffs(MT_INT_RX_DONE_WED_RX_DATA_MT7996) - 1;
+		}
 		wed->wlan.rro_rx_tbit[0] = ffs(MT_INT_RX_DONE_RRO_BAND0) - 1;
 		if (is_mt7996(&dev->mt76)) {
 			wed->wlan.rx_tbit[1] = ffs(MT_INT_RX_DONE_BAND2) - 1;
@@ -574,15 +626,18 @@ int mt7996_mmio_wed_init(struct mt7996_dev *dev, void *pdev_ptr,
 			wed->wlan.rro_rx_tbit[1] = ffs(MT_INT_RX_DONE_RRO_BAND1) - 1;
 		}
 
-		wed->wlan.rx_pg_tbit[0] = ffs(MT_INT_RX_DONE_MSDU_PG_BAND0) - 1;
-		wed->wlan.rx_pg_tbit[1] = ffs(MT_INT_RX_DONE_MSDU_PG_BAND1) - 1;
-		wed->wlan.rx_pg_tbit[2] = ffs(MT_INT_RX_DONE_MSDU_PG_BAND2) - 1;
-
+		if (dev->mt76.hwrro_mode == MT76_HWRRO_V3) {
+			wed->wlan.rx_pg_tbit[0] = ffs(MT_INT_RX_DONE_MSDU_PG_BAND0) - 1;
+			wed->wlan.rx_pg_tbit[1] = ffs(MT_INT_RX_DONE_MSDU_PG_BAND1) - 1;
+			wed->wlan.rx_pg_tbit[2] = ffs(MT_INT_RX_DONE_MSDU_PG_BAND2) - 1;
+		} else {
+			wed->wlan.rro_3_1_rx_tbit = ffs(MT_INT_RX_DONE_RRO_RXDMAD_C) - 1;
+		}
 		wed->wlan.tx_tbit[0] = ffs(MT_INT_TX_DONE_BAND0) - 1;
 		wed->wlan.tx_tbit[1] = ffs(MT_INT_TX_DONE_BAND1) - 1;
 		switch (mt76_chip(&dev->mt76)) {
 		case MT7996_DEVICE_ID:
-			if (dev->has_rro) {
+			if (mt7996_has_hwrro(dev)) {
 				intr = MT_INT_RX_TXFREE_MAIN;
 				ring = MT7996_RXQ_TXFREE0;
 			} else {
@@ -607,10 +662,10 @@ int mt7996_mmio_wed_init(struct mt7996_dev *dev, void *pdev_ptr,
 		wed->wlan.txfree_tbit = ffs(intr) - 1;
 		wed->wlan.wpdma_txfree = wed->wlan.phy_base + MT_RXQ_RING_BASE(0) +
 					 ring * MT_RING_SIZE;
-		dev->mt76.rx_token_size = MT7996_TOKEN_SIZE + wed->wlan.rx_npkt;
+		dev->mt76.rx_token_size = tx_token_size + rx_token_size;
 	}
 
-	wed->wlan.nbuf = MT7996_TOKEN_SIZE;
+	wed->wlan.nbuf = tx_token_size;
 	wed->wlan.token_start = 0;
 
 	wed->wlan.amsdu_max_subframes = 8;
@@ -628,7 +683,8 @@ int mt7996_mmio_wed_init(struct mt7996_dev *dev, void *pdev_ptr,
 
 	if (mtk_wed_device_attach(wed)) {
 		wed_enable = false;
-		dev->has_rro = false;
+		dev->mt76.hwrro_mode = MT76_HWRRO_DISABLE;
+		dev_err(dev->mt76.dev, "wed attach fail\n");
 		return 0;
 	}
 
diff --git a/mt7996/mt7996.h b/mt7996/mt7996.h
index 9f814bd48..288652aeb 100644
--- a/mt7996/mt7996.h
+++ b/mt7996/mt7996.h
@@ -96,7 +96,8 @@
 #define MT7996_EEPROM_BLOCK_SIZE		16
 #define MT7996_EXT_EEPROM_BLOCK_SIZE	1024
 #define MT7996_TOKEN_SIZE		16384
-#define MT7996_HW_TOKEN_SIZE		8192
+#define MT7996_WED_TOKEN_SIZE_V3	16384
+#define MT7996_WED_TOKEN_SIZE_V3_1	9216
 #define MT7996_SW_TOKEN_SIZE		15360
 #define MT7996_PER_BAND_TOKEN_SIZE	4000
 
@@ -243,6 +244,7 @@ enum mt7996_coredump_state {
 };
 
 enum mt7996_txq_id {
+	MT7996_TXQ_WED_RX = 0,
 	MT7996_TXQ_FWDL = 16,
 	MT7996_TXQ_MCU_WM,
 	MT7996_TXQ_BAND0,
@@ -265,11 +267,13 @@ enum mt7996_rxq_id {
 	MT7996_RXQ_RRO_BAND2 = 6,
 	MT7996_RXQ_MSDU_PG_BAND0 = 10,
 	MT7996_RXQ_MSDU_PG_BAND1 = 11,
+	MT7996_RXQ_WED_RX_DATA = 11, /* for mt7992 and mt7990 with wed 3.1 */
 	MT7996_RXQ_MSDU_PG_BAND2 = 12,
 	MT7996_RXQ_TXFREE0 = 9,
 	MT7996_RXQ_TXFREE1 = 9,
 	MT7996_RXQ_TXFREE2 = 7,
 	MT7996_RXQ_RRO_IND = 0,
+	MT7996_RXQ_RRO_RXDMAD_C = 0,
 	MT7990_RXQ_TXFREE0 = 6,
 	MT7990_RXQ_TXFREE1 = 7,
 };
@@ -830,7 +834,6 @@ struct mt7996_dev {
 
 	bool flash_mode:1;
 	bool has_eht:1;
-	bool has_rro:1;
 
 	bool fips_cap:1;
 	bool pwr_boost_cap:1;
@@ -1362,6 +1365,11 @@ static inline bool mt7996_has_wa(struct mt7996_dev *dev)
 	return !is_mt7990(&dev->mt76);
 }
 
+static inline bool mt7996_has_hwrro(struct mt7996_dev *dev)
+{
+	return dev->mt76.hwrro_mode != MT76_HWRRO_DISABLE;
+}
+
 static inline u8 mt7996_max_interface_num(struct mt7996_dev *dev)
 {
 	return min(MT7996_MAX_INTERFACES * (1 + mt7996_band_valid(dev, MT_BAND1) +
diff --git a/mt7996/mtk_debug.h b/mt7996/mtk_debug.h
index d73430458..26b4bdd42 100644
--- a/mt7996/mtk_debug.h
+++ b/mt7996/mtk_debug.h
@@ -972,7 +972,8 @@ struct queue_desc {
 /* RRO TOP */
 #define WF_RRO_TOP_BASE                                        0xA000 /*0x820C2000 */
 #define WF_RRO_TOP_IND_CMD_0_CTRL0_ADDR                        (WF_RRO_TOP_BASE + 0x40) // 2040
-											//
+#define WF_RRO_TOP_RX_RING_AP_0_CTRL0_ADDR                     (WF_RRO_TOP_BASE + 0x650) // 2650
+
 /* WTBL */
 enum mt7996_wtbl_type {
 	WTBL_TYPE_LMAC, 	/* WTBL in LMAC */
diff --git a/mt7996/mtk_debugfs.c b/mt7996/mtk_debugfs.c
index 0b064d766..f49235661 100644
--- a/mt7996/mtk_debugfs.c
+++ b/mt7996/mtk_debugfs.c
@@ -606,6 +606,8 @@ mt7996_show_dma_info(struct seq_file *s, struct mt7996_dev *dev)
 		WF_WFDMA_HOST_DMA0_WPDMA_RX_RING12_CTRL0_ADDR);
 	dump_dma_rx_ring_info(s, dev, "IND:IND_CMD(MAC2H)", "Both",
 		WF_RRO_TOP_IND_CMD_0_CTRL0_ADDR);
+	dump_dma_rx_ring_info(s, dev, "RRO:Data0(MAC2H)", "Both",
+		WF_RRO_TOP_RX_RING_AP_0_CTRL0_ADDR);
 
 	if (dev->hif2) {
 		seq_printf(s, "HOST_DMA0 PCIe1 Ring Configuration\n");
@@ -4395,7 +4397,12 @@ mt7996_rx_drop_show(struct seq_file *s, void *data)
 	struct mt76_queue *q[2];
 	int i = 0;
 
-	q[0] = &mdev->q_rx[MT_RXQ_MAIN];
+	if (mtk_wed_device_active(&mdev->mmio.wed) &&
+	    mdev->mmio.wed.version == MTK_WED_HW_V3_1)
+		q[0] = &mdev->q_rx[MT_RXQ_WED_RX_DATA];
+	else
+		q[0] = &mdev->q_rx[MT_RXQ_MAIN];
+
 	q[1] = is_mt7996(mdev) ? &mdev->q_rx[MT_RXQ_BAND2] :
 				 &mdev->q_rx[MT_RXQ_BAND1];
 
@@ -4654,7 +4661,7 @@ void mt7996_mtk_init_dev_debugfs(struct mt7996_dev *dev, struct dentry *dir)
 	debugfs_create_file("muru_fixed_group_rate", 0600, dir, dev,
 			    &fops_muru_fixed_group_rate);
 
-	if (dev->has_rro) {
+	if (mt7996_has_hwrro(dev)) {
 		debugfs_create_u32("rro_sid", 0600, dir, &dev->dbg.sid);
 		debugfs_create_devm_seqfile(dev->mt76.dev, "rro_sid_info", dir,
 					    mt7996_rro_session_read);
diff --git a/mt7996/pci.c b/mt7996/pci.c
index 1abd57415..8c4f774e0 100644
--- a/mt7996/pci.c
+++ b/mt7996/pci.c
@@ -13,8 +13,8 @@
 static bool hif2_enable = false;
 module_param(hif2_enable, bool, 0644);
 
-static bool rro_enable = false;
-module_param(rro_enable, bool, 0644);
+static int rro_mode = MT76_HWRRO_DISABLE;
+module_param(rro_mode, int, 0644);
 
 static LIST_HEAD(hif_list);
 static DEFINE_SPINLOCK(hif_lock);
@@ -151,8 +151,20 @@ static int mt7996_pci_probe(struct pci_dev *pdev,
 	if (IS_ERR(dev))
 		return PTR_ERR(dev);
 
-	dev->has_rro = rro_enable;
 	mdev = &dev->mt76;
+	switch (rro_mode) {
+	case MT76_HWRRO_V3:
+		mdev->hwrro_mode = rro_mode;
+		break;
+	case MT76_HWRRO_V3_1:
+		mdev->hwrro_mode = is_mt7996(mdev) ? MT76_HWRRO_V3 : MT76_HWRRO_V3_1;
+		break;
+	case MT76_HWRRO_DISABLE:
+	default:
+		mdev->hwrro_mode = MT76_HWRRO_DISABLE;
+		break;
+	}
+
 	mt7996_wfsys_reset(dev);
 	hif2 = mt7996_pci_init_hif2(pdev);
 	if (hif2)
diff --git a/mt7996/regs.h b/mt7996/regs.h
index 414e9edfb..660c90f1a 100644
--- a/mt7996/regs.h
+++ b/mt7996/regs.h
@@ -115,8 +115,13 @@ enum offs_rev {
 
 #define MT_RRO_3_1_GLOBAL_CONFIG		MT_RRO_TOP(0x604)
 #define MT_RRO_3_1_GLOBAL_CONFIG_INTERLEAVE_EN	BIT(0)
+#define MT_RRO_3_1_GLOBAL_CONFIG_RX_DIDX_WR_EN	BIT(2)
+#define MT_RRO_3_1_GLOBAL_CONFIG_RX_CIDX_RD_EN	BIT(3)
+#define MT_RRO_3_1_GLOBAL_CONFIG_RXDMAD_SEL	BIT(6)
 
 #define MT_RRO_MSDU_PG_SEG_ADDR0		MT_RRO_TOP(0x620)
+#define MT_RRO_RX_RING_AP_CIDX_ADDR		MT_RRO_TOP(0x6f0)
+#define MT_RRO_RX_RING_AP_DIDX_ADDR		MT_RRO_TOP(0x6f4)
 
 #define MT_RRO_ACK_SN_CTRL			MT_RRO_TOP(0x50)
 #define MT_RRO_ACK_SN_CTRL_SN_MASK		GENMASK(27, 16)
@@ -514,6 +519,7 @@ enum offs_rev {
 #define MT_TXQ_RING_BASE(q)			(MT_Q_BASE(__TXQ(q)) + 0x300)
 #define MT_RXQ_RING_BASE(q)			(MT_Q_BASE(__RXQ(q)) + 0x500)
 #define MT_RXQ_RRO_IND_RING_BASE		MT_RRO_TOP(0x40)
+#define MT_RXQ_RRO_AP_RING_BASE			MT_RRO_TOP(0x650)
 
 #define MT_MCUQ_EXT_CTRL(q)			(MT_Q_BASE(q) +	0x600 +	\
 						 MT_MCUQ_ID(q) * 0x4)
@@ -536,6 +542,7 @@ enum offs_rev {
 #define MT_INT_RX_DONE_WA_MAIN			BIT(2)
 #define MT_INT_RX_DONE_WA_EXT			BIT(3) /* for mt7992 */
 #define MT_INT_RX_DONE_WA_TRI			BIT(3)
+#define MT_INT_RX_DONE_WED_RX_DATA_MT7996	BIT(4) /* for mt7996 */
 #define MT_INT_RX_TXFREE_MAIN			BIT(17)
 #define MT_INT_RX_TXFREE_BAND1			BIT(15)
 #define MT_INT_RX_TXFREE_TRI			BIT(15)
@@ -550,8 +557,10 @@ enum offs_rev {
 #define MT_INT_RX_DONE_RRO_BAND1		BIT(17)
 #define MT_INT_RX_DONE_RRO_BAND2		BIT(14)
 #define MT_INT_RX_DONE_RRO_IND			BIT(11)
+#define MT_INT_RX_DONE_RRO_RXDMAD_C		BIT(11)
 #define MT_INT_RX_DONE_MSDU_PG_BAND0		BIT(18)
 #define MT_INT_RX_DONE_MSDU_PG_BAND1		BIT(19)
+#define MT_INT_RX_DONE_WED_RX_DATA		BIT(19) /* for mt7992 and mt7990 */
 #define MT_INT_RX_DONE_MSDU_PG_BAND2		BIT(23)
 
 #define MT_INT_RX(q)				(dev->q_int_mask[__RXQ(q)])
diff --git a/wed.c b/wed.c
index b6eaffe48..ec18fce0e 100644
--- a/wed.c
+++ b/wed.c
@@ -182,6 +182,11 @@ int mt76_wed_dma_setup(struct mt76_dev *dev, struct mt76_queue *q, bool reset)
 		mt76_dma_rx_fill_buf(dev, q, false);
 		mtk_wed_device_ind_rx_ring_setup(q->wed, q->regs);
 		break;
+	case MT76_WED_RRO_Q_RXDMAD_C:
+		q->flags &= ~MT_QFLAG_WED;
+		mt76_dma_queue_reset(dev, q, true);
+		mtk_wed_device_rro_3_1_rx_ring_setup(q->wed, q->regs);
+		break;
 	default:
 		ret = -EINVAL;
 		break;
-- 
2.45.2

