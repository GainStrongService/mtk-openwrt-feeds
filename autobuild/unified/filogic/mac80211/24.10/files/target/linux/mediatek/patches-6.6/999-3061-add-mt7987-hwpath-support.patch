From 0ae58df86194465ecd14f458cfa78a5e1df534d8 Mon Sep 17 00:00:00 2001
From: Rex Lu <rex.lu@mediatek.com>
Date: Mon, 21 Oct 2024 16:19:03 +0800
Subject: [PATCH] add mt7987 hwpath support

---
 drivers/net/ethernet/mediatek/mtk_wed.c       | 841 +++++++++++++-----
 drivers/net/ethernet/mediatek/mtk_wed.h       |  17 +-
 .../net/ethernet/mediatek/mtk_wed_debugfs.c   |  54 +-
 drivers/net/ethernet/mediatek/mtk_wed_mcu.c   |   2 +-
 drivers/net/ethernet/mediatek/mtk_wed_regs.h  |  57 +-
 drivers/net/ethernet/mediatek/mtk_wed_wo.c    |   6 +
 include/linux/soc/mediatek/mtk_wed.h          |  58 +-
 7 files changed, 773 insertions(+), 262 deletions(-)

diff --git a/drivers/net/ethernet/mediatek/mtk_wed.c b/drivers/net/ethernet/mediatek/mtk_wed.c
index be09568..396b948 100644
--- a/drivers/net/ethernet/mediatek/mtk_wed.c
+++ b/drivers/net/ethernet/mediatek/mtk_wed.c
@@ -47,6 +47,7 @@
 
 #define MTK_WED_TX_BM_DMA_SIZE		65536
 #define MTK_WED_TX_BM_PKT_CNT		32768
+#define MTK_WED_TX_BM_PKT_CNT_V3_1	17408
 
 static struct mtk_wed_hw *hw_list[3];
 static DEFINE_MUTEX(hw_lock);
@@ -64,9 +65,11 @@ static const struct mtk_wed_soc_data mt7622_data = {
 		.wpdma_rx_ring1		= 0,
 		.reset_idx_tx_mask	= GENMASK(3, 0),
 		.reset_idx_rx_mask	= GENMASK(17, 16),
+		.msdu_pg_ring2_cfg	= 0,
 	},
 	.tx_ring_desc_size = sizeof(struct mtk_wdma_desc),
 	.wdma_desc_size = sizeof(struct mtk_wdma_desc),
+	.wo_support = 0,
 };
 
 static const struct mtk_wed_soc_data mt7986_data = {
@@ -77,9 +80,11 @@ static const struct mtk_wed_soc_data mt7986_data = {
 		.wpdma_rx_ring1		= 0,
 		.reset_idx_tx_mask	= GENMASK(1, 0),
 		.reset_idx_rx_mask	= GENMASK(7, 6),
+		.msdu_pg_ring2_cfg	= 0,
 	},
 	.tx_ring_desc_size = sizeof(struct mtk_wdma_desc),
 	.wdma_desc_size = 2 * sizeof(struct mtk_wdma_desc),
+	.wo_support = 1,
 };
 
 static const struct mtk_wed_soc_data mt7988_data = {
@@ -90,9 +95,25 @@ static const struct mtk_wed_soc_data mt7988_data = {
 		.wpdma_rx_ring1		= 0x7d8,
 		.reset_idx_tx_mask	= GENMASK(1, 0),
 		.reset_idx_rx_mask	= GENMASK(7, 6),
+		.msdu_pg_ring2_cfg	= 0xe58,
 	},
 	.tx_ring_desc_size = sizeof(struct mtk_wed_bm_desc),
 	.wdma_desc_size = 2 * sizeof(struct mtk_wdma_desc),
+	.wo_support = 1,
+};
+
+static const struct mtk_wed_soc_data mt7987_data = {
+	.regmap = {
+		.wed_rev_id		= 0x4,
+		.tx_bm_tkid		= 0x0c8,
+		.wpdma_rx_ring0		= 0,
+		.reset_idx_tx_mask	= GENMASK(1, 0),
+		.reset_idx_rx_mask	= GENMASK(7, 6),
+		.msdu_pg_ring2_cfg	= 0xe2c,
+	},
+	.tx_ring_desc_size = sizeof(struct mtk_wed_bm_desc),
+	.wdma_desc_size = 2 * sizeof(struct mtk_wdma_desc),
+	.wo_support = 0,
 };
 
 static void
@@ -409,6 +430,9 @@ mtk_wed_wo_reset(struct mtk_wed_device *dev)
 	mtk_wdma_tx_reset(dev);
 	mtk_wed_reset(dev, MTK_WED_RESET_WED);
 
+	if (!dev->hw->soc->wo_support || !wo)
+		return;
+
 	if (mtk_wed_mcu_send_msg(wo, MTK_WED_MODULE_ID_WO,
 				 MTK_WED_WO_CMD_CHANGE_STATE, &state,
 				 sizeof(state), false))
@@ -582,38 +606,44 @@ mtk_wed_amsdu_init(struct mtk_wed_device *dev)
 		return 0;
 
 	for (i = 0; i < MTK_WED_AMSDU_NPAGES; i++)
-		wed_w32(dev, MTK_WED_AMSDU_HIFTXD_BASE_L(i),
-			wed_amsdu[i].txd_phy);
-
-	/* init all sta parameter */
-	wed_w32(dev, MTK_WED_AMSDU_STA_INFO_INIT, MTK_WED_AMSDU_STA_RMVL |
-		MTK_WED_AMSDU_STA_WTBL_HDRT_MODE |
-		FIELD_PREP(MTK_WED_AMSDU_STA_MAX_AMSDU_LEN,
-			   dev->wlan.amsdu_max_len >> 8) |
-		FIELD_PREP(MTK_WED_AMSDU_STA_MAX_AMSDU_NUM,
-			   dev->wlan.amsdu_max_subframes));
-
-	wed_w32(dev, MTK_WED_AMSDU_STA_INFO, MTK_WED_AMSDU_STA_INFO_DO_INIT);
-
-	ret = mtk_wed_poll_busy(dev, MTK_WED_AMSDU_STA_INFO,
-				MTK_WED_AMSDU_STA_INFO_DO_INIT);
-	if (ret) {
-		dev_err(dev->hw->dev, "amsdu initialization failed\n");
-		return ret;
+		if (mtk_wed_is_v3_1(dev->hw))
+			wed_w32(dev, MTK_WED_HIFTXD_BASE_L(i),
+				wed_amsdu[i].txd_phy);
+		else
+			wed_w32(dev, MTK_WED_AMSDU_HIFTXD_BASE_L(i),
+				wed_amsdu[i].txd_phy);
+
+	if (mtk_wed_is_v3(dev->hw)) {
+		/* init all sta parameter */
+		wed_w32(dev, MTK_WED_AMSDU_STA_INFO_INIT, MTK_WED_AMSDU_STA_RMVL |
+			MTK_WED_AMSDU_STA_WTBL_HDRT_MODE |
+			FIELD_PREP(MTK_WED_AMSDU_STA_MAX_AMSDU_LEN,
+				   dev->wlan.amsdu_max_len >> 8) |
+			FIELD_PREP(MTK_WED_AMSDU_STA_MAX_AMSDU_NUM,
+				   dev->wlan.amsdu_max_subframes));
+
+		wed_w32(dev, MTK_WED_AMSDU_STA_INFO, MTK_WED_AMSDU_STA_INFO_DO_INIT);
+
+		ret = mtk_wed_poll_busy(dev, MTK_WED_AMSDU_STA_INFO,
+					MTK_WED_AMSDU_STA_INFO_DO_INIT);
+		if (ret) {
+			dev_err(dev->hw->dev, "amsdu initialization failed\n");
+			return ret;
+		}
 	}
-
 	/* init partial amsdu offload txd src */
 	wed_set(dev, MTK_WED_AMSDU_HIFTXD_CFG,
 		FIELD_PREP(MTK_WED_AMSDU_HIFTXD_SRC, dev->hw->index));
 
-	/* init qmem */
-	wed_set(dev, MTK_WED_AMSDU_PSE, MTK_WED_AMSDU_PSE_RESET);
-	ret = mtk_wed_poll_busy(dev, MTK_WED_MON_AMSDU_QMEM_STS1, BIT(29));
-	if (ret) {
-		pr_info("%s: amsdu qmem initialization failed\n", __func__);
-		return ret;
+	if (mtk_wed_is_v3(dev->hw)) {
+		/* init qmem */
+		wed_set(dev, MTK_WED_AMSDU_PSE, MTK_WED_AMSDU_PSE_RESET);
+		ret = mtk_wed_poll_busy(dev, MTK_WED_MON_AMSDU_QMEM_STS1, BIT(29));
+		if (ret) {
+			pr_info("%s: amsdu qmem initialization failed\n", __func__);
+			return ret;
+		}
 	}
-
 	/* eagle E1 PCIE1 tx ring 22 flow control issue */
 	if (dev->wlan.id == 0x7991 || dev->wlan.id == 0x7992)
 		wed_clr(dev, MTK_WED_AMSDU_FIFO, MTK_WED_AMSDU_IS_PRIOR0_RING);
@@ -638,7 +668,10 @@ mtk_wed_tx_buffer_alloc(struct mtk_wed_device *dev)
 		dev->tx_buf_ring.size = ring_size;
 	} else {
 		dev->tx_buf_ring.size = MTK_WED_TX_BM_DMA_SIZE;
-		ring_size = MTK_WED_TX_BM_PKT_CNT;
+		if (mtk_wed_is_v3_1(dev->hw))
+			ring_size = MTK_WED_TX_BM_PKT_CNT_V3_1;
+		else
+			ring_size = MTK_WED_TX_BM_PKT_CNT;
 	}
 	n_pages = dev->tx_buf_ring.size / MTK_WED_BUF_PER_PAGE;
 
@@ -763,8 +796,14 @@ mtk_wed_hwrro_buffer_alloc(struct mtk_wed_device *dev)
 	dma_addr_t desc_phys;
 	int i, page_idx = 0;
 
-	if (!dev->wlan.hw_rro)
+	switch (dev->wlan.hw_rro) {
+	case MTK_WED_HWRRO_V3:
+		break;
+	case MTK_WED_HWRRO_DISABLE:
+	case MTK_WED_HWRRO_V3_1:
+	default:
 		return 0;
+	}
 
 	page_list = kcalloc(n_pages, sizeof(*page_list), GFP_KERNEL);
 	if (!page_list)
@@ -844,8 +883,14 @@ mtk_wed_hwrro_free_buffer(struct mtk_wed_device *dev)
 	struct mtk_wed_bm_desc *desc = dev->hw_rro.desc;
 	int i, page_idx = 0;
 
-	if (!dev->wlan.hw_rro)
+	switch (dev->wlan.hw_rro) {
+	case MTK_WED_HWRRO_V3:
+		break;
+	case MTK_WED_HWRRO_DISABLE:
+	case MTK_WED_HWRRO_V3_1:
+	default:
 		return;
+	}
 
 	if (!page_list)
 		return;
@@ -893,11 +938,22 @@ mtk_wed_hwrro_init(struct mtk_wed_device *dev)
 	if (!mtk_wed_get_rx_capa(dev) || !dev->wlan.hw_rro)
 		return;
 
+	if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3_1) {
+		wed_set(dev, MTK_WED_RRO_CTRL, MTK_WED_RRO_CTRL_VER_SEL);
+		wed_set(dev, MTK_WED_WPDMA_RRO3_1_RX_PREF_CFG,
+			MTK_WED_WPDMA_RRO3_1_RX_PREF_EN);
+		return;
+	}
 	wed_set(dev, MTK_WED_RRO_PG_BM_RX_DMAM,
 		FIELD_PREP(MTK_WED_RRO_PG_BM_RX_SDL0, 128));
 
 	wed_w32(dev, MTK_WED_RRO_PG_BM_BASE, dev->hw_rro.desc_phys);
 
+	if (mtk_wed_is_v3_1(dev->hw))
+		wed_w32(dev, MTK_WED_RRO_PG_BM_RANGE_CFG,
+			FIELD_PREP(MTK_WED_RRO_PG_BM_RANGE_SW_BUF,
+				   MTK_WED_RX_PG_BM_CNT));
+
 	wed_w32(dev, MTK_WED_RRO_PG_BM_INIT_PTR,
 		MTK_WED_RRO_PG_BM_INIT_SW_TAIL_IDX |
 		FIELD_PREP(MTK_WED_RRO_PG_BM_SW_TAIL_IDX,
@@ -913,10 +969,17 @@ mtk_wed_rx_buffer_hw_init(struct mtk_wed_device *dev)
 	wed_w32(dev, MTK_WED_RX_BM_RX_DMAD,
 		FIELD_PREP(MTK_WED_RX_BM_RX_DMAD_SDL0, dev->wlan.rx_size));
 	wed_w32(dev, MTK_WED_RX_BM_BASE, dev->rx_buf_ring.desc_phys);
+
+	if (mtk_wed_is_v3_1(dev->hw))
+		wed_w32(dev, MTK_WED_RX_BM_RANGE_CFG,
+			FIELD_PREP(MTK_WED_RX_BM_RANGE_SW_BUF, dev->wlan.rx_npkt));
+
 	wed_w32(dev, MTK_WED_RX_BM_INIT_PTR, MTK_WED_RX_BM_INIT_SW_TAIL |
 		FIELD_PREP(MTK_WED_RX_BM_SW_TAIL, dev->wlan.rx_npkt));
-	wed_w32(dev, MTK_WED_RX_BM_DYN_ALLOC_TH,
-		FIELD_PREP(MTK_WED_RX_BM_DYN_ALLOC_TH_H, 0xffff));
+
+	if (!mtk_wed_is_v3_1(dev->hw))
+		wed_w32(dev, MTK_WED_RX_BM_DYN_ALLOC_TH,
+			FIELD_PREP(MTK_WED_RX_BM_DYN_ALLOC_TH_H, 0xffff));
 	wed_set(dev, MTK_WED_CTRL, MTK_WED_CTRL_WED_RX_BM_EN);
 
 	mtk_wed_hwrro_init(dev);
@@ -969,18 +1032,18 @@ mtk_wed_set_ext_int(struct mtk_wed_device *dev, bool en)
 	u32 mask = MTK_WED_EXT_INT_STATUS_ERROR_MASK;
 
 	switch (dev->hw->version) {
-	case 1:
+	case MTK_WED_HW_V1:
 		mask |= MTK_WED_EXT_INT_STATUS_TX_DRV_R_RESP_ERR;
 		break;
-	case 2:
+	case MTK_WED_HW_V2:
 		mask |= MTK_WED_EXT_INT_STATUS_RX_FBUF_LO_TH |
 			MTK_WED_EXT_INT_STATUS_RX_FBUF_HI_TH |
 			MTK_WED_EXT_INT_STATUS_RX_DRV_COHERENT |
 			MTK_WED_EXT_INT_STATUS_TX_DMA_W_RESP_ERR;
 		break;
-	case 3:
-		mask = MTK_WED_EXT_INT_STATUS_RX_DRV_COHERENT |
-		       MTK_WED_EXT_INT_STATUS_TKID_WO_PYLD;
+	case MTK_WED_HW_V3:
+	case MTK_WED_HW_V3_1:
+		mask = MTK_WED_EXT_INT_STATUS_RX_DRV_COHERENT;
 		break;
 	default:
 		break;
@@ -1018,7 +1081,6 @@ mtk_wed_check_wfdma_rx_fill(struct mtk_wed_device *dev,
 
 	for (i = 0; i < 3; i++) {
 		u32 cur_idx = readl(ring->wpdma + MTK_WED_RING_OFS_CPU_IDX);
-
 		if (cur_idx == MTK_WED_RX_RING_SIZE - 1)
 			break;
 
@@ -1056,12 +1118,13 @@ mtk_wed_dma_disable(struct mtk_wed_device *dev)
 		wdma_clr(dev, MTK_WDMA_GLO_CFG,
 			 MTK_WDMA_GLO_CFG_RX_INFO3_PRERES);
 	} else {
-		wed_clr(dev, MTK_WED_WPDMA_GLO_CFG,
-			MTK_WED_WPDMA_GLO_CFG_RX_DRV_R0_PKT_PROC |
-			MTK_WED_WPDMA_GLO_CFG_RX_DRV_R0_CRX_SYNC);
-
-		wed_clr(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
-			MTK_WED_WPDMA_RX_D_RX_DRV_EN);
+		if (!mtk_wed_is_v3_1(dev->hw)) {
+			wed_clr(dev, MTK_WED_WPDMA_GLO_CFG,
+				MTK_WED_WPDMA_GLO_CFG_RX_DRV_R0_PKT_PROC |
+				MTK_WED_WPDMA_GLO_CFG_RX_DRV_R0_CRX_SYNC);
+			wed_clr(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
+				MTK_WED_WPDMA_RX_D_RX_DRV_EN);
+		}
 		wed_clr(dev, MTK_WED_WDMA_GLO_CFG,
 			MTK_WED_WDMA_GLO_CFG_TX_DDONE_CHK);
 
@@ -1137,11 +1200,9 @@ __mtk_wed_detach(struct mtk_wed_device *dev)
 	mtk_wed_free_tx_rings(dev);
 
 	if (mtk_wed_get_rx_capa(dev)) {
-		if (hw->wed_wo)
-			mtk_wed_wo_reset(dev);
+		mtk_wed_wo_reset(dev);
 		mtk_wed_free_rx_rings(dev);
-		if (hw->wed_wo)
-			mtk_wed_wo_deinit(hw);
+		mtk_wed_wo_deinit(hw);
 	}
 
 	if (dev->wlan.bus_type == MTK_WED_BUS_PCIE) {
@@ -1195,12 +1256,16 @@ mtk_wed_bus_init(struct mtk_wed_device *dev)
 			wed_w32(dev, MTK_WED_PCIE_CFG_BASE,
 				dev->hw->pcie_base | 0xc04);
 			wed_w32(dev, MTK_WED_PCIE_INT_TRIGGER, BIT(8));
+			if (mtk_wed_is_v3_1(dev->hw))
+				wed_w32(dev, MTK_WED_PCIE_INT_CLR, BIT(8));
 		} else {
 			wed_w32(dev, MTK_WED_PCIE_CFG_INTM,
 				dev->hw->pcie_base | 0x180);
 			wed_w32(dev, MTK_WED_PCIE_CFG_BASE,
 				dev->hw->pcie_base | 0x184);
 			wed_w32(dev, MTK_WED_PCIE_INT_TRIGGER, BIT(24));
+			if (mtk_wed_is_v3_1(dev->hw))
+				wed_w32(dev, MTK_WED_PCIE_INT_CLR, BIT(24));
 		}
 
 		wed_w32(dev, MTK_WED_PCIE_INT_CTRL,
@@ -1224,6 +1289,58 @@ mtk_wed_bus_init(struct mtk_wed_device *dev)
 	}
 }
 
+static void
+mtk_wed_v3_1_set_wpdma(struct mtk_wed_device *dev)
+{
+	int i;
+
+	/* Tx data ring */
+	wed_w32(dev, MTK_WED_WPDMA_CFG_TX0_CIDX,
+		dev->wlan.wpdma_tx[0] + MTK_WED_RING_OFS_CPU_IDX);
+	wed_w32(dev, MTK_WED_WPDMA_CFG_TX0_DIDX,
+		dev->wlan.wpdma_tx[0] + MTK_WED_RING_OFS_DMA_IDX);
+	wed_w32(dev, MTK_WED_WPDMA_CFG_TX1_CIDX,
+		dev->wlan.wpdma_tx[1] + MTK_WED_RING_OFS_CPU_IDX);
+	wed_w32(dev, MTK_WED_WPDMA_CFG_TX1_DIDX,
+		dev->wlan.wpdma_tx[1] + MTK_WED_RING_OFS_DMA_IDX);
+	/* Tx free done event ring */
+	wed_w32(dev, MTK_WED_WPDMA_CFG_TX_FREE_CIDX,
+		dev->wlan.wpdma_txfree + MTK_WED_RING_OFS_CPU_IDX);
+	wed_w32(dev, MTK_WED_WPDMA_CFG_TX_FREE_DIDX,
+		dev->wlan.wpdma_txfree + MTK_WED_RING_OFS_DMA_IDX);
+
+	if (!mtk_wed_get_rx_capa(dev))
+		return;
+
+	/* GLO CFG */
+	wed_w32(dev, MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG_BASE,
+		dev->wlan.wpdma_rx_glo);
+
+	/* RRO data rings */
+	for (i = 0; i < MTK_WED_RX_QUEUES; i++) {
+		wed_w32(dev, MTK_WED_WPDMA_RRO_RX_RING_CIDX(i),
+			dev->wlan.wpdma_rx_rro[i] + MTK_WED_RING_OFS_CPU_IDX);
+		wed_w32(dev, MTK_WED_WPDMA_RRO_RX_RING_DIDX(i),
+			dev->wlan.wpdma_rx_rro[i] + MTK_WED_RING_OFS_DMA_IDX);
+	}
+
+	if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3) {
+		/* RRO MSDU page rings */
+		for (i = 0; i < MTK_WED_RX_PAGE_QUEUES; i++) {
+			wed_w32(dev, MTK_WED_RRO_MSDU_PG_RING_CFG_CIDX(i),
+				dev->wlan.wpdma_rx_pg + i * 0x10 + MTK_WED_RING_OFS_CPU_IDX);
+			wed_w32(dev, MTK_WED_RRO_MSDU_PG_RING_CFG_DIDX(i),
+				dev->wlan.wpdma_rx_pg + i * 0x10 + MTK_WED_RING_OFS_DMA_IDX);
+		}
+	} else {
+		/* MTK_WED_HWRRO_V3_1 ring */
+		wed_w32(dev, MTK_WED_WPDMA_RRO3_1_RX_RING_CIDX,
+			dev->wlan.wpdma_rro_3_1_rx + MTK_WED_RING_OFS_CPU_IDX);
+		wed_w32(dev, MTK_WED_WPDMA_RRO3_1_RX_RING_DIDX,
+			dev->wlan.wpdma_rro_3_1_rx + MTK_WED_RING_OFS_DMA_IDX);
+	}
+}
+
 static void
 mtk_wed_set_wpdma(struct mtk_wed_device *dev)
 {
@@ -1238,7 +1355,13 @@ mtk_wed_set_wpdma(struct mtk_wed_device *dev)
 
 	wed_w32(dev, MTK_WED_WPDMA_CFG_BASE, dev->wlan.wpdma_int);
 	wed_w32(dev, MTK_WED_WPDMA_CFG_INT_MASK, dev->wlan.wpdma_mask);
-	wed_w32(dev, MTK_WED_WPDMA_CFG_TX, dev->wlan.wpdma_tx);
+
+	if (mtk_wed_is_v3_1(dev->hw)) {
+		mtk_wed_v3_1_set_wpdma(dev);
+		return;
+	}
+
+	wed_w32(dev, MTK_WED_WPDMA_CFG_TX, dev->wlan.wpdma_tx[0]);
 	wed_w32(dev, MTK_WED_WPDMA_CFG_TX_FREE, dev->wlan.wpdma_txfree);
 
 	if (!mtk_wed_get_rx_capa(dev))
@@ -1276,7 +1399,8 @@ mtk_wed_hw_init_early(struct mtk_wed_device *dev)
 		set |= MTK_WED_WDMA_GLO_CFG_DYNAMIC_SKIP_DMAD_PREP |
 		       MTK_WED_WDMA_GLO_CFG_IDLE_DMAD_SUPPLY;
 	}
-	wed_m32(dev, MTK_WED_WDMA_GLO_CFG, mask, set);
+	if (!mtk_wed_is_v3_1(dev->hw))
+		wed_m32(dev, MTK_WED_WDMA_GLO_CFG, mask, set);
 
 	if (mtk_wed_is_v1(dev->hw)) {
 		u32 offset = dev->hw->index ? 0x04000400 : 0;
@@ -1305,6 +1429,13 @@ mtk_wed_hw_init_early(struct mtk_wed_device *dev)
 			FIELD_PREP(MTK_WED_WDMA_OFST1_RX_CTRL,
 				   MTK_WDMA_RING_RX(0)));
 	}
+
+	if (mtk_wed_is_v3_1(dev->hw)) {
+		wed_clr(dev, MTK_WED_WDMA_GLO_CFG,
+			MTK_WED_WDMA_GLO_CFG_DYNAMIC_SKIP_DMAD_PREP);
+		wed_set(dev, MTK_WED_WDMA_GLO_CFG,
+			MTK_WED_WDMA_GLO_CFG_DYNAMIC_SKIP_DMAD_THRES);
+	}
 }
 
 static int
@@ -1499,6 +1630,16 @@ mtk_wed_hw_init(struct mtk_wed_device *dev)
 				   dev->tx_buf_ring.size / 128) |
 			FIELD_PREP(MTK_WED_TX_TKID_CTRL_RSV_GRP_NUM,
 				   dev->tx_buf_ring.size / 128));
+	} else if (mtk_wed_is_v3_1(dev->hw)) {
+		wed_w32(dev, MTK_WED_TX_TKID_CTRL,
+			FIELD_PREP(MTK_WED_TX_TKID_CTRL_VLD_GRP_NUM_V3,
+				   dev->wlan.nbuf / 128) |
+			FIELD_PREP(MTK_WED_TX_TKID_CTRL_RSV_GRP_NUM_V3,
+				   dev->wlan.nbuf / 128));
+		wed_w32(dev, MTK_WED_TX_TKID_DYN_THR,
+			FIELD_PREP(MTK_WED_TX_TKID_DYN_THR_LO, 0) |
+			FIELD_PREP(MTK_WED_TX_TKID_DYN_THR_HI_V3,
+				   dev->wlan.nbuf / 128));
 	}
 
 	wed_w32(dev, dev->hw->soc->regmap.tx_bm_tkid,
@@ -1508,7 +1649,7 @@ mtk_wed_hw_init(struct mtk_wed_device *dev)
 
 	mtk_wed_reset(dev, MTK_WED_RESET_TX_BM);
 
-	if (mtk_wed_is_v3_or_greater(dev->hw)) {
+	if (mtk_wed_is_v3(dev->hw)) {
 		/* switch to new bm architecture */
 		wed_clr(dev, MTK_WED_TX_BM_CTRL,
 			MTK_WED_TX_BM_CTRL_LEGACY_EN);
@@ -1526,6 +1667,13 @@ mtk_wed_hw_init(struct mtk_wed_device *dev)
 		wed_w32(dev, MTK_WED_TX_BM_INIT_PTR,
 			MTK_WED_TX_BM_PKT_CNT |
 			MTK_WED_TX_BM_INIT_SW_TAIL_IDX);
+	} else if (mtk_wed_is_v3_1(dev->hw)) {
+		wed_w32(dev, MTK_WED_TX_BM_RANGE_CFG,
+			MTK_WED_TX_BM_PKT_CNT_V3_1);
+
+		wed_w32(dev, MTK_WED_TX_BM_INIT_PTR,
+			MTK_WED_TX_BM_PKT_CNT_V3_1 |
+			MTK_WED_TX_BM_INIT_SW_TAIL_IDX);
 	}
 
 	if (mtk_wed_is_v1(dev->hw)) {
@@ -1534,33 +1682,40 @@ mtk_wed_hw_init(struct mtk_wed_device *dev)
 			MTK_WED_CTRL_WED_TX_FREE_AGENT_EN);
 	} else if (mtk_wed_get_rx_capa(dev)) {
 		/* rx hw init */
-		wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX,
-			MTK_WED_WPDMA_RX_D_RST_CRX_IDX |
-			MTK_WED_WPDMA_RX_D_RST_DRV_IDX);
-		wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX, 0);
-
-		/* reset prefetch index of ring */
-		wed_set(dev, MTK_WED_WPDMA_RX_D_PREF_RX0_SIDX,
-			MTK_WED_WPDMA_RX_D_PREF_SIDX_IDX_CLR);
-		wed_clr(dev, MTK_WED_WPDMA_RX_D_PREF_RX0_SIDX,
-			MTK_WED_WPDMA_RX_D_PREF_SIDX_IDX_CLR);
-
-		wed_set(dev, MTK_WED_WPDMA_RX_D_PREF_RX1_SIDX,
-			MTK_WED_WPDMA_RX_D_PREF_SIDX_IDX_CLR);
-		wed_clr(dev, MTK_WED_WPDMA_RX_D_PREF_RX1_SIDX,
-			MTK_WED_WPDMA_RX_D_PREF_SIDX_IDX_CLR);
-
-		/* reset prefetch FIFO of ring */
-		wed_set(dev, MTK_WED_WPDMA_RX_D_PREF_FIFO_CFG,
-			MTK_WED_WPDMA_RX_D_PREF_FIFO_CFG_R0_CLR |
-			MTK_WED_WPDMA_RX_D_PREF_FIFO_CFG_R1_CLR);
-		wed_w32(dev, MTK_WED_WPDMA_RX_D_PREF_FIFO_CFG, 0);
+		if (!mtk_wed_is_v3_1(dev->hw)) {
+			wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX,
+				MTK_WED_WPDMA_RX_D_RST_CRX_IDX |
+				MTK_WED_WPDMA_RX_D_RST_DRV_IDX);
+
+			wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX, 0);
+
+			/* reset prefetch index of ring */
+			wed_set(dev, MTK_WED_WPDMA_RX_D_PREF_RX0_SIDX,
+				MTK_WED_WPDMA_RX_D_PREF_SIDX_IDX_CLR);
+			wed_clr(dev, MTK_WED_WPDMA_RX_D_PREF_RX0_SIDX,
+				MTK_WED_WPDMA_RX_D_PREF_SIDX_IDX_CLR);
+
+			wed_set(dev, MTK_WED_WPDMA_RX_D_PREF_RX1_SIDX,
+				MTK_WED_WPDMA_RX_D_PREF_SIDX_IDX_CLR);
+			wed_clr(dev, MTK_WED_WPDMA_RX_D_PREF_RX1_SIDX,
+				MTK_WED_WPDMA_RX_D_PREF_SIDX_IDX_CLR);
+
+			/* reset prefetch FIFO of ring */
+			wed_set(dev, MTK_WED_WPDMA_RX_D_PREF_FIFO_CFG,
+				MTK_WED_WPDMA_RX_D_PREF_FIFO_CFG_R0_CLR |
+				MTK_WED_WPDMA_RX_D_PREF_FIFO_CFG_R1_CLR);
+			wed_w32(dev, MTK_WED_WPDMA_RX_D_PREF_FIFO_CFG, 0);
+		}
 
 		mtk_wed_rx_buffer_hw_init(dev);
-		mtk_wed_rro_hw_init(dev);
+		if (!mtk_wed_is_v3_1(dev->hw))
+			mtk_wed_rro_hw_init(dev);
 		mtk_wed_route_qm_hw_init(dev);
 	}
 
+	if (mtk_wed_is_v3_1(dev->hw))
+		return;
+
 	wed_clr(dev, MTK_WED_TX_BM_CTRL, MTK_WED_TX_BM_CTRL_PAUSE);
 	if (!mtk_wed_is_v1(dev->hw))
 		wed_clr(dev, MTK_WED_TX_TKID_CTRL, MTK_WED_TX_TKID_CTRL_PAUSE);
@@ -1593,80 +1748,108 @@ mtk_wed_rx_reset(struct mtk_wed_device *dev)
 	u8 val = MTK_WED_WO_STATE_SER_RESET;
 	int i, ret;
 
-	ret = mtk_wed_mcu_send_msg(wo, MTK_WED_MODULE_ID_WO,
-				   MTK_WED_WO_CMD_CHANGE_STATE, &val,
-				   sizeof(val), true);
-	if (ret)
-		return ret;
+	if (dev->hw->soc->wo_support) {
+		ret = mtk_wed_mcu_send_msg(wo, MTK_WED_MODULE_ID_WO,
+					   MTK_WED_WO_CMD_CHANGE_STATE, &val,
+					   sizeof(val), true);
 
-	if (dev->wlan.hw_rro) {
+		if (ret)
+			return ret;
+	}
+
+	if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3_1) {
+		/* Disable RRO 3.1 Drv */
+		wed_clr(dev, MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG,
+			MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG_RX_DRV_EN);
+
+		mtk_wed_poll_busy(dev, MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG,
+				  MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG_RX_DRV_BUSY);
+
+		/* Disable RRO 3.1 Prefetch */
+		wed_clr(dev, MTK_WED_WPDMA_RRO3_1_RX_PREF_CFG,
+			MTK_WED_WPDMA_RRO3_1_RX_PREF_EN);
+
+		mtk_wed_poll_busy(dev, MTK_WED_WPDMA_RRO3_1_RX_PREF_CFG,
+				  MTK_WED_WPDMA_RRO3_1_RX_PREF_BUSY);
+
+		/* Reset RRO 3.1 Drv */
+		mtk_wed_reset(dev, MTK_WED_RESET_WPDMA_RRO3_1_RX_D_DRV);
+	} else if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3) {
+		/* Reset RRO IND CMD*/
 		wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_WED_RX_IND_CMD_EN);
 		mtk_wed_poll_busy(dev, MTK_WED_RRO_RX_HW_STS,
 				  MTK_WED_RX_IND_CMD_BUSY);
+		if (mtk_wed_is_v3_1(dev->hw)) {
+			wed_set(dev, MTK_WED_RX_IND_CMD_CNT0, MTK_WED_RX_IND_CMD_DBG_CNT_RST);
+			wed_clr(dev, MTK_WED_RX_IND_CMD_CNT0, MTK_WED_RX_IND_CMD_DBG_CNT_RST);
+		}
 		mtk_wed_reset(dev, MTK_WED_RESET_RRO_RX_TO_PG);
 	}
 
-	wed_clr(dev, MTK_WED_WPDMA_RX_D_GLO_CFG, MTK_WED_WPDMA_RX_D_RX_DRV_EN);
-	ret = mtk_wed_poll_busy(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
-				MTK_WED_WPDMA_RX_D_RX_DRV_BUSY);
-	if (!ret && mtk_wed_is_v3_or_greater(dev->hw))
-		ret = mtk_wed_poll_busy(dev, MTK_WED_WPDMA_RX_D_PREF_CFG,
-					MTK_WED_WPDMA_RX_D_PREF_BUSY);
-	if (ret) {
-		mtk_wed_reset(dev, MTK_WED_RESET_WPDMA_INT_AGENT);
-		mtk_wed_reset(dev, MTK_WED_RESET_WPDMA_RX_D_DRV);
-	} else {
-		if (mtk_wed_is_v3_or_greater(dev->hw)) {
-			/* 1.a. disable prefetch HW */
-			wed_clr(dev, MTK_WED_WPDMA_RX_D_PREF_CFG,
-				MTK_WED_WPDMA_RX_D_PREF_EN);
-			mtk_wed_poll_busy(dev, MTK_WED_WPDMA_RX_D_PREF_CFG,
-					  MTK_WED_WPDMA_RX_D_PREF_BUSY);
-			wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX,
-				MTK_WED_WPDMA_RX_D_RST_DRV_IDX_ALL);
-		}
+	if (!mtk_wed_is_v3_1(dev->hw)) {
+		wed_clr(dev, MTK_WED_WPDMA_RX_D_GLO_CFG, MTK_WED_WPDMA_RX_D_RX_DRV_EN);
+		ret = mtk_wed_poll_busy(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
+					MTK_WED_WPDMA_RX_D_RX_DRV_BUSY);
+		if (!ret && mtk_wed_is_v3(dev->hw))
+			ret = mtk_wed_poll_busy(dev, MTK_WED_WPDMA_RX_D_PREF_CFG,
+						MTK_WED_WPDMA_RX_D_PREF_BUSY);
+		if (ret) {
+			mtk_wed_reset(dev, MTK_WED_RESET_WPDMA_INT_AGENT);
+			mtk_wed_reset(dev, MTK_WED_RESET_WPDMA_RX_D_DRV);
+		} else {
+			if (mtk_wed_is_v3(dev->hw)) {
+				/*1.a. Disable Prefetch HW*/
+				wed_clr(dev, MTK_WED_WPDMA_RX_D_PREF_CFG,
+					MTK_WED_WPDMA_RX_D_PREF_EN);
+				mtk_wed_poll_busy(dev, MTK_WED_WPDMA_RX_D_PREF_CFG,
+						  MTK_WED_WPDMA_RX_D_PREF_BUSY);
+				wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX,
+					MTK_WED_WPDMA_RX_D_RST_DRV_IDX_ALL);
+			}
 
-		wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX,
-			MTK_WED_WPDMA_RX_D_RST_CRX_IDX |
-			MTK_WED_WPDMA_RX_D_RST_DRV_IDX);
+			wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX,
+				MTK_WED_WPDMA_RX_D_RST_CRX_IDX |
+				MTK_WED_WPDMA_RX_D_RST_DRV_IDX);
 
-		wed_set(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
-			MTK_WED_WPDMA_RX_D_RST_INIT_COMPLETE |
-			MTK_WED_WPDMA_RX_D_FSM_RETURN_IDLE);
-		wed_clr(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
-			MTK_WED_WPDMA_RX_D_RST_INIT_COMPLETE |
-			MTK_WED_WPDMA_RX_D_FSM_RETURN_IDLE);
+			wed_set(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
+				MTK_WED_WPDMA_RX_D_RST_INIT_COMPLETE |
+				MTK_WED_WPDMA_RX_D_FSM_RETURN_IDLE);
+			wed_clr(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
+				MTK_WED_WPDMA_RX_D_RST_INIT_COMPLETE |
+				MTK_WED_WPDMA_RX_D_FSM_RETURN_IDLE);
 
-		wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX, 0);
-	}
+			wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX, 0);
+		}
 
-	/* reset rro qm */
-	wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_RX_RRO_QM_EN);
-	ret = mtk_wed_poll_busy(dev, MTK_WED_CTRL,
-				MTK_WED_CTRL_RX_RRO_QM_BUSY);
-	if (ret) {
-		mtk_wed_reset(dev, MTK_WED_RESET_RX_RRO_QM);
-	} else {
-		wed_set(dev, MTK_WED_RROQM_RST_IDX,
-			MTK_WED_RROQM_RST_IDX_MIOD |
-			MTK_WED_RROQM_RST_IDX_FDBK);
-		wed_w32(dev, MTK_WED_RROQM_RST_IDX, 0);
+		/* reset rro qm */
+		wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_RX_RRO_QM_EN);
+		ret = mtk_wed_poll_busy(dev, MTK_WED_CTRL,
+					MTK_WED_CTRL_RX_RRO_QM_BUSY);
+		if (ret) {
+			mtk_wed_reset(dev, MTK_WED_RESET_RX_RRO_QM);
+		} else {
+			wed_set(dev, MTK_WED_RROQM_RST_IDX,
+				MTK_WED_RROQM_RST_IDX_MIOD |
+				MTK_WED_RROQM_RST_IDX_FDBK);
+			wed_w32(dev, MTK_WED_RROQM_RST_IDX, 0);
+		}
 	}
 
 	if (dev->wlan.hw_rro) {
-		/* disable rro msdu page drv */
-		wed_clr(dev, MTK_WED_RRO_MSDU_PG_RING2_CFG,
-			MTK_WED_RRO_MSDU_PG_DRV_EN);
-
+		if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3) {
+			/* disable rro msdu page drv */
+			wed_clr(dev, dev->hw->soc->regmap.msdu_pg_ring2_cfg,
+				MTK_WED_RRO_MSDU_PG_DRV_EN);
+
+			/* rro mdsu page drv reset */
+			wed_w32(dev, dev->hw->soc->regmap.msdu_pg_ring2_cfg,
+				MTK_WED_RRO_MSDU_PG_DRV_CLR);
+			mtk_wed_poll_busy(dev, dev->hw->soc->regmap.msdu_pg_ring2_cfg,
+					  MTK_WED_RRO_MSDU_PG_DRV_CLR);
+		}
 		/* disable rro data drv */
 		wed_clr(dev, MTK_WED_RRO_RX_D_CFG(2), MTK_WED_RRO_RX_D_DRV_EN);
 
-		/* rro msdu page drv reset */
-		wed_w32(dev, MTK_WED_RRO_MSDU_PG_RING2_CFG,
-			MTK_WED_RRO_MSDU_PG_DRV_CLR);
-		mtk_wed_poll_busy(dev, MTK_WED_RRO_MSDU_PG_RING2_CFG,
-				  MTK_WED_RRO_MSDU_PG_DRV_CLR);
-
 		/* rro data drv reset */
 		wed_w32(dev, MTK_WED_RRO_RX_D_CFG(2),
 			MTK_WED_RRO_RX_D_DRV_CLR);
@@ -1678,7 +1861,7 @@ mtk_wed_rx_reset(struct mtk_wed_device *dev)
 	wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_RX_ROUTE_QM_EN);
 	ret = mtk_wed_poll_busy(dev, MTK_WED_CTRL,
 				MTK_WED_CTRL_RX_ROUTE_QM_BUSY);
-	if (ret) {
+	if (ret || mtk_wed_is_v3_1(dev->hw)) {
 		mtk_wed_reset(dev, MTK_WED_RESET_RX_ROUTE_QM);
 	} else if (mtk_wed_is_v3_or_greater(dev->hw)) {
 		wed_set(dev, MTK_WED_RTQM_RST, BIT(0));
@@ -1705,7 +1888,7 @@ mtk_wed_rx_reset(struct mtk_wed_device *dev)
 	ret = mtk_wed_poll_busy(dev, MTK_WED_GLO_CFG,
 				MTK_WED_GLO_CFG_RX_DMA_BUSY);
 	wed_clr(dev, MTK_WED_GLO_CFG, MTK_WED_GLO_CFG_RX_DMA_EN);
-	if (ret) {
+	if (ret || mtk_wed_is_v3_1(dev->hw)) {
 		mtk_wed_reset(dev, MTK_WED_RESET_WED_RX_DMA);
 	} else {
 		wed_set(dev, MTK_WED_RESET_IDX,
@@ -1719,22 +1902,22 @@ mtk_wed_rx_reset(struct mtk_wed_device *dev)
 			  MTK_WED_CTRL_WED_RX_BM_BUSY);
 	mtk_wed_reset(dev, MTK_WED_RESET_RX_BM);
 
-	if (dev->wlan.hw_rro) {
+	if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3) {
 		wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_WED_RX_PG_BM_EN);
 		mtk_wed_poll_busy(dev, MTK_WED_CTRL,
 				  MTK_WED_CTRL_WED_RX_PG_BM_BUSY);
-		wed_set(dev, MTK_WED_RESET, MTK_WED_RESET_RX_PG_BM);
-		wed_clr(dev, MTK_WED_RESET, MTK_WED_RESET_RX_PG_BM);
+		mtk_wed_reset(dev, MTK_WED_RESET_RX_PG_BM);
 	}
 
-	/* wo change to enable state */
-	val = MTK_WED_WO_STATE_ENABLE;
-	ret = mtk_wed_mcu_send_msg(wo, MTK_WED_MODULE_ID_WO,
-				   MTK_WED_WO_CMD_CHANGE_STATE, &val,
-				   sizeof(val), true);
-	if (ret)
-		return ret;
-
+	if (dev->hw->soc->wo_support) {
+		/* wo change to enable state */
+		val = MTK_WED_WO_STATE_ENABLE;
+		ret = mtk_wed_mcu_send_msg(wo, MTK_WED_MODULE_ID_WO,
+					   MTK_WED_WO_CMD_CHANGE_STATE, &val,
+					   sizeof(val), true);
+		if (ret)
+			return ret;
+	}
 	/* wed_rx_ring_reset */
 	for (i = 0; i < ARRAY_SIZE(dev->rx_ring); i++) {
 		if (!dev->rx_ring[i].desc)
@@ -1749,35 +1932,13 @@ mtk_wed_rx_reset(struct mtk_wed_device *dev)
 }
 
 static void
-mtk_wed_reset_dma(struct mtk_wed_device *dev)
+mtk_wed_reset_wdma_rx_dma(struct mtk_wed_device *dev)
 {
 	bool busy = false;
 	u32 val;
-	int i;
-
-	for (i = 0; i < ARRAY_SIZE(dev->tx_ring); i++) {
-		if (!dev->tx_ring[i].desc)
-			continue;
-
-		mtk_wed_ring_reset(&dev->tx_ring[i], MTK_WED_TX_RING_SIZE,
-				   true);
-	}
 
-	/* 1. reset WED tx DMA */
-	wed_clr(dev, MTK_WED_GLO_CFG, MTK_WED_GLO_CFG_TX_DMA_EN);
-	busy = mtk_wed_poll_busy(dev, MTK_WED_GLO_CFG,
-				 MTK_WED_GLO_CFG_TX_DMA_BUSY);
-	if (busy) {
-		mtk_wed_reset(dev, MTK_WED_RESET_WED_TX_DMA);
-	} else {
-		wed_w32(dev, MTK_WED_RESET_IDX,
-			dev->hw->soc->regmap.reset_idx_tx_mask);
-		wed_w32(dev, MTK_WED_RESET_IDX, 0);
-	}
-
-	/* 2. reset WDMA rx DMA */
 	busy = !!mtk_wdma_rx_reset(dev);
-	if (mtk_wed_is_v3_or_greater(dev->hw)) {
+	if (mtk_wed_is_v3(dev->hw)) {
 		val = MTK_WED_WDMA_GLO_CFG_RX_DIS_FSM_AUTO_IDLE |
 		      wed_r32(dev, MTK_WED_WDMA_GLO_CFG);
 		val &= ~MTK_WED_WDMA_GLO_CFG_RX_DRV_EN;
@@ -1798,8 +1959,8 @@ mtk_wed_reset_dma(struct mtk_wed_device *dev)
 		mtk_wed_reset(dev, MTK_WED_RESET_WDMA_INT_AGENT);
 		mtk_wed_reset(dev, MTK_WED_RESET_WDMA_RX_DRV);
 	} else {
-		if (mtk_wed_is_v3_or_greater(dev->hw)) {
-			/* 1.a. disable prefetch HW */
+		if (mtk_wed_is_v3(dev->hw)) {
+			/*1.a. Disable Prefetch HW*/
 			wed_clr(dev, MTK_WED_WDMA_RX_PREF_CFG,
 				MTK_WED_WDMA_RX_PREF_EN);
 			mtk_wed_poll_busy(dev, MTK_WED_WDMA_RX_PREF_CFG,
@@ -1807,7 +1968,7 @@ mtk_wed_reset_dma(struct mtk_wed_device *dev)
 			wed_clr(dev, MTK_WED_WDMA_RX_PREF_CFG,
 				MTK_WED_WDMA_RX_PREF_DDONE2_EN);
 
-			/* Reset prefetch index */
+			/* reset prefetch index */
 			wed_set(dev, MTK_WED_WDMA_RX_PREF_CFG,
 				MTK_WED_WDMA_RX_PREF_RX0_SIDX_CLR |
 				MTK_WED_WDMA_RX_PREF_RX1_SIDX_CLR);
@@ -1816,13 +1977,13 @@ mtk_wed_reset_dma(struct mtk_wed_device *dev)
 				MTK_WED_WDMA_RX_PREF_RX0_SIDX_CLR |
 				MTK_WED_WDMA_RX_PREF_RX1_SIDX_CLR);
 
-			/* Reset prefetch FIFO */
+			/* reset prefetch FIFO */
 			wed_w32(dev, MTK_WED_WDMA_RX_PREF_FIFO_CFG,
 				MTK_WED_WDMA_RX_PREF_FIFO_RX0_CLR |
 				MTK_WED_WDMA_RX_PREF_FIFO_RX1_CLR);
 			wed_w32(dev, MTK_WED_WDMA_RX_PREF_FIFO_CFG, 0);
 
-			/* 2. Reset dma index */
+			/*2. Reset dma index*/
 			wed_w32(dev, MTK_WED_WDMA_RESET_IDX,
 				MTK_WED_WDMA_RESET_IDX_RX_ALL);
 		}
@@ -1837,26 +1998,76 @@ mtk_wed_reset_dma(struct mtk_wed_device *dev)
 		wed_clr(dev, MTK_WED_WDMA_GLO_CFG,
 			MTK_WED_WDMA_GLO_CFG_RST_INIT_COMPLETE);
 	}
+}
 
-	/* 3. reset WED WPDMA tx */
-	wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_WED_TX_FREE_AGENT_EN);
+static void
+mtk_wed_reset_wdma_rx_dma_v3_1(struct mtk_wed_device *dev)
+{
+	bool busy = false;
+	u32 val, status;
+
+	val = MTK_WED_WDMA_GLO_CFG_FSM_RETURN_IDLE |
+	      wed_r32(dev, MTK_WED_WDMA_GLO_CFG);
+	val &= ~MTK_WED_WDMA_GLO_CFG_RX_DRV_EN;
+	wed_w32(dev, MTK_WED_WDMA_GLO_CFG, val);
+
+	busy = mtk_wed_poll_busy(dev, MTK_WED_WDMA_GLO_CFG,
+				 MTK_WED_WDMA_GLO_CFG_RX_DRV_BUSY);
+	busy = mtk_wed_poll_busy(dev, MTK_WED_WDMA_RX_PREF_CFG,
+				 MTK_WED_WDMA_RX_PREF_BUSY);
+	/* Recycle mode settings prepare */
+	wed_clr(dev, MTK_WED_WDMA_GLO_CFG,
+		MTK_WED_WDMA_GLO_CFG_FSM_RETURN_IDLE |
+		MTK_WED_WDMA_GLO_CFG_RECYCLE_DESC);
+
+	val = MTK_WED_WDMA_GLO_CFG_DYNAMIC_SKIP_DMAD_PREP |
+	      wed_r32(dev, MTK_WED_WDMA_GLO_CFG);
+	val &= ~MTK_WED_WDMA_GLO_CFG_DYNAMIC_SKIP_DMAD_THRES;
+	wed_w32(dev, MTK_WED_WDMA_GLO_CFG, val);
+
+	/* Enable WDMA_RX_DRV recycle mode */
+	wed_set(dev, MTK_WED_WDMA_GLO_CFG,
+		MTK_WED_WDMA_GLO_CFG_DYNAMIC_DMAD_RECYCLE |
+		MTK_WED_WDMA_GLO_CFG_RX_DRV_EN);
 
-	for (i = 0; i < 100; i++) {
-		if (mtk_wed_is_v1(dev->hw))
-			val = FIELD_GET(MTK_WED_TX_BM_INTF_TKFIFO_FDEP,
-					wed_r32(dev, MTK_WED_TX_BM_INTF));
-		else
-			val = FIELD_GET(MTK_WED_TX_TKID_INTF_TKFIFO_FDEP,
-					wed_r32(dev, MTK_WED_TX_TKID_INTF));
-		if (val == 0x40)
-			break;
-	}
+	val = MTK_WDMA_GLO_CFG_RX_DMA_BUSY |
+	      MTK_WDMA_GLO_CFG_TX_DMA_BUSY;
 
-	mtk_wed_reset(dev, MTK_WED_RESET_TX_FREE_AGENT);
-	wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_WED_TX_BM_EN);
-	mtk_wed_reset(dev, MTK_WED_RESET_TX_BM);
+	busy = readx_poll_timeout(mtk_wdma_read_reset, dev, status,
+				  !(status & val), 0, 10000);
+
+	wed_clr(dev, MTK_WED_WDMA_GLO_CFG,
+		MTK_WED_WDMA_GLO_CFG_RX_DRV_EN);
+
+	busy = mtk_wed_poll_busy(dev, MTK_WED_WDMA_GLO_CFG,
+				 MTK_WED_WDMA_GLO_CFG_RX_DRV_BUSY);
+	busy = mtk_wed_poll_busy(dev, MTK_WED_WDMA_RX_PREF_CFG,
+				 MTK_WED_WDMA_RX_PREF_BUSY);
+	/* Disable WDMA_RX_DRV recycle mode */
+	val = MTK_WED_WDMA_GLO_CFG_RECYCLE_DESC |
+	      wed_r32(dev, MTK_WED_WDMA_GLO_CFG);
+	val &= ~MTK_WED_WDMA_GLO_CFG_DYNAMIC_DMAD_RECYCLE;
+	wed_w32(dev, MTK_WED_WDMA_GLO_CFG, val);
+
+	val = MTK_WED_WDMA_GLO_CFG_DYNAMIC_SKIP_DMAD_THRES |
+	      wed_r32(dev, MTK_WED_WDMA_GLO_CFG);
+	val &= ~MTK_WED_WDMA_GLO_CFG_DYNAMIC_SKIP_DMAD_PREP;
+	wed_w32(dev, MTK_WED_WDMA_GLO_CFG, val);
+	wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_WDMA_INT_AGENT_EN);
+
+	mtk_wed_poll_busy(dev, MTK_WED_CTRL,
+			  MTK_WED_CTRL_WDMA_INT_AGENT_BUSY);
+	mtk_wed_reset(dev, MTK_WED_RESET_WDMA_INT_AGENT);
+	mtk_wed_reset(dev, MTK_WED_RESET_WDMA_RX_DRV);
+
+	mtk_wdma_rx_reset(dev);
+}
+
+static void
+mtk_wed_wpdma_tx_reset(struct mtk_wed_device *dev)
+{
+	bool busy = false;
 
-	/* 4. reset WED WPDMA tx */
 	busy = mtk_wed_poll_busy(dev, MTK_WED_WPDMA_GLO_CFG,
 				 MTK_WED_WPDMA_GLO_CFG_TX_DRV_BUSY);
 	wed_clr(dev, MTK_WED_WPDMA_GLO_CFG,
@@ -1866,11 +2077,12 @@ mtk_wed_reset_dma(struct mtk_wed_device *dev)
 		busy = mtk_wed_poll_busy(dev, MTK_WED_WPDMA_GLO_CFG,
 					 MTK_WED_WPDMA_GLO_CFG_RX_DRV_BUSY);
 
-	if (busy) {
+	if (busy || mtk_wed_is_v3_1(dev->hw)) {
+		wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_WPDMA_INT_AGENT_EN);
 		mtk_wed_reset(dev, MTK_WED_RESET_WPDMA_INT_AGENT);
 		mtk_wed_reset(dev, MTK_WED_RESET_WPDMA_TX_DRV);
 		mtk_wed_reset(dev, MTK_WED_RESET_WPDMA_RX_DRV);
-		if (mtk_wed_is_v3_or_greater(dev->hw))
+		if (mtk_wed_is_v3(dev->hw))
 			wed_w32(dev, MTK_WED_RX1_CTRL2, 0);
 	} else {
 		wed_w32(dev, MTK_WED_WPDMA_RESET_IDX,
@@ -1878,12 +2090,67 @@ mtk_wed_reset_dma(struct mtk_wed_device *dev)
 			MTK_WED_WPDMA_RESET_IDX_RX);
 		wed_w32(dev, MTK_WED_WPDMA_RESET_IDX, 0);
 	}
+}
+
+static void
+mtk_wed_reset_dma(struct mtk_wed_device *dev)
+{
+	bool busy = false;
+	u32 val;
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(dev->tx_ring); i++) {
+		if (!dev->tx_ring[i].desc)
+			continue;
+
+		mtk_wed_ring_reset(&dev->tx_ring[i], MTK_WED_TX_RING_SIZE,
+				   true);
+	}
+
+	/* 1. reset WED tx DMA */
+	wed_clr(dev, MTK_WED_GLO_CFG, MTK_WED_GLO_CFG_TX_DMA_EN);
+	busy = mtk_wed_poll_busy(dev, MTK_WED_GLO_CFG,
+				 MTK_WED_GLO_CFG_TX_DMA_BUSY);
+	if (busy) {
+		mtk_wed_reset(dev, MTK_WED_RESET_WED_TX_DMA);
+	} else {
+		wed_w32(dev, MTK_WED_RESET_IDX,
+			dev->hw->soc->regmap.reset_idx_tx_mask);
+		wed_w32(dev, MTK_WED_RESET_IDX, 0);
+	}
+
+	/* 2. reset WDMA rx DMA */
+	if (mtk_wed_is_v3_1(dev->hw))
+		mtk_wed_reset_wdma_rx_dma_v3_1(dev);
+	else
+		mtk_wed_reset_wdma_rx_dma(dev);
+
+	/* 3. reset WED TX BM */
+	wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_WED_TX_FREE_AGENT_EN);
+
+	for (i = 0; i < 100; i++) {
+		if (mtk_wed_is_v1(dev->hw))
+			val = FIELD_GET(MTK_WED_TX_BM_INTF_TKFIFO_FDEP,
+					wed_r32(dev, MTK_WED_TX_BM_INTF));
+		else
+			val = FIELD_GET(MTK_WED_TX_TKID_INTF_TKFIFO_FDEP,
+					wed_r32(dev, MTK_WED_TX_TKID_INTF));
+		if (val == 0x40)
+			break;
+	}
+
+	mtk_wed_reset(dev, MTK_WED_RESET_TX_FREE_AGENT);
+	wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_WED_TX_BM_EN);
+	mtk_wed_reset(dev, MTK_WED_RESET_TX_BM);
+
+	/* 4. reset WED WPDMA tx */
+	mtk_wed_wpdma_tx_reset(dev);
 
 	dev->init_done = false;
 	if (mtk_wed_is_v1(dev->hw))
 		return;
 
-	if (!busy) {
+	if (!busy && !mtk_wed_is_v3_1(dev->hw)) {
 		wed_w32(dev, MTK_WED_RESET_IDX, MTK_WED_RESET_WPDMA_IDX_RX);
 		wed_w32(dev, MTK_WED_RESET_IDX, 0);
 	}
@@ -1891,7 +2158,13 @@ mtk_wed_reset_dma(struct mtk_wed_device *dev)
 	if (mtk_wed_is_v3_or_greater(dev->hw)) {
 		/* reset amsdu engine */
 		wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_TX_AMSDU_EN);
+		if (mtk_wed_is_v3_1(dev->hw)) {
+			wed_set(dev, MTK_WED_AMSDU_DBG_CFG, MTK_WED_AMSDU_DBG_CFG_EN);
+			wed_set(dev, MTK_WED_AMSDU_DBG_CNT, MTK_WED_AMSDU_DBG_CNT_CLR);
+			wed_clr(dev, MTK_WED_AMSDU_DBG_CFG, MTK_WED_AMSDU_DBG_CFG_EN);
+		}
 		mtk_wed_reset(dev, MTK_WED_RESET_TX_AMSDU);
+		mtk_wed_wpdma_tx_reset(dev);
 	}
 
 	if (mtk_wed_get_rx_capa(dev))
@@ -2065,15 +2338,23 @@ mtk_wed_configure_irq(struct mtk_wed_device *dev, u32 irq_mask)
 				   dev->wlan.txfree_tbit));
 
 		if (mtk_wed_get_rx_capa(dev)) {
-			wed_w32(dev, MTK_WED_WPDMA_INT_CTRL_RX,
-				MTK_WED_WPDMA_INT_CTRL_RX0_EN |
-				MTK_WED_WPDMA_INT_CTRL_RX0_CLR |
-				MTK_WED_WPDMA_INT_CTRL_RX1_EN |
-				MTK_WED_WPDMA_INT_CTRL_RX1_CLR |
-				FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RX0_DONE_TRIG,
-					   dev->wlan.rx_tbit[0]) |
-				FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RX1_DONE_TRIG,
-					   dev->wlan.rx_tbit[1]));
+			if (mtk_wed_is_v3_1(dev->hw)) {
+				wed_w32(dev, MTK_WED_INT_CTRL,
+					FIELD_PREP(MTK_WED_INT_CTRL_RX0_DONE_TRIG,
+						   dev->wlan.rx_tbit[0]) |
+					FIELD_PREP(MTK_WED_INT_CTRL_RX1_DONE_TRIG,
+						   dev->wlan.rx_tbit[1]));
+			} else {
+				wed_w32(dev, MTK_WED_WPDMA_INT_CTRL_RX,
+					MTK_WED_WPDMA_INT_CTRL_RX0_EN |
+					MTK_WED_WPDMA_INT_CTRL_RX0_CLR |
+					MTK_WED_WPDMA_INT_CTRL_RX1_EN |
+					MTK_WED_WPDMA_INT_CTRL_RX1_CLR |
+					FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RX0_DONE_TRIG,
+						   dev->wlan.rx_tbit[0]) |
+					FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RX1_DONE_TRIG,
+						   dev->wlan.rx_tbit[1]));
+			}
 
 			wdma_mask |= FIELD_PREP(MTK_WDMA_INT_MASK_TX_DONE,
 						GENMASK(1, 0));
@@ -2131,22 +2412,31 @@ mtk_wed_dma_enable(struct mtk_wed_device *dev)
 		return;
 	}
 
-	wed_set(dev, MTK_WED_WPDMA_GLO_CFG,
-		MTK_WED_WPDMA_GLO_CFG_RX_DRV_R0_PKT_PROC |
-		MTK_WED_WPDMA_GLO_CFG_RX_DRV_R0_CRX_SYNC);
+	if (!mtk_wed_is_v3_1(dev->hw))
+		wed_set(dev, MTK_WED_WPDMA_GLO_CFG,
+			MTK_WED_WPDMA_GLO_CFG_RX_DRV_R0_PKT_PROC |
+			MTK_WED_WPDMA_GLO_CFG_RX_DRV_R0_CRX_SYNC);
 
 	if (mtk_wed_is_v3_or_greater(dev->hw)) {
-		wed_set(dev, MTK_WED_WDMA_RX_PREF_CFG,
-			FIELD_PREP(MTK_WED_WDMA_RX_PREF_BURST_SIZE, 0x10) |
-			FIELD_PREP(MTK_WED_WDMA_RX_PREF_LOW_THRES, 0x8));
-		wed_clr(dev, MTK_WED_WDMA_RX_PREF_CFG,
-			MTK_WED_WDMA_RX_PREF_DDONE2_EN);
+		if (mtk_wed_is_v3_1(dev->hw)) {
+			wed_set(dev, MTK_WED_WDMA_RX_PREF_CFG,
+				MTK_WED_WDMA_RX_PREF_DDONE2_EN);
+
+			wed_set(dev, MTK_WED_WPDMA_GLO_CFG,
+				MTK_WED_WPDMA_GLO_CFG_TXD_VER);
+		} else {
+			wed_set(dev, MTK_WED_WDMA_RX_PREF_CFG,
+				FIELD_PREP(MTK_WED_WDMA_RX_PREF_BURST_SIZE, 0x10) |
+				FIELD_PREP(MTK_WED_WDMA_RX_PREF_LOW_THRES, 0x8));
+
+			wed_clr(dev, MTK_WED_WDMA_RX_PREF_CFG,
+				MTK_WED_WDMA_RX_PREF_DDONE2_EN);
+		}
 		wed_set(dev, MTK_WED_WDMA_RX_PREF_CFG, MTK_WED_WDMA_RX_PREF_EN);
 
-		wed_set(dev, MTK_WED_WPDMA_GLO_CFG,
-			MTK_WED_WPDMA_GLO_CFG_TX_DDONE_CHK_LAST);
 		wed_set(dev, MTK_WED_WPDMA_GLO_CFG,
 			MTK_WED_WPDMA_GLO_CFG_TX_DDONE_CHK |
+			MTK_WED_WPDMA_GLO_CFG_TX_DDONE_CHK_LAST |
 			MTK_WED_WPDMA_GLO_CFG_RX_DRV_EVENT_PKT_FMT_CHK |
 			MTK_WED_WPDMA_GLO_CFG_RX_DRV_UNS_VER_FORCE_4);
 
@@ -2165,18 +2455,21 @@ mtk_wed_dma_enable(struct mtk_wed_device *dev)
 		MTK_WED_WDMA_GLO_CFG_TX_DRV_EN |
 		MTK_WED_WDMA_GLO_CFG_TX_DDONE_CHK);
 
-	wed_clr(dev, MTK_WED_WPDMA_RX_D_GLO_CFG, MTK_WED_WPDMA_RX_D_RXD_READ_LEN);
-	wed_set(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
-		MTK_WED_WPDMA_RX_D_RX_DRV_EN |
-		FIELD_PREP(MTK_WED_WPDMA_RX_D_RXD_READ_LEN, 0x18) |
-		FIELD_PREP(MTK_WED_WPDMA_RX_D_INIT_PHASE_RXEN_SEL, 0x2));
+	if (!mtk_wed_is_v3_1(dev->hw)) {
+		wed_clr(dev, MTK_WED_WPDMA_RX_D_GLO_CFG, MTK_WED_WPDMA_RX_D_RXD_READ_LEN);
+		wed_set(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
+			MTK_WED_WPDMA_RX_D_RX_DRV_EN |
+			FIELD_PREP(MTK_WED_WPDMA_RX_D_RXD_READ_LEN, 0x18) |
+			FIELD_PREP(MTK_WED_WPDMA_RX_D_INIT_PHASE_RXEN_SEL, 0x2));
+	}
 
-	if (mtk_wed_is_v3_or_greater(dev->hw)) {
+	if (mtk_wed_is_v3(dev->hw))
 		wed_set(dev, MTK_WED_WPDMA_RX_D_PREF_CFG,
 			MTK_WED_WPDMA_RX_D_PREF_EN |
 			FIELD_PREP(MTK_WED_WPDMA_RX_D_PREF_BURST_SIZE, 0x10) |
 			FIELD_PREP(MTK_WED_WPDMA_RX_D_PREF_LOW_THRES, 0x8));
 
+	if (mtk_wed_is_v3_or_greater(dev->hw)) {
 		wed_set(dev, MTK_WED_RRO_RX_D_CFG(2), MTK_WED_RRO_RX_D_DRV_EN);
 		wdma_set(dev, MTK_WDMA_PREF_TX_CFG, MTK_WDMA_PREF_TX_CFG_PREF_EN);
 		wdma_set(dev, MTK_WDMA_WRBK_TX_CFG, MTK_WDMA_WRBK_TX_CFG_WRBK_EN);
@@ -2189,7 +2482,7 @@ mtk_wed_dma_enable(struct mtk_wed_device *dev)
 		if (!(ring->flags & MTK_WED_RING_CONFIGURED))
 			continue; /* queue is not configured by mt76 */
 
-		if (mtk_wed_check_wfdma_rx_fill(dev, ring)) {
+		if (!mtk_wed_is_v3_1(dev->hw) && mtk_wed_check_wfdma_rx_fill(dev, ring)) {
 			dev_err(dev->hw->dev,
 				"rx_ring(%d) dma enable failed\n", i);
 			continue;
@@ -2216,13 +2509,19 @@ mtk_wed_start_hw_rro(struct mtk_wed_device *dev, u32 irq_mask, bool reset)
 		return;
 
 	if (reset) {
-		wed_set(dev, MTK_WED_RRO_MSDU_PG_RING2_CFG,
-			MTK_WED_RRO_MSDU_PG_DRV_EN);
+		if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3)
+			wed_set(dev, dev->hw->soc->regmap.msdu_pg_ring2_cfg,
+				MTK_WED_RRO_MSDU_PG_DRV_EN);
+		else if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3_1) {
+			wed_set(dev, MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG,
+				MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG_RX_DRV_EN);
+		}
 		return;
 	}
 
-	wed_w32(dev, MTK_WED_RRO_MSDU_PG_RING2_CFG,
-		MTK_WED_RRO_MSDU_PG_DRV_CLR);
+	if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3)
+		wed_w32(dev, dev->hw->soc->regmap.msdu_pg_ring2_cfg,
+			MTK_WED_RRO_MSDU_PG_DRV_CLR);
 
 	wed_w32(dev, MTK_WED_WPDMA_INT_CTRL_RRO_RX,
 		MTK_WED_WPDMA_INT_CTRL_RRO_RX0_EN |
@@ -2234,25 +2533,37 @@ mtk_wed_start_hw_rro(struct mtk_wed_device *dev, u32 irq_mask, bool reset)
 		FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RRO_RX1_DONE_TRIG,
 			   dev->wlan.rro_rx_tbit[1]));
 
-	wed_w32(dev, MTK_WED_WPDMA_INT_CTRL_RRO_MSDU_PG,
-		MTK_WED_WPDMA_INT_CTRL_RRO_PG0_EN |
-		MTK_WED_WPDMA_INT_CTRL_RRO_PG0_CLR |
-		MTK_WED_WPDMA_INT_CTRL_RRO_PG1_EN |
-		MTK_WED_WPDMA_INT_CTRL_RRO_PG1_CLR |
-		MTK_WED_WPDMA_INT_CTRL_RRO_PG2_EN |
-		MTK_WED_WPDMA_INT_CTRL_RRO_PG2_CLR |
-		FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RRO_PG0_DONE_TRIG,
-			   dev->wlan.rx_pg_tbit[0]) |
-		FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RRO_PG1_DONE_TRIG,
-			   dev->wlan.rx_pg_tbit[1]) |
-		FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RRO_PG2_DONE_TRIG,
-			   dev->wlan.rx_pg_tbit[2]));
+	if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3_1)
+		wed_set(dev, MTK_WED_WPDMA_INT_CTRL_RX,
+			MTK_WED_WPDMA_INT_CTRL_RX_RRO_3_1_EN |
+			MTK_WED_WPDMA_INT_CTRL_RX_RRO_3_1_CLR |
+			FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RX_RRO_3_1_TRIG,
+				   dev->wlan.rro_3_1_rx_tbit));
+
+	if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3)
+		wed_w32(dev, MTK_WED_WPDMA_INT_CTRL_RRO_MSDU_PG,
+			MTK_WED_WPDMA_INT_CTRL_RRO_PG0_EN |
+			MTK_WED_WPDMA_INT_CTRL_RRO_PG0_CLR |
+			MTK_WED_WPDMA_INT_CTRL_RRO_PG1_EN |
+			MTK_WED_WPDMA_INT_CTRL_RRO_PG1_CLR |
+			MTK_WED_WPDMA_INT_CTRL_RRO_PG2_EN |
+			MTK_WED_WPDMA_INT_CTRL_RRO_PG2_CLR |
+			FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RRO_PG0_DONE_TRIG,
+				   dev->wlan.rx_pg_tbit[0]) |
+			FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RRO_PG1_DONE_TRIG,
+				   dev->wlan.rx_pg_tbit[1]) |
+			FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RRO_PG2_DONE_TRIG,
+				   dev->wlan.rx_pg_tbit[2]));
 
 	/* RRO_MSDU_PG_RING2_CFG1_FLD_DRV_EN should be enabled after
 	 * WM FWDL completed, otherwise RRO_MSDU_PG ring may broken
 	 */
-	wed_set(dev, MTK_WED_RRO_MSDU_PG_RING2_CFG,
-		MTK_WED_RRO_MSDU_PG_DRV_EN);
+	if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3)
+		wed_set(dev, dev->hw->soc->regmap.msdu_pg_ring2_cfg,
+			MTK_WED_RRO_MSDU_PG_DRV_EN);
+	else if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3_1)
+		wed_set(dev, MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG,
+			MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG_RX_DRV_EN);
 
 	for (i = 0; i < MTK_WED_RX_QUEUES; i++) {
 		struct mtk_wed_ring *ring = &dev->rx_rro_ring[i];
@@ -2268,6 +2579,8 @@ mtk_wed_start_hw_rro(struct mtk_wed_device *dev, u32 irq_mask, bool reset)
 	for (i = 0; i < MTK_WED_RX_PAGE_QUEUES; i++) {
 		struct mtk_wed_ring *ring = &dev->rx_page_ring[i];
 
+		if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3_1)
+			return;
 		if (!(ring->flags & MTK_WED_RING_CONFIGURED))
 			continue;
 
@@ -2288,6 +2601,7 @@ mtk_wed_rro_rx_ring_setup(struct mtk_wed_device *dev, int idx,
 		readl(regs));
 	wed_w32(dev, MTK_WED_RRO_RX_D_RX(idx) + MTK_WED_RING_OFS_COUNT,
 		readl(regs + MTK_WED_RING_OFS_COUNT));
+
 	ring->flags |= MTK_WED_RING_CONFIGURED;
 }
 
@@ -2301,6 +2615,7 @@ mtk_wed_msdu_pg_rx_ring_setup(struct mtk_wed_device *dev, int idx, void __iomem
 		readl(regs));
 	wed_w32(dev, MTK_WED_RRO_MSDU_PG_CTRL0(idx) + MTK_WED_RING_OFS_COUNT,
 		readl(regs + MTK_WED_RING_OFS_COUNT));
+
 	ring->flags |= MTK_WED_RING_CONFIGURED;
 }
 
@@ -2368,6 +2683,21 @@ mtk_wed_ind_rx_ring_setup(struct mtk_wed_device *dev, void __iomem *regs)
 	return 0;
 }
 
+static void
+mtk_wed_rro_3_1_rx_ring_setup(struct mtk_wed_device *dev, void __iomem *regs)
+{
+	struct mtk_wed_ring *ring = &dev->rx_rro_3_1_ring;
+
+	ring->wpdma = regs;
+
+	wed_w32(dev, MTK_WED_WPDMA_RRO3_1_RX_D_RX + MTK_WED_RING_OFS_BASE,
+		readl(regs));
+	wed_w32(dev, MTK_WED_WPDMA_RRO3_1_RX_D_RX + MTK_WED_RING_OFS_COUNT,
+		readl(regs + MTK_WED_RING_OFS_COUNT));
+
+	ring->flags |= MTK_WED_RING_CONFIGURED;
+}
+
 static void
 mtk_wed_start(struct mtk_wed_device *dev, u32 irq_mask)
 {
@@ -2402,7 +2732,7 @@ mtk_wed_start(struct mtk_wed_device *dev, u32 irq_mask)
 
 		val |= BIT(0) | (BIT(1) * !!dev->hw->index);
 		regmap_write(dev->hw->mirror, dev->hw->index * 4, val);
-	} else if (mtk_wed_get_rx_capa(dev)) {
+	} else if (mtk_wed_get_rx_capa(dev) && dev->hw->soc->wo_support) {
 		/* driver set mid ready and only once */
 		wed_w32(dev, MTK_WED_EXT_INT_MASK1,
 			MTK_WED_EXT_INT_STATUS_WPDMA_MID_RDY);
@@ -2412,7 +2742,7 @@ mtk_wed_start(struct mtk_wed_device *dev, u32 irq_mask)
 		wed_r32(dev, MTK_WED_EXT_INT_MASK1);
 		wed_r32(dev, MTK_WED_EXT_INT_MASK2);
 
-		if (mtk_wed_is_v3_or_greater(dev->hw)) {
+		if (mtk_wed_is_v3(dev->hw)) {
 			wed_w32(dev, MTK_WED_EXT_INT_MASK3,
 				MTK_WED_EXT_INT_STATUS_WPDMA_MID_RDY);
 			wed_r32(dev, MTK_WED_EXT_INT_MASK3);
@@ -2460,8 +2790,6 @@ mtk_wed_attach(struct mtk_wed_device *dev)
 	device = dev->wlan.bus_type == MTK_WED_BUS_PCIE
 		? &dev->wlan.pci_dev->dev
 		: &dev->wlan.platform_dev->dev;
-	dev_info(device, "attaching wed device %d version %d\n",
-		 hw->index, hw->version);
 
 	dev->hw = hw;
 	dev->dev = hw->dev;
@@ -2477,6 +2805,11 @@ mtk_wed_attach(struct mtk_wed_device *dev)
 	    of_dma_is_coherent(hw->eth->dev->of_node))
 		mtk_eth_set_dma_device(hw->eth, hw->dev);
 
+	dev_info(device, "attaching wed device %d version %ld.%ld\n",
+		 hw->index,
+		 FIELD_GET(MTK_WED_REV_ID_MAJOR, wed_r32(dev, MTK_WED_REV_ID)),
+		 FIELD_GET(MTK_WED_REV_ID_MINOR, wed_r32(dev, MTK_WED_REV_ID)));
+
 	ret = mtk_wed_tx_buffer_alloc(dev);
 	if (ret)
 		goto out;
@@ -2485,7 +2818,7 @@ mtk_wed_attach(struct mtk_wed_device *dev)
 	if (ret)
 		goto out;
 
-	if (mtk_wed_get_rx_capa(dev)) {
+	if (mtk_wed_get_rx_capa(dev) && dev->hw->soc->wo_support) {
 		ret = mtk_wed_rro_alloc(dev);
 		if (ret)
 			goto out;
@@ -2620,6 +2953,8 @@ mtk_wed_rx_ring_setup(struct mtk_wed_device *dev, int idx, void __iomem *regs,
 	ring->wpdma = regs;
 	ring->flags |= MTK_WED_RING_CONFIGURED;
 
+	if (mtk_wed_is_v3_1(dev->hw))
+		return 0;
 	/* WPDMA ->  WED */
 	wpdma_rx_w32(dev, idx, MTK_WED_RING_OFS_BASE, ring->desc_phys);
 	wpdma_rx_w32(dev, idx, MTK_WED_RING_OFS_COUNT, MTK_WED_RX_RING_SIZE);
@@ -2813,6 +3148,20 @@ mtk_wed_setup_tc(struct mtk_wed_device *wed, struct net_device *dev,
 	}
 }
 
+static int
+mtk_wed_get_hw_version(void)
+{
+	struct mtk_wed_hw *hw = hw_list[0];
+
+	RCU_LOCKDEP_WARN(!rcu_read_lock_held(),
+			 "mtk_wed_get_hw_version without holding the RCU read lock");
+
+	if (!hw)
+		return 0;
+	else
+		return hw->version;
+}
+
 void mtk_wed_add_hw(struct device_node *np, struct mtk_eth *eth,
 		    void __iomem *wdma, phys_addr_t wdma_phy,
 		    int index)
@@ -2832,11 +3181,13 @@ void mtk_wed_add_hw(struct device_node *np, struct mtk_eth *eth,
 		.irq_set_mask = mtk_wed_irq_set_mask,
 		.detach = mtk_wed_detach,
 		.ppe_check = mtk_wed_ppe_check,
+		.get_hw_version = mtk_wed_get_hw_version,
 		.setup_tc = mtk_wed_setup_tc,
 		.start_hw_rro = mtk_wed_start_hw_rro,
 		.rro_rx_ring_setup = mtk_wed_rro_rx_ring_setup,
 		.msdu_pg_rx_ring_setup = mtk_wed_msdu_pg_rx_ring_setup,
 		.ind_rx_ring_setup = mtk_wed_ind_rx_ring_setup,
+		.rro_3_1_rx_ring_setup = mtk_wed_rro_3_1_rx_ring_setup,
 	};
 	struct device_node *eth_np = eth->dev->of_node;
 	struct platform_device *pdev;
@@ -2879,14 +3230,17 @@ void mtk_wed_add_hw(struct device_node *np, struct mtk_eth *eth,
 	hw->wdma = wdma;
 	hw->index = index;
 	hw->irq = irq;
-	hw->version = eth->soc->version;
 
-	switch (hw->version) {
+	switch (eth->soc->version) {
 	case 2:
 		hw->soc = &mt7986_data;
+		hw->version = MTK_WED_HW_V2;
 		break;
 	case 3:
-		hw->soc = &mt7988_data;
+		hw->soc = MTK_HAS_CAPS(eth->soc->caps, MT7988_CAPS) ?
+			  &mt7988_data : &mt7987_data;
+		hw->version = MTK_HAS_CAPS(eth->soc->caps, MT7988_CAPS) ?
+				MTK_WED_HW_V3 : MTK_WED_HW_V3_1;
 		break;
 	default:
 	case 1:
@@ -2894,6 +3248,7 @@ void mtk_wed_add_hw(struct device_node *np, struct mtk_eth *eth,
 				"mediatek,pcie-mirror");
 		hw->hifsys = syscon_regmap_lookup_by_phandle(eth_np,
 				"mediatek,hifsys");
+		hw->version = MTK_WED_HW_V1;
 		if (IS_ERR(hw->mirror) || IS_ERR(hw->hifsys)) {
 			kfree(hw);
 			goto unlock;
diff --git a/drivers/net/ethernet/mediatek/mtk_wed.h b/drivers/net/ethernet/mediatek/mtk_wed.h
index 56a5c16..13c9520 100644
--- a/drivers/net/ethernet/mediatek/mtk_wed.h
+++ b/drivers/net/ethernet/mediatek/mtk_wed.h
@@ -22,9 +22,11 @@ struct mtk_wed_soc_data {
 		u32 wpdma_rx_ring1;
 		u32 reset_idx_tx_mask;
 		u32 reset_idx_rx_mask;
+		u32 msdu_pg_ring2_cfg;
 	} regmap;
 	u32 tx_ring_desc_size;
 	u32 wdma_desc_size;
+	u8 wo_support;
 };
 
 struct mtk_wed_amsdu {
@@ -67,22 +69,27 @@ struct mtk_wdma_info {
 #ifdef CONFIG_NET_MEDIATEK_SOC_WED
 static inline bool mtk_wed_is_v1(struct mtk_wed_hw *hw)
 {
-	return hw->version == 1;
+	return hw->version == MTK_WED_HW_V1;
 }
 
 static inline bool mtk_wed_is_v2(struct mtk_wed_hw *hw)
 {
-	return hw->version == 2;
+	return hw->version == MTK_WED_HW_V2;
 }
 
 static inline bool mtk_wed_is_v3(struct mtk_wed_hw *hw)
 {
-	return hw->version == 3;
+	return hw->version == MTK_WED_HW_V3;
 }
 
 static inline bool mtk_wed_is_v3_or_greater(struct mtk_wed_hw *hw)
 {
-	return hw->version > 2;
+	return hw->version > MTK_WED_HW_V2;
+}
+
+static inline bool mtk_wed_is_v3_1(struct mtk_wed_hw *hw)
+{
+	return hw->version == MTK_WED_HW_V3_1;
 }
 
 static inline void
@@ -169,7 +176,7 @@ wpdma_txfree_w32(struct mtk_wed_device *dev, u32 reg, u32 val)
 
 static inline u32 mtk_wed_get_pcie_base(struct mtk_wed_device *dev)
 {
-	if (!mtk_wed_is_v3_or_greater(dev->hw))
+	if (!mtk_wed_is_v3(dev->hw))
 		return MTK_WED_PCIE_BASE;
 
 	switch (dev->hw->index) {
diff --git a/drivers/net/ethernet/mediatek/mtk_wed_debugfs.c b/drivers/net/ethernet/mediatek/mtk_wed_debugfs.c
index 57c56e0..4a5c1c3 100644
--- a/drivers/net/ethernet/mediatek/mtk_wed_debugfs.c
+++ b/drivers/net/ethernet/mediatek/mtk_wed_debugfs.c
@@ -262,7 +262,8 @@ wed_txinfo_show(struct seq_file *s, void *data)
 	if (dev) {
 		dump_wed_regs(s, dev, regs, ARRAY_SIZE(regs));
 		switch(dev->hw->version) {
-		case 3:
+		case MTK_WED_HW_V3:
+		case MTK_WED_HW_V3_1:
 			dump_wed_regs(s, dev, regs_v3, ARRAY_SIZE(regs_v3));
 			break;
 		default:
@@ -434,17 +435,62 @@ wed_rxinfo_show(struct seq_file *s, void *data)
 		DUMP_WED(WED_RTQM_DEQ_USED_PFDBK_CNT),
 		DUMP_WED(WED_RTQM_DEQ_ERR_CNT),
 	};
+
+	static const struct reg_dump regs_wed_v3_1[] = {
+		DUMP_STR("WED RX INT info"),
+		DUMP_WED(WED_PCIE_INT_CTRL),
+		DUMP_WED(WED_PCIE_INT_REC),
+		DUMP_WED(WED_WPDMA_INT_STA_REC),
+		DUMP_WED(WED_WPDMA_INT_MON),
+		DUMP_WED(WED_WPDMA_INT_CTRL),
+		DUMP_WED(WED_WPDMA_INT_CTRL_TX),
+		DUMP_WED(WED_WPDMA_INT_CTRL_RX),
+		DUMP_WED(WED_WPDMA_INT_CTRL_TX_FREE),
+		DUMP_WED(WED_WPDMA_STATUS),
+		DUMP_WED(WED_WPDMA_D_ST),
+
+		DUMP_STR("WED RX"),
+		DUMP_WED_RING_RX_TYPE2(WED_RING_RX_DATA(0)),
+
+		DUMP_STR("WED WPDMA RRO3.1 RX"),
+		DUMP_WED(WED_WPDMA_RRO3_1_RX_D_RX_MIB),
+		DUMP_WED_RING_RX_TYPE1(WED_WPDMA_RRO3_1_RX_D_RX),
+		DUMP_STR("WED WDMA TX"),
+		DUMP_WED_RING(WED_WDMA_RING_TX),
+		DUMP_WED(WED_WDMA_TX_MIB),
+
+		DUMP_STR("WDMA TX"),
+		DUMP_WDMA(WDMA_GLO_CFG),
+		DUMP_WDMA_RING(WDMA_RING_TX(0)),
+		DUMP_WDMA_RING(WDMA_RING_TX(1)),
+
+		DUMP_STR("WED RX BM"),
+		DUMP_WED(WED_RX_BM_BASE),
+		DUMP_WED(WED_RX_BM_PTR),
+		DUMP_WED_MASK(WED_RX_BM_PTR, WED_RX_BM_PTR_HEAD),
+		DUMP_WED_MASK(WED_RX_BM_PTR, WED_RX_BM_PTR_TAIL),
+	};
+
 	struct mtk_wed_hw *hw = s->private;
 	struct mtk_wed_device *dev = hw->wed_dev;
 
 	if (dev) {
-		dump_wed_regs(s, dev, regs_common, ARRAY_SIZE(regs_common));
 		switch(dev->hw->version) {
-		case 2:
+		case MTK_WED_HW_V2:
+			dump_wed_regs(s, dev,
+				      regs_common, ARRAY_SIZE(regs_common));
 			dump_wed_regs(s, dev,
 				      regs_wed_v2, ARRAY_SIZE(regs_wed_v2));
 			break;
-		case 3:
+		case MTK_WED_HW_V3:
+			dump_wed_regs(s, dev,
+				      regs_common, ARRAY_SIZE(regs_common));
+			dump_wed_regs(s, dev,
+				      regs_wed_v3, ARRAY_SIZE(regs_wed_v3));
+			break;
+		case MTK_WED_HW_V3_1:
+			dump_wed_regs(s, dev,
+				      regs_wed_v3_1, ARRAY_SIZE(regs_wed_v3_1));
 			dump_wed_regs(s, dev,
 				      regs_wed_v3, ARRAY_SIZE(regs_wed_v3));
 			break;
diff --git a/drivers/net/ethernet/mediatek/mtk_wed_mcu.c b/drivers/net/ethernet/mediatek/mtk_wed_mcu.c
index c698a74..9535c8f 100644
--- a/drivers/net/ethernet/mediatek/mtk_wed_mcu.c
+++ b/drivers/net/ethernet/mediatek/mtk_wed_mcu.c
@@ -234,7 +234,7 @@ int mtk_wed_mcu_msg_update(struct mtk_wed_device *dev, int id, void *data,
 {
 	struct mtk_wed_wo *wo = dev->hw->wed_wo;
 
-	if (!mtk_wed_get_rx_capa(dev))
+	if (!mtk_wed_get_rx_capa(dev) || !dev->hw->soc->wo_support)
 		return 0;
 
 	if (WARN_ON(!wo))
diff --git a/drivers/net/ethernet/mediatek/mtk_wed_regs.h b/drivers/net/ethernet/mediatek/mtk_wed_regs.h
index 15676e7..54c5333 100644
--- a/drivers/net/ethernet/mediatek/mtk_wed_regs.h
+++ b/drivers/net/ethernet/mediatek/mtk_wed_regs.h
@@ -33,6 +33,7 @@ struct mtk_wdma_desc {
 #define MTK_WED_RESET_RX_PG_BM				BIT(2)
 #define MTK_WED_RESET_RRO_RX_TO_PG			BIT(3)
 #define MTK_WED_RESET_TX_FREE_AGENT			BIT(4)
+#define MTK_WED_RESET_WPDMA_RRO3_1_RX_D_DRV		BIT(7)
 #define MTK_WED_RESET_WPDMA_TX_DRV			BIT(8)
 #define MTK_WED_RESET_WPDMA_RX_DRV			BIT(9)
 #define MTK_WED_RESET_WPDMA_RX_D_DRV			BIT(10)
@@ -166,7 +167,7 @@ struct mtk_wdma_desc {
 #define MTK_WED_TX_TKID_DYN_THR				0x0e0
 #define MTK_WED_TX_TKID_DYN_THR_LO			GENMASK(6, 0)
 #define MTK_WED_TX_TKID_DYN_THR_HI			GENMASK(22, 16)
-
+#define MTK_WED_TX_TKID_DYN_THR_HI_V3		GENMASK(23, 16)
 #define MTK_WED_TX_TKID_STATUS				0x0e4
 #define MTK_WED_TX_TKID_RECYC				0x0e8
 
@@ -256,6 +257,7 @@ struct mtk_wdma_desc {
 #define MTK_WED_WPDMA_GLO_CFG_RX_DDONE2_WR		BIT(21)
 #define MTK_WED_WPDMA_GLO_CFG_TX_TKID_KEEP		BIT(24)
 #define MTK_WED_WPDMA_GLO_CFG_TX_DDONE_CHK_LAST		BIT(25)
+#define MTK_WED_WPDMA_GLO_CFG_TXD_VER				BIT(26)
 #define MTK_WED_WPDMA_GLO_CFG_TX_DMAD_DW3_PREV		BIT(28)
 #define MTK_WED_WPDMA_GLO_CFG_TX_DDONE_CHK		BIT(30)
 
@@ -289,14 +291,16 @@ struct mtk_wdma_desc {
 #define MTK_WED_WPDMA_INT_CTRL_RX1_EN			BIT(8)
 #define MTK_WED_WPDMA_INT_CTRL_RX1_CLR			BIT(9)
 #define MTK_WED_WPDMA_INT_CTRL_RX1_DONE_TRIG		GENMASK(14, 10)
+#define MTK_WED_WPDMA_INT_CTRL_RX_RRO_3_1_EN		BIT(15)
+#define MTK_WED_WPDMA_INT_CTRL_RX_RRO_3_1_CLR		BIT(16)
+#define MTK_WED_WPDMA_INT_CTRL_RX_RRO_3_1_TRIG		GENMASK(21, 17)
 
 #define MTK_WED_WPDMA_INT_CTRL_TX_FREE			0x538
 #define MTK_WED_WPDMA_INT_CTRL_TX_FREE_DONE_EN		BIT(0)
 #define MTK_WED_WPDMA_INT_CTRL_TX_FREE_DONE_CLR		BIT(1)
 #define MTK_WED_WPDMA_INT_CTRL_TX_FREE_DONE_TRIG	GENMASK(6, 2)
 
-#define MTK_WED_PCIE_CFG_BASE				0x560
-
+#define MTK_WED_PCIE_INT_CLR				0x550
 #define MTK_WED_PCIE_CFG_BASE				0x560
 #define MTK_WED_PCIE_CFG_INTM				0x564
 #define MTK_WED_PCIE_CFG_MSIS				0x568
@@ -326,6 +330,7 @@ struct mtk_wdma_desc {
 
 
 #define MTK_WED_WPDMA_RING_TX(_n)			(0x600 + (_n) * 0x10)
+#define MTK_WED_WPDMA_RING_TX_BASE_PTR_H		GENMASK(23, 16)
 #define MTK_WED_WPDMA_RING_RX(_n)			(0x700 + (_n) * 0x10)
 #define MTK_WED_WPDMA_RING_RX_DATA(_n)			(0x730 + (_n) * 0x10)
 
@@ -394,6 +399,7 @@ struct mtk_wdma_desc {
 #define MTK_WED_WDMA_GLO_CFG_TX_DDONE_CHK		BIT(1)
 #define MTK_WED_WDMA_GLO_CFG_RX_DRV_EN			BIT(2)
 #define MTK_WED_WDMA_GLO_CFG_RX_DRV_BUSY		BIT(3)
+#define MTK_WED_WDMA_GLO_CFG_RECYCLE_DESC		BIT(4)
 #define MTK_WED_WDMA_GLO_CFG_BT_SIZE			GENMASK(5, 4)
 #define MTK_WED_WDMA_GLO_CFG_TX_WB_DDONE		BIT(6)
 #define MTK_WED_WDMA_GLO_CFG_RX_DIS_FSM_AUTO_IDLE	BIT(13)
@@ -408,6 +414,7 @@ struct mtk_wdma_desc {
 #define MTK_WED_WDMA_GLO_CFG_DYNAMIC_SKIP_DMAD_PREP	BIT(24)
 #define MTK_WED_WDMA_GLO_CFG_DYNAMIC_DMAD_RECYCLE	BIT(25)
 #define MTK_WED_WDMA_GLO_CFG_RST_INIT_COMPLETE		BIT(26)
+#define MTK_WED_WDMA_GLO_CFG_DYNAMIC_SKIP_DMAD_THRES	BIT(28)
 #define MTK_WED_WDMA_GLO_CFG_RXDRV_CLKGATE_BYPASS	BIT(30)
 
 #define MTK_WED_WDMA_RESET_IDX				0xa08
@@ -444,6 +451,7 @@ struct mtk_wdma_desc {
 
 #define MTK_WED_RX_BM_RX_DMAD				0xd80
 #define MTK_WED_RX_BM_RX_DMAD_SDL0			GENMASK(13, 0)
+#define MTK_WED_RX_BM_RX_DMAD_BASE_PTR_H		GENMASK(23, 16)
 
 #define MTK_WED_RX_BM_BASE				0xd84
 #define MTK_WED_RX_BM_INIT_PTR				0xd88
@@ -720,12 +728,14 @@ struct mtk_wdma_desc {
 #define MTK_WED_RRO_MSDU_PG_DRV_EN			BIT(31)
 
 #define MTK_WED_RRO_MSDU_PG_CTRL0(_n)			(0xe5c + (_n) * 0xc)
+#define MTK_WED_RRO_MSDU_PG_BASE_PTR_H			GENMASK(23, 16)
 #define MTK_WED_RRO_MSDU_PG_CTRL1(_n)			(0xe60 + (_n) * 0xc)
 #define MTK_WED_RRO_MSDU_PG_CTRL2(_n)			(0xe64 + (_n) * 0xc)
 
 #define MTK_WED_RRO_RX_D_RX(_n)				(0xe80 + (_n) * 0x10)
 #define MTK_WED_RRO_RX_D_RX_CNT(_n)			(0xe84 + (_n) * 0x10)
 #define MTK_WED_RRO_RX_D_RX_MAX_CNT			GENMASK(11, 0)
+#define MTK_WED_RRO_RX_D_RX_BASE_PTR_H			GENMASK(23, 16)
 #define MTK_WED_RRO_RX_D_RX_MAGIC_CNT			GENMASK(31, 28)
 
 #define MTK_WED_RRO_RX_MAGIC_CNT			BIT(13)
@@ -736,6 +746,7 @@ struct mtk_wdma_desc {
 
 #define MTK_WED_RRO_PG_BM_RX_DMAM			0xeb0
 #define MTK_WED_RRO_PG_BM_RX_SDL0			GENMASK(13, 0)
+#define MTK_WED_RRO_PG_BM_RX_BASE_PTR_H			GENMASK(23, 16)
 
 #define MTK_WED_RRO_PG_BM_BASE				0xeb4
 #define MTK_WED_RRO_PG_BM_INIT_PTR			0xeb8
@@ -774,6 +785,7 @@ struct mtk_wdma_desc {
 #define MTK_WED_RX_IND_CMD_BUSY				GENMASK(31, 0)
 
 #define MTK_WED_RX_IND_CMD_CNT0				0xf20
+#define MTK_WED_RX_IND_CMD_DBG_CNT_RST			BIT(30)
 #define MTK_WED_RX_IND_CMD_DBG_CNT_EN			BIT(31)
 
 #define MTK_WED_RX_IND_CMD_CNT(_n)			(0xf20 + (_n) * 0x4)
@@ -868,9 +880,48 @@ struct mtk_wdma_desc {
 
 #define MTK_WED_MON_AMSDU_HIFTXD_FETCH_BUFF(_n)		(0x1e90 + (_n - 1) * 0x4)
 #define MTK_WED_MON_AMSDU_HIFTXD_FETCH_MSDU(_n)		(0x1ec4 + (_n - 1) * 0x4)
+#define MTK_WED_AMSDU_DBG_CFG				0x1fe0
+#define MTK_WED_AMSDU_DBG_CFG_EN			BIT(18)
+#define MTK_WED_AMSDU_DBG_CNT				0x1fe4
+#define MTK_WED_AMSDU_DBG_CNT_CLR			BIT(0)
 
 #define MTK_WED_PCIE_BASE			0x11280000
 #define MTK_WED_PCIE_BASE0			0x11300000
 #define MTK_WED_PCIE_BASE1			0x11310000
 #define MTK_WED_PCIE_BASE2			0x11290000
+
+/* wed 3.1*/
+#define MTK_WED_INT_CTRL					0x218
+#define MTK_WED_INT_CTRL_RX0_DONE_TRIG		GENMASK(4, 0)
+#define MTK_WED_INT_CTRL_RX1_DONE_TRIG		GENMASK(12, 8)
+#define MTK_WED_HIFTXD_BASE_L(_n)		(0x320 + (_n) * 0x4)
+#define MTK_WED_TX_BM_RANGE_CFG			0x098
+#define MTK_WED_WPDMA_CFG_TX0_CIDX		0x588
+#define MTK_WED_WPDMA_CFG_TX_FREE_CIDX		0x58c
+#define MTK_WED_WPDMA_CFG_TX_FREE_DIDX		0x594
+#define MTK_WED_WPDMA_CFG_TX1_CIDX		0x5A0
+#define MTK_WED_WPDMA_CFG_TX0_DIDX		0x5b0
+#define MTK_WED_WPDMA_CFG_TX1_DIDX		0x5cc
+#define MTK_WED_WPDMA_RRO3_1_RX_D_RX		0x814
+#define MTK_WED_WPDMA_RRO3_1_RX_D_RX_BASE_PTR_H		GENMASK(23, 16)
+#define MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG		0x824
+#define MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG_RX_DRV_EN	BIT(0)
+#define MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG_RX_DRV_BUSY	BIT(1)
+#define MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG_BASE	0x834
+#define MTK_WED_WPDMA_RRO3_1_RX_D_RX_MIB	0x840
+#define MTK_WED_WPDMA_RRO3_1_RX_PREF_CFG	0x850
+#define MTK_WED_WPDMA_RRO3_1_RX_PREF_EN		BIT(0)
+#define MTK_WED_WPDMA_RRO3_1_RX_PREF_BUSY	BIT(1)
+#define MTK_WED_WPDMA_RRO3_1_RX_RING_CIDX	0x864
+#define MTK_WED_WPDMA_RRO3_1_RX_RING_DIDX	0x888
+#define MTK_WED_RRO_CTRL			0x8fc
+#define MTK_WED_RRO_CTRL_VER_SEL		BIT(0)
+#define MTK_WED_WPDMA_RRO_RX_RING_CIDX(_n)	(0xea0 + (_n) * 0x4)
+#define MTK_WED_WPDMA_RRO_RX_RING_DIDX(_n)	(0xef8 + (_n) * 0x4)
+#define MTK_WED_RRO_MSDU_PG_RING_CFG_CIDX(_n)	(0xde8 + (_n) * 0x8)
+#define MTK_WED_RX_BM_RANGE_CFG			0xda0
+#define MTK_WED_RX_BM_RANGE_SW_BUF		GENMASK(15, 0)
+#define MTK_WED_RRO_MSDU_PG_RING_CFG_DIDX(_n)	(0xe44 + (_n) * 0x8)
+#define MTK_WED_RRO_PG_BM_RANGE_CFG		0xec8
+#define MTK_WED_RRO_PG_BM_RANGE_SW_BUF		GENMASK(15, 0)
 #endif
diff --git a/drivers/net/ethernet/mediatek/mtk_wed_wo.c b/drivers/net/ethernet/mediatek/mtk_wed_wo.c
index d58b07e..c578ce2 100644
--- a/drivers/net/ethernet/mediatek/mtk_wed_wo.c
+++ b/drivers/net/ethernet/mediatek/mtk_wed_wo.c
@@ -478,6 +478,9 @@ int mtk_wed_wo_init(struct mtk_wed_hw *hw)
 	struct mtk_wed_wo *wo;
 	int ret;
 
+	if (!hw->soc->wo_support)
+		return 0;
+
 	wo = devm_kzalloc(hw->dev, sizeof(*wo), GFP_KERNEL);
 	if (!wo)
 		return -ENOMEM;
@@ -500,5 +503,8 @@ void mtk_wed_wo_deinit(struct mtk_wed_hw *hw)
 {
 	struct mtk_wed_wo *wo = hw->wed_wo;
 
+	if(!hw->soc->wo_support || !wo)
+		return;
+
 	mtk_wed_wo_hw_deinit(wo);
 }
diff --git a/include/linux/soc/mediatek/mtk_wed.h b/include/linux/soc/mediatek/mtk_wed.h
index 5ab3a93..7b40626 100644
--- a/include/linux/soc/mediatek/mtk_wed.h
+++ b/include/linux/soc/mediatek/mtk_wed.h
@@ -56,6 +56,20 @@ enum mtk_wed_bus_tye {
 	MTK_WED_BUS_AXI,
 };
 
+enum mtk_wed_hwrro_mode {
+	MTK_WED_HWRRO_DISABLE,
+	MTK_WED_HWRRO_V3,
+	MTK_WED_HWRRO_V3_1,
+};
+
+enum mtk_wed_hw_version {
+	MTK_WED_DISABLE,
+	MTK_WED_HW_V1,
+	MTK_WED_HW_V2,
+	MTK_WED_HW_V3,
+	MTK_WED_HW_V3_1 = 5,
+};
+
 #define MTK_WED_RING_CONFIGURED		BIT(0)
 struct mtk_wed_ring {
 	struct mtk_wdma_desc *desc;
@@ -103,6 +117,7 @@ struct mtk_wed_device {
 	struct mtk_wed_ring rx_rro_ring[MTK_WED_RX_QUEUES];
 	struct mtk_wed_ring rx_page_ring[MTK_WED_RX_PAGE_QUEUES];
 	struct mtk_wed_ring ind_cmd_ring;
+	struct mtk_wed_ring rx_rro_3_1_ring;
 
 	struct {
 		int size;
@@ -144,15 +159,16 @@ struct mtk_wed_device {
 		u32 wpdma_phys;
 		u32 wpdma_int;
 		u32 wpdma_mask;
-		u32 wpdma_tx;
+		u32 wpdma_tx[MTK_WED_TX_QUEUES];
 		u32 wpdma_txfree;
 		u32 wpdma_rx_glo;
 		u32 wpdma_rx[MTK_WED_RX_QUEUES];
 		u32 wpdma_rx_rro[MTK_WED_RX_QUEUES];
 		u32 wpdma_rx_pg;
+		u32 wpdma_rro_3_1_rx;
 
 		bool wcid_512;
-		bool hw_rro;
+		enum mtk_wed_hwrro_mode hw_rro;
 		bool msi;
 
 		u16 token_start;
@@ -167,6 +183,7 @@ struct mtk_wed_device {
 		u8 rro_rx_tbit[MTK_WED_RX_QUEUES];
 		u8 rx_pg_tbit[MTK_WED_RX_PAGE_QUEUES];
 		u8 txfree_tbit;
+		u8 rro_3_1_rx_tbit;
 		u8 amsdu_max_subframes;
 
 		struct {
@@ -216,6 +233,7 @@ struct mtk_wed_ops {
 	void (*irq_set_mask)(struct mtk_wed_device *dev, u32 mask);
 	int (*setup_tc)(struct mtk_wed_device *wed, struct net_device *dev,
 			enum tc_setup_type type, void *type_data);
+	int (*get_hw_version)(void);
 	void (*start_hw_rro)(struct mtk_wed_device *dev, u32 irq_mask,
 			     bool reset);
 	void (*rro_rx_ring_setup)(struct mtk_wed_device *dev, int ring,
@@ -224,6 +242,8 @@ struct mtk_wed_ops {
 				      void __iomem *regs);
 	int (*ind_rx_ring_setup)(struct mtk_wed_device *dev,
 				 void __iomem *regs);
+	void (*rro_3_1_rx_ring_setup)(struct mtk_wed_device *dev,
+				      void __iomem *regs);
 };
 
 extern const struct mtk_wed_ops __rcu *mtk_soc_wed_ops;
@@ -248,13 +268,36 @@ mtk_wed_device_attach(struct mtk_wed_device *dev)
 	return ret;
 }
 
+static inline int
+mtk_wed_device_get_hw_version(void)
+{
+	int ret = MTK_WED_DISABLE;
+	const struct mtk_wed_ops *ops;
+#ifdef CONFIG_NET_MEDIATEK_SOC_WED
+	rcu_read_lock();
+	ops = rcu_dereference(mtk_soc_wed_ops);
+	if (ops)
+		ret = ops->get_hw_version();
+
+	rcu_read_unlock();
+#endif
+
+	return ret;
+}
+
 static inline bool mtk_wed_get_rx_capa(struct mtk_wed_device *dev)
 {
 #ifdef CONFIG_NET_MEDIATEK_SOC_WED
-	if (dev->version == 3)
+	switch (dev->version) {
+	case MTK_WED_HW_V3:
+	case MTK_WED_HW_V3_1:
 		return dev->wlan.hw_rro;
-
-	return dev->version != 1;
+	case MTK_WED_HW_V2:
+		return true;
+	case MTK_WED_HW_V1:
+	default:
+		return false;
+	}
 #else
 	return false;
 #endif
@@ -263,7 +306,7 @@ static inline bool mtk_wed_get_rx_capa(struct mtk_wed_device *dev)
 static inline bool mtk_wed_is_amsdu_supported(struct mtk_wed_device *dev)
 {
 #ifdef CONFIG_NET_MEDIATEK_SOC_WED
-	return dev->version == 3;
+	return dev->version > MTK_WED_HW_V2;
 #else
 	return false;
 #endif
@@ -303,6 +346,8 @@ static inline bool mtk_wed_is_amsdu_supported(struct mtk_wed_device *dev)
 	(_dev)->ops->msdu_pg_rx_ring_setup(_dev, _ring, _regs)
 #define mtk_wed_device_ind_rx_ring_setup(_dev, _regs) \
 	(_dev)->ops->ind_rx_ring_setup(_dev, _regs)
+#define mtk_wed_device_rro_3_1_rx_ring_setup(_dev, _regs) \
+	(_dev)->ops->rro_3_1_rx_ring_setup(_dev, _regs)
 
 #else
 static inline bool mtk_wed_device_active(struct mtk_wed_device *dev)
@@ -327,6 +372,7 @@ static inline bool mtk_wed_device_active(struct mtk_wed_device *dev)
 #define mtk_wed_device_rro_rx_ring_setup(_dev, _ring, _regs) -ENODEV
 #define mtk_wed_device_msdu_pg_rx_ring_setup(_dev, _ring, _regs) -ENODEV
 #define mtk_wed_device_ind_rx_ring_setup(_dev, _regs) -ENODEV
+#define mtk_wed_device_rro_3_1_rx_ring_setup(_dev, _regs) -ENODEV
 #endif
 
 #endif
-- 
2.45.2

