From 38ed7691ba0892df5ff4751a3d2b3a3d4ba4ccfb Mon Sep 17 00:00:00 2001
From: Rex Lu <rex.lu@mediatek.com>
Date: Mon, 21 Oct 2024 16:19:03 +0800
Subject: [PATCH] add mt7987 hwpath support

---
 drivers/net/ethernet/mediatek/mtk_eth_soc.c   |  10 +
 drivers/net/ethernet/mediatek/mtk_eth_soc.h   |   1 +
 drivers/net/ethernet/mediatek/mtk_wed.c       | 809 +++++++++++++-----
 drivers/net/ethernet/mediatek/mtk_wed.h       |  18 +-
 .../net/ethernet/mediatek/mtk_wed_debugfs.c   |  52 +-
 drivers/net/ethernet/mediatek/mtk_wed_mcu.c   |   2 +-
 drivers/net/ethernet/mediatek/mtk_wed_regs.h  |  55 +-
 drivers/net/ethernet/mediatek/mtk_wed_wo.c    |   5 +-
 include/linux/soc/mediatek/mtk_wed.h          |  59 +-
 9 files changed, 759 insertions(+), 252 deletions(-)

diff --git a/drivers/net/ethernet/mediatek/mtk_eth_soc.c b/drivers/net/ethernet/mediatek/mtk_eth_soc.c
index ca993e6..a7bf38d 100644
--- a/drivers/net/ethernet/mediatek/mtk_eth_soc.c
+++ b/drivers/net/ethernet/mediatek/mtk_eth_soc.c
@@ -6790,6 +6790,7 @@ static const struct mtk_soc_data mt2701_data = {
 	.required_pctl = true,
 	.has_sram = false,
 	.offload_version = 1,
+	.version = 1,
 	.hash_offset = 2,
 	.has_accounting = false,
 	.foe_entry_size = MTK_FOE_ENTRY_V1_SIZE,
@@ -6815,6 +6816,7 @@ static const struct mtk_soc_data mt7621_data = {
 	.has_sram = false,
 	.offload_version = 1,
 	.hash_offset = 2,
+	.version = 1,
 	.has_accounting = false,
 	.foe_entry_size = MTK_FOE_ENTRY_V1_SIZE,
 	.rss_num = 0,
@@ -6839,6 +6841,7 @@ static const struct mtk_soc_data mt7622_data = {
 	.required_pctl = false,
 	.has_sram = false,
 	.offload_version = 2,
+	.version = 1,
 	.hash_offset = 2,
 	.has_accounting = true,
 	.foe_entry_size = MTK_FOE_ENTRY_V1_SIZE,
@@ -6864,6 +6867,7 @@ static const struct mtk_soc_data mt7623_data = {
 	.has_sram = false,
 	.offload_version = 1,
 	.hash_offset = 2,
+	.version = 1,
 	.has_accounting = false,
 	.foe_entry_size = MTK_FOE_ENTRY_V1_SIZE,
 	.rss_num = 0,
@@ -6888,6 +6892,7 @@ static const struct mtk_soc_data mt7629_data = {
 	.required_pctl = false,
 	.has_sram = false,
 	.has_accounting = true,
+	.version = 1,
 	.rss_num = 0,
 	.txrx = {
 		.txd_size = sizeof(struct mtk_tx_dma),
@@ -6911,6 +6916,7 @@ static const struct mtk_soc_data mt7986_data = {
 	.has_sram = false,
 	.offload_version = 2,
 	.hash_offset = 4,
+	.version = 2,
 	.has_accounting = true,
 	.foe_entry_size = MTK_FOE_ENTRY_V2_SIZE,
 	.rss_num = 4,
@@ -6935,6 +6941,7 @@ static const struct mtk_soc_data mt7981_data = {
 	.required_pctl = false,
 	.has_sram = false,
 	.offload_version = 2,
+	.version = 2,
 	.hash_offset = 4,
 	.has_accounting = true,
 	.foe_entry_size = MTK_FOE_ENTRY_V2_SIZE,
@@ -6961,6 +6968,7 @@ static const struct mtk_soc_data mt7988_data = {
 	.has_sram = true,
 	.offload_version = 2,
 	.hash_offset = 4,
+	.version = 3,
 	.has_accounting = true,
 	.foe_entry_size = MTK_FOE_ENTRY_V3_SIZE,
 	.rss_num = 4,
@@ -6986,6 +6994,7 @@ static const struct mtk_soc_data mt7987_data = {
 	.has_sram = true,
 	.offload_version = 2,
 	.hash_offset = 4,
+	.version = 3,
 	.has_accounting = true,
 	.foe_entry_size = MTK_FOE_ENTRY_V3_SIZE,
 	.rss_num = 4,
@@ -7007,6 +7016,7 @@ static const struct mtk_soc_data rt5350_data = {
 	.hw_features = MTK_HW_FEATURES_MT7628,
 	.required_clks = MT7628_CLKS_BITMAP,
 	.required_pctl = false,
+	.version = 1,
 	.has_sram = false,
 	.rss_num = 0,
 	.txrx = {
diff --git a/drivers/net/ethernet/mediatek/mtk_eth_soc.h b/drivers/net/ethernet/mediatek/mtk_eth_soc.h
index 3aa877f..5135f01 100644
--- a/drivers/net/ethernet/mediatek/mtk_eth_soc.h
+++ b/drivers/net/ethernet/mediatek/mtk_eth_soc.h
@@ -2072,6 +2072,7 @@ struct mtk_soc_data {
 	bool		required_pctl;
 	u8		offload_version;
 	u8		hash_offset;
+	u8		version;
 	u16		foe_entry_size;
 	netdev_features_t hw_features;
 	bool		has_sram;
diff --git a/drivers/net/ethernet/mediatek/mtk_wed.c b/drivers/net/ethernet/mediatek/mtk_wed.c
index 813ba6a..a36e88f 100644
--- a/drivers/net/ethernet/mediatek/mtk_wed.c
+++ b/drivers/net/ethernet/mediatek/mtk_wed.c
@@ -35,9 +35,11 @@ static const struct mtk_wed_soc_data mt7622_data = {
 		.wpdma_rx_ring0		= 0x770,
 		.reset_idx_tx_mask	= GENMASK(3, 0),
 		.reset_idx_rx_mask	= GENMASK(17, 16),
+		.msdu_pg_ring2_cfg	= 0,
 	},
 	.tx_ring_desc_size = sizeof(struct mtk_wdma_desc),
 	.wdma_desc_size = sizeof(struct mtk_wdma_desc),
+	.wo_support = 0,
 };
 
 static const struct mtk_wed_soc_data mt7986_data = {
@@ -46,9 +48,11 @@ static const struct mtk_wed_soc_data mt7986_data = {
 		.wpdma_rx_ring0		= 0x770,
 		.reset_idx_tx_mask	= GENMASK(1, 0),
 		.reset_idx_rx_mask	= GENMASK(7, 6),
+		.msdu_pg_ring2_cfg	= 0,
 	},
 	.tx_ring_desc_size = sizeof(struct mtk_wdma_desc),
 	.wdma_desc_size = 2 * sizeof(struct mtk_wdma_desc),
+	.wo_support = 1,
 };
 
 static const struct mtk_wed_soc_data mt7988_data = {
@@ -57,9 +61,24 @@ static const struct mtk_wed_soc_data mt7988_data = {
 		.wpdma_rx_ring0		= 0x7d0,
 		.reset_idx_tx_mask	= GENMASK(1, 0),
 		.reset_idx_rx_mask	= GENMASK(7, 6),
+		.msdu_pg_ring2_cfg	= 0xe58,
 	},
 	.tx_ring_desc_size = sizeof(struct mtk_wed_bm_desc),
 	.wdma_desc_size = 2 * sizeof(struct mtk_wdma_desc),
+	.wo_support = 1,
+};
+
+static const struct mtk_wed_soc_data mt7987_data = {
+	.regmap = {
+		.tx_bm_tkid		= 0x0c8,
+		.wpdma_rx_ring0		= 0,
+		.reset_idx_tx_mask	= GENMASK(1, 0),
+		.reset_idx_rx_mask	= GENMASK(7, 6),
+		.msdu_pg_ring2_cfg	= 0xe2c,
+	},
+	.tx_ring_desc_size = sizeof(struct mtk_wed_bm_desc),
+	.wdma_desc_size = 2 * sizeof(struct mtk_wdma_desc),
+	.wo_support = 0,
 };
 
 static void
@@ -361,6 +380,9 @@ mtk_wed_wo_reset(struct mtk_wed_device *dev)
 	void __iomem *reg;
 	u32 val;
 
+	if (!dev->hw->soc->wo_support)
+		return;
+
 	mtk_wdma_tx_reset(dev);
 
 	mtk_wed_reset(dev, MTK_WED_RESET_WED);
@@ -544,38 +566,44 @@ mtk_wed_amsdu_init(struct mtk_wed_device *dev)
 		return 0;
 
 	for (i = 0; i < MTK_WED_AMSDU_NPAGES; i++)
-		wed_w32(dev, MTK_WED_AMSDU_HIFTXD_BASE_L(i),
-			wed_amsdu[i].txd_phy);
-
-	/* init all sta parameter */
-	wed_w32(dev, MTK_WED_AMSDU_STA_INFO_INIT, MTK_WED_AMSDU_STA_RMVL |
-		MTK_WED_AMSDU_STA_WTBL_HDRT_MODE |
-		FIELD_PREP(MTK_WED_AMSDU_STA_MAX_AMSDU_LEN,
-			   dev->wlan.amsdu_max_len >> 8) |
-		FIELD_PREP(MTK_WED_AMSDU_STA_MAX_AMSDU_NUM,
-			   dev->wlan.amsdu_max_subframes));
-
-	wed_w32(dev, MTK_WED_AMSDU_STA_INFO, MTK_WED_AMSDU_STA_INFO_DO_INIT);
-
-	ret = mtk_wed_poll_busy(dev, MTK_WED_AMSDU_STA_INFO,
-				MTK_WED_AMSDU_STA_INFO_DO_INIT);
-	if (ret) {
-		dev_err(dev->hw->dev, "amsdu initialization failed\n");
-		return ret;
+		if (mtk_wed_is_v3_1(dev->hw))
+			wed_w32(dev, MTK_WED_HIFTXD_BASE_L(i),
+				wed_amsdu[i].txd_phy);
+		else
+			wed_w32(dev, MTK_WED_AMSDU_HIFTXD_BASE_L(i),
+				wed_amsdu[i].txd_phy);
+
+	if (mtk_wed_is_v3(dev->hw)) {
+		/* init all sta parameter */
+		wed_w32(dev, MTK_WED_AMSDU_STA_INFO_INIT, MTK_WED_AMSDU_STA_RMVL |
+			MTK_WED_AMSDU_STA_WTBL_HDRT_MODE |
+			FIELD_PREP(MTK_WED_AMSDU_STA_MAX_AMSDU_LEN,
+				   dev->wlan.amsdu_max_len >> 8) |
+			FIELD_PREP(MTK_WED_AMSDU_STA_MAX_AMSDU_NUM,
+				   dev->wlan.amsdu_max_subframes));
+
+		wed_w32(dev, MTK_WED_AMSDU_STA_INFO, MTK_WED_AMSDU_STA_INFO_DO_INIT);
+
+		ret = mtk_wed_poll_busy(dev, MTK_WED_AMSDU_STA_INFO,
+					MTK_WED_AMSDU_STA_INFO_DO_INIT);
+		if (ret) {
+			dev_err(dev->hw->dev, "amsdu initialization failed\n");
+			return ret;
+		}
 	}
-
 	/* init partial amsdu offload txd src */
 	wed_set(dev, MTK_WED_AMSDU_HIFTXD_CFG,
 		FIELD_PREP(MTK_WED_AMSDU_HIFTXD_SRC, dev->hw->index));
 
-	/* init qmem */
-	wed_set(dev, MTK_WED_AMSDU_PSE, MTK_WED_AMSDU_PSE_RESET);
-	ret = mtk_wed_poll_busy(dev, MTK_WED_MON_AMSDU_QMEM_STS1, BIT(29));
-	if (ret) {
-		pr_info("%s: amsdu qmem initialization failed\n", __func__);
-		return ret;
+	if (mtk_wed_is_v3(dev->hw)) {
+		/* init qmem */
+		wed_set(dev, MTK_WED_AMSDU_PSE, MTK_WED_AMSDU_PSE_RESET);
+		ret = mtk_wed_poll_busy(dev, MTK_WED_MON_AMSDU_QMEM_STS1, BIT(29));
+		if (ret) {
+			pr_info("%s: amsdu qmem initialization failed\n", __func__);
+			return ret;
+		}
 	}
-
 	/* eagle E1 PCIE1 tx ring 22 flow control issue */
 	if (dev->wlan.id == 0x7991 || dev->wlan.id == 0x7992)
 		wed_clr(dev, MTK_WED_AMSDU_FIFO, MTK_WED_AMSDU_IS_PRIOR0_RING);
@@ -600,7 +628,10 @@ mtk_wed_tx_buffer_alloc(struct mtk_wed_device *dev)
 		dev->tx_buf_ring.size = ring_size;
 	} else {
 		dev->tx_buf_ring.size = MTK_WED_TX_BM_DMA_SIZE;
-		ring_size = MTK_WED_TX_BM_PKT_CNT;
+		if (mtk_wed_is_v3_1(dev->hw))
+			ring_size = MTK_WED_TX_BM_PKT_CNT_V3_1;
+		else
+			ring_size = MTK_WED_TX_BM_PKT_CNT;
 	}
 	n_pages = dev->tx_buf_ring.size / MTK_WED_BUF_PER_PAGE;
 
@@ -725,8 +756,14 @@ mtk_wed_hwrro_buffer_alloc(struct mtk_wed_device *dev)
 	dma_addr_t desc_phys;
 	int i, page_idx = 0;
 
-	if (!dev->wlan.hw_rro)
+	switch (dev->wlan.hw_rro) {
+	case MTK_WED_HWRRO_V3:
+		break;
+	case MTK_WED_HWRRO_DISABLE:
+	case MTK_WED_HWRRO_V3_1:
+	default:
 		return 0;
+	}
 
 	page_list = kcalloc(n_pages, sizeof(*page_list), GFP_KERNEL);
 	if (!page_list)
@@ -810,8 +847,14 @@ mtk_wed_hwrro_free_buffer(struct mtk_wed_device *dev)
 	struct mtk_wed_bm_desc *desc = dev->hw_rro.desc;
 	int i, page_idx = 0;
 
-	if (!dev->wlan.hw_rro)
+	switch (dev->wlan.hw_rro) {
+	case MTK_WED_HWRRO_V3:
+		break;
+	case MTK_WED_HWRRO_DISABLE:
+	case MTK_WED_HWRRO_V3_1:
+	default:
 		return;
+	}
 
 	if (!page_list)
 		return;
@@ -860,11 +903,22 @@ mtk_wed_hwrro_init(struct mtk_wed_device *dev)
 	if (!mtk_wed_get_rx_capa(dev) || !dev->wlan.hw_rro)
 		return;
 
+	if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3_1) {
+		wed_set(dev, MTK_WED_RRO_CTRL, MTK_WED_RRO_CTRL_VER_SEL);
+		wed_set(dev, MTK_WED_WPDMA_RRO3_1_RX_PREF_CFG,
+			MTK_WED_WPDMA_RRO3_1_RX_PREF_EN);
+		return;
+	}
 	wed_set(dev, MTK_WED_RRO_PG_BM_RX_DMAM,
 		FIELD_PREP(MTK_WED_RRO_PG_BM_RX_SDL0, 128));
 
 	wed_w32(dev, MTK_WED_RRO_PG_BM_BASE, dev->hw_rro.desc_phys);
 
+	if (mtk_wed_is_v3_1(dev->hw))
+		wed_w32(dev, MTK_WED_RRO_PG_BM_RANGE_CFG,
+			FIELD_PREP(MTK_WED_RRO_PG_BM_RANGE_SW_BUF,
+				   MTK_WED_RX_PG_BM_CNT));
+
 	wed_w32(dev, MTK_WED_RRO_PG_BM_INIT_PTR,
 		MTK_WED_RRO_PG_BM_INIT_SW_TAIL_IDX |
 		FIELD_PREP(MTK_WED_RRO_PG_BM_SW_TAIL_IDX,
@@ -878,15 +932,20 @@ static void
 mtk_wed_rx_buffer_hw_init(struct mtk_wed_device *dev)
 {
 	wed_w32(dev, MTK_WED_RX_BM_RX_DMAD,
-		FIELD_PREP(MTK_WED_RX_BM_RX_DMAD_SDL0,  dev->wlan.rx_size));
+		FIELD_PREP(MTK_WED_RX_BM_RX_DMAD_SDL0, dev->wlan.rx_size));
 
 	wed_w32(dev, MTK_WED_RX_BM_BASE, dev->rx_buf_ring.desc_phys);
 
+	if (mtk_wed_is_v3_1(dev->hw))
+		wed_w32(dev, MTK_WED_RX_BM_RANGE_CFG,
+			FIELD_PREP(MTK_WED_RX_BM_RANGE_SW_BUF, dev->wlan.rx_npkt));
+
 	wed_w32(dev, MTK_WED_RX_BM_INIT_PTR, MTK_WED_RX_BM_INIT_SW_TAIL |
 		FIELD_PREP(MTK_WED_RX_BM_SW_TAIL, dev->wlan.rx_npkt));
 
-	wed_w32(dev, MTK_WED_RX_BM_DYN_ALLOC_TH,
-		FIELD_PREP(MTK_WED_RX_BM_DYN_ALLOC_TH_H, 0xffff));
+	if (!mtk_wed_is_v3_1(dev->hw))
+		wed_w32(dev, MTK_WED_RX_BM_DYN_ALLOC_TH,
+			FIELD_PREP(MTK_WED_RX_BM_DYN_ALLOC_TH_H, 0xffff));
 
 	wed_set(dev, MTK_WED_CTRL, MTK_WED_CTRL_WED_RX_BM_EN);
 
@@ -940,16 +999,17 @@ mtk_wed_set_ext_int(struct mtk_wed_device *dev, bool en)
 	u32 mask = MTK_WED_EXT_INT_STATUS_ERROR_MASK;
 
 	switch (dev->hw->version) {
-	case 1:
+	case MTK_WED_HW_V1:
 		mask |= MTK_WED_EXT_INT_STATUS_TX_DRV_R_RESP_ERR;
 		break;
-	case 2:
+	case MTK_WED_HW_V2:
 		mask |= MTK_WED_EXT_INT_STATUS_RX_FBUF_LO_TH2 |
 			MTK_WED_EXT_INT_STATUS_RX_FBUF_HI_TH2 |
 			MTK_WED_EXT_INT_STATUS_RX_DRV_COHERENT |
 			MTK_WED_EXT_INT_STATUS_TX_DMA_W_RESP_ERR;
 		break;
-	case 3:
+	case MTK_WED_HW_V3:
+	case MTK_WED_HW_V3_1:
 		mask = MTK_WED_EXT_INT_STATUS_RX_DRV_COHERENT;
 		break;
 	}
@@ -986,7 +1046,6 @@ mtk_wed_check_wfdma_rx_fill(struct mtk_wed_device *dev,
 
 	for (i = 0; i < 3; i++) {
 		u32 cur_idx = readl(ring->wpdma + MTK_WED_RING_OFS_CPU_IDX);
-
 		if (cur_idx == MTK_WED_RX_RING_SIZE - 1)
 			break;
 
@@ -1026,11 +1085,13 @@ mtk_wed_dma_disable(struct mtk_wed_device *dev)
 		wdma_clr(dev, MTK_WDMA_GLO_CFG,
 			 MTK_WDMA_GLO_CFG_RX_INFO3_PRERES);
 	} else {
-		wed_clr(dev, MTK_WED_WPDMA_GLO_CFG,
-			MTK_WED_WPDMA_GLO_CFG_RX_DRV_R0_PKT_PROC |
-			MTK_WED_WPDMA_GLO_CFG_RX_DRV_R0_CRX_SYNC);
-		wed_clr(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
-			MTK_WED_WPDMA_RX_D_RX_DRV_EN);
+		if (!mtk_wed_is_v3_1(dev->hw)) {
+			wed_clr(dev, MTK_WED_WPDMA_GLO_CFG,
+				MTK_WED_WPDMA_GLO_CFG_RX_DRV_R0_PKT_PROC |
+				MTK_WED_WPDMA_GLO_CFG_RX_DRV_R0_CRX_SYNC);
+			wed_clr(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
+				MTK_WED_WPDMA_RX_D_RX_DRV_EN);
+		}
 		wed_clr(dev, MTK_WED_WDMA_GLO_CFG,
 			MTK_WED_WDMA_GLO_CFG_TX_DDONE_CHK);
 
@@ -1157,17 +1218,21 @@ mtk_wed_bus_init(struct mtk_wed_device *dev)
 		}
 
 		if (dev->wlan.msi) {
-		     wed_w32(dev, MTK_WED_PCIE_CFG_INTM,
-			     dev->hw->pci_base | 0xc08);
-		     wed_w32(dev, MTK_WED_PCIE_CFG_BASE,
-			     dev->hw->pci_base | 0xc04);
-		     wed_w32(dev, MTK_WED_PCIE_INT_TRIGGER, BIT(8));
+			wed_w32(dev, MTK_WED_PCIE_CFG_INTM,
+				dev->hw->pci_base | 0xc08);
+			wed_w32(dev, MTK_WED_PCIE_CFG_BASE,
+				dev->hw->pci_base | 0xc04);
+			wed_w32(dev, MTK_WED_PCIE_INT_TRIGGER, BIT(8));
+			if (mtk_wed_is_v3_1(dev->hw))
+				wed_w32(dev, MTK_WED_PCIE_INT_CLR, BIT(8));
 		} else {
-		     wed_w32(dev, MTK_WED_PCIE_CFG_INTM,
-			     dev->hw->pci_base | 0x180);
-		     wed_w32(dev, MTK_WED_PCIE_CFG_BASE,
-			     dev->hw->pci_base | 0x184);
-		     wed_w32(dev, MTK_WED_PCIE_INT_TRIGGER, BIT(24));
+			wed_w32(dev, MTK_WED_PCIE_CFG_INTM,
+				dev->hw->pci_base | 0x180);
+			wed_w32(dev, MTK_WED_PCIE_CFG_BASE,
+				dev->hw->pci_base | 0x184);
+			wed_w32(dev, MTK_WED_PCIE_INT_TRIGGER, BIT(24));
+			if (mtk_wed_is_v3_1(dev->hw))
+				wed_w32(dev, MTK_WED_PCIE_INT_CLR, BIT(24));
 		}
 
 		wed_w32(dev, MTK_WED_PCIE_INT_CTRL,
@@ -1191,6 +1256,57 @@ mtk_wed_bus_init(struct mtk_wed_device *dev)
 	}
 }
 
+static void
+mtk_wed_v3_1_set_wpdma(struct mtk_wed_device *dev)
+{
+	int i;
+
+	/* Tx data ring */
+	wed_w32(dev, MTK_WED_WPDMA_CFG_TX0_CIDX,
+		dev->wlan.wpdma_tx[0] + MTK_WED_RING_OFS_CPU_IDX);
+	wed_w32(dev, MTK_WED_WPDMA_CFG_TX0_DIDX,
+		dev->wlan.wpdma_tx[0] + MTK_WED_RING_OFS_DMA_IDX);
+	wed_w32(dev, MTK_WED_WPDMA_CFG_TX1_CIDX,
+		dev->wlan.wpdma_tx[1] + MTK_WED_RING_OFS_CPU_IDX);
+	wed_w32(dev, MTK_WED_WPDMA_CFG_TX1_DIDX,
+		dev->wlan.wpdma_tx[1] + MTK_WED_RING_OFS_DMA_IDX);
+	/* Tx free done event ring */
+	wed_w32(dev, MTK_WED_WPDMA_CFG_TX_FREE_CIDX,
+		dev->wlan.wpdma_txfree + MTK_WED_RING_OFS_CPU_IDX);
+	wed_w32(dev, MTK_WED_WPDMA_CFG_TX_FREE_DIDX,
+		dev->wlan.wpdma_txfree + MTK_WED_RING_OFS_DMA_IDX);
+
+	if (!mtk_wed_get_rx_capa(dev))
+		return;
+
+	/* GLO CFG */
+	wed_w32(dev, MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG_BASE,
+		dev->wlan.wpdma_rx_glo);
+
+	/* RRO data rings */
+	for (i = 0; i < MTK_WED_RX_QUEUES; i++) {
+		wed_w32(dev, MTK_WED_WPDMA_RRO_RX_RING_CIDX(i),
+			dev->wlan.wpdma_rx_rro[i] + MTK_WED_RING_OFS_CPU_IDX);
+		wed_w32(dev, MTK_WED_WPDMA_RRO_RX_RING_DIDX(i),
+			dev->wlan.wpdma_rx_rro[i] + MTK_WED_RING_OFS_DMA_IDX);
+	}
+
+	if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3) {
+		/* RRO MSDU page rings */
+		for (i = 0; i < MTK_WED_RX_PAGE_QUEUES; i++) {
+			wed_w32(dev, MTK_WED_RRO_MSDU_PG_RING_CFG_CIDX(i),
+				dev->wlan.wpdma_rx_pg + i * 0x10 + MTK_WED_RING_OFS_CPU_IDX);
+			wed_w32(dev, MTK_WED_RRO_MSDU_PG_RING_CFG_DIDX(i),
+				dev->wlan.wpdma_rx_pg + i * 0x10 + MTK_WED_RING_OFS_DMA_IDX);
+		}
+	} else {
+		/* MTK_WED_HWRRO_V3_1 ring */
+		wed_w32(dev, MTK_WED_WPDMA_RRO3_1_RX_RING_CIDX,
+			dev->wlan.wpdma_rro_3_1_rx + MTK_WED_RING_OFS_CPU_IDX);
+		wed_w32(dev, MTK_WED_WPDMA_RRO3_1_RX_RING_DIDX,
+			dev->wlan.wpdma_rro_3_1_rx + MTK_WED_RING_OFS_DMA_IDX);
+	}
+}
 static void
 mtk_wed_set_wpdma(struct mtk_wed_device *dev)
 {
@@ -1205,7 +1321,12 @@ mtk_wed_set_wpdma(struct mtk_wed_device *dev)
 
 	wed_w32(dev, MTK_WED_WPDMA_CFG_BASE,  dev->wlan.wpdma_int);
 	wed_w32(dev, MTK_WED_WPDMA_CFG_INT_MASK,  dev->wlan.wpdma_mask);
-	wed_w32(dev, MTK_WED_WPDMA_CFG_TX, dev->wlan.wpdma_tx);
+	if (mtk_wed_is_v3_1(dev->hw)) {
+		mtk_wed_v3_1_set_wpdma(dev);
+		return;
+	}
+
+	wed_w32(dev, MTK_WED_WPDMA_CFG_TX, dev->wlan.wpdma_tx[0]);
 	wed_w32(dev, MTK_WED_WPDMA_CFG_TX_FREE,  dev->wlan.wpdma_txfree);
 
 	if (!mtk_wed_get_rx_capa(dev)) 
@@ -1224,7 +1345,7 @@ mtk_wed_set_wpdma(struct mtk_wed_device *dev)
 	wed_w32(dev, MTK_WED_RRO_RX_D_CFG(1), dev->wlan.wpdma_rx_rro[1]);
 	for (i = 0; i < MTK_WED_RX_PAGE_QUEUES; i++)
 		wed_w32(dev, MTK_WED_RRO_MSDU_PG_RING_CFG(i),
-			dev->wlan.wpdma_rx_pg + i * 0x10);	
+			dev->wlan.wpdma_rx_pg + i * 0x10);
 }
 
 static void
@@ -1243,7 +1364,8 @@ mtk_wed_hw_init_early(struct mtk_wed_device *dev)
 		set |= MTK_WED_WDMA_GLO_CFG_DYNAMIC_SKIP_DMAD_PREP |
 		       MTK_WED_WDMA_GLO_CFG_IDLE_DMAD_SUPPLY;
 	}
-	wed_m32(dev, MTK_WED_WDMA_GLO_CFG, mask, set);
+	if (!mtk_wed_is_v3_1(dev->hw))
+		wed_m32(dev, MTK_WED_WDMA_GLO_CFG, mask, set);
 
 	if (mtk_wed_is_v1(dev->hw)) {
 		u32 offset;
@@ -1275,6 +1397,13 @@ mtk_wed_hw_init_early(struct mtk_wed_device *dev)
 				   MTK_WDMA_RING_RX(0)));
 	}
 
+	if (mtk_wed_is_v3_1(dev->hw)) {
+		wed_clr(dev, MTK_WED_WDMA_GLO_CFG,
+			MTK_WED_WDMA_GLO_CFG_DYNAMIC_SKIP_DMAD_PREP);
+		wed_set(dev, MTK_WED_WDMA_GLO_CFG,
+			MTK_WED_WDMA_GLO_CFG_DYNAMIC_SKIP_DMAD_THRES);
+	}
+
 }
 
 static int
@@ -1461,6 +1590,16 @@ mtk_wed_hw_init(struct mtk_wed_device *dev)
 				   dev->tx_buf_ring.size / 128) |
 			FIELD_PREP(MTK_WED_TX_TKID_CTRL_RSV_GRP_NUM,
 				   dev->tx_buf_ring.size / 128));
+	} else if (mtk_wed_is_v3_1(dev->hw)) {
+		wed_w32(dev, MTK_WED_TX_TKID_CTRL,
+			FIELD_PREP(MTK_WED_TX_TKID_CTRL_VLD_GRP_NUM_V3,
+				   dev->wlan.nbuf / 128) |
+			FIELD_PREP(MTK_WED_TX_TKID_CTRL_RSV_GRP_NUM_V3,
+				   dev->wlan.nbuf / 128));
+		wed_w32(dev, MTK_WED_TX_TKID_DYN_THR,
+			FIELD_PREP(MTK_WED_TX_TKID_DYN_THR_LO, 0) |
+			FIELD_PREP(MTK_WED_TX_TKID_DYN_THR_HI_V3,
+				   dev->wlan.nbuf / 128));
 	}
 
 	wed_w32(dev, dev->hw->soc->regmap.tx_bm_tkid,
@@ -1471,7 +1610,7 @@ mtk_wed_hw_init(struct mtk_wed_device *dev)
 
 	mtk_wed_reset(dev, MTK_WED_RESET_TX_BM);
 
-	if (mtk_wed_is_v3_or_greater(dev->hw)) {
+	if (mtk_wed_is_v3(dev->hw)) {
 		/* switch to new bm architecture */
 		wed_clr(dev, MTK_WED_TX_BM_CTRL,
 			MTK_WED_TX_BM_CTRL_LEGACY_EN);
@@ -1489,6 +1628,13 @@ mtk_wed_hw_init(struct mtk_wed_device *dev)
 		wed_w32(dev, MTK_WED_TX_BM_INIT_PTR,
 			MTK_WED_TX_BM_PKT_CNT |
 			MTK_WED_TX_BM_INIT_SW_TAIL_IDX);
+	} else if (mtk_wed_is_v3_1(dev->hw)) {
+		wed_w32(dev, MTK_WED_TX_BM_RANGE_CFG,
+			MTK_WED_TX_BM_PKT_CNT_V3_1);
+
+		wed_w32(dev, MTK_WED_TX_BM_INIT_PTR,
+			MTK_WED_TX_BM_PKT_CNT_V3_1 |
+			MTK_WED_TX_BM_INIT_SW_TAIL_IDX);
 	}
 
 	if (mtk_wed_is_v1(dev->hw)) {
@@ -1497,34 +1643,39 @@ mtk_wed_hw_init(struct mtk_wed_device *dev)
 			MTK_WED_CTRL_WED_TX_FREE_AGENT_EN);
 	} else if (mtk_wed_get_rx_capa(dev)) {
 		/* rx hw init */
-		wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX,
-			MTK_WED_WPDMA_RX_D_RST_CRX_IDX |
-			MTK_WED_WPDMA_RX_D_RST_DRV_IDX);
-
-		wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX, 0);
-
-		/* reset prefetch index of ring */
-		wed_set(dev, MTK_WED_WPDMA_RX_D_PREF_RX0_SIDX,
-			MTK_WED_WPDMA_RX_D_PREF_SIDX_IDX_CLR);
-		wed_clr(dev, MTK_WED_WPDMA_RX_D_PREF_RX0_SIDX,
-			MTK_WED_WPDMA_RX_D_PREF_SIDX_IDX_CLR);
-
-		wed_set(dev, MTK_WED_WPDMA_RX_D_PREF_RX1_SIDX,
-			MTK_WED_WPDMA_RX_D_PREF_SIDX_IDX_CLR);
-		wed_clr(dev, MTK_WED_WPDMA_RX_D_PREF_RX1_SIDX,
-			MTK_WED_WPDMA_RX_D_PREF_SIDX_IDX_CLR);
-
-		/* reset prefetch FIFO of ring */
-		wed_set(dev, MTK_WED_WPDMA_RX_D_PREF_FIFO_CFG,
-			MTK_WED_WPDMA_RX_D_PREF_FIFO_CFG_R0_CLR |
-			MTK_WED_WPDMA_RX_D_PREF_FIFO_CFG_R1_CLR);
-		wed_w32(dev, MTK_WED_WPDMA_RX_D_PREF_FIFO_CFG, 0);
-
+		if (!mtk_wed_is_v3_1(dev->hw)) {
+			wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX,
+				MTK_WED_WPDMA_RX_D_RST_CRX_IDX |
+				MTK_WED_WPDMA_RX_D_RST_DRV_IDX);
+
+			wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX, 0);
+
+			/* reset prefetch index of ring */
+			wed_set(dev, MTK_WED_WPDMA_RX_D_PREF_RX0_SIDX,
+				MTK_WED_WPDMA_RX_D_PREF_SIDX_IDX_CLR);
+			wed_clr(dev, MTK_WED_WPDMA_RX_D_PREF_RX0_SIDX,
+				MTK_WED_WPDMA_RX_D_PREF_SIDX_IDX_CLR);
+
+			wed_set(dev, MTK_WED_WPDMA_RX_D_PREF_RX1_SIDX,
+				MTK_WED_WPDMA_RX_D_PREF_SIDX_IDX_CLR);
+			wed_clr(dev, MTK_WED_WPDMA_RX_D_PREF_RX1_SIDX,
+				MTK_WED_WPDMA_RX_D_PREF_SIDX_IDX_CLR);
+
+			/* reset prefetch FIFO of ring */
+			wed_set(dev, MTK_WED_WPDMA_RX_D_PREF_FIFO_CFG,
+				MTK_WED_WPDMA_RX_D_PREF_FIFO_CFG_R0_CLR |
+				MTK_WED_WPDMA_RX_D_PREF_FIFO_CFG_R1_CLR);
+			wed_w32(dev, MTK_WED_WPDMA_RX_D_PREF_FIFO_CFG, 0);
+		}
 		mtk_wed_rx_buffer_hw_init(dev);
-		mtk_wed_rro_hw_init(dev);
+		if (!mtk_wed_is_v3_1(dev->hw))
+			mtk_wed_rro_hw_init(dev);
 		mtk_wed_route_qm_hw_init(dev);
 	}
 
+	if (mtk_wed_is_v3_1(dev->hw))
+		return;
+
 	wed_clr(dev, MTK_WED_TX_BM_CTRL, MTK_WED_TX_BM_CTRL_PAUSE);
 	if (!mtk_wed_is_v1(dev->hw))
 		wed_clr(dev, MTK_WED_TX_TKID_CTRL, MTK_WED_TX_TKID_CTRL_PAUSE);
@@ -1558,78 +1709,108 @@ mtk_wed_rx_reset(struct mtk_wed_device *dev)
 	u8 val = WO_STATE_SER_RESET;
 	int i, ret;
 
-	ret = mtk_wed_mcu_send_msg(wo, MTK_WED_MODULE_ID_WO,
-				   MTK_WED_WO_CMD_CHANGE_STATE, &val,
-				   sizeof(val), true);
+	if (dev->hw->soc->wo_support) {
+		ret = mtk_wed_mcu_send_msg(wo, MTK_WED_MODULE_ID_WO,
+					   MTK_WED_WO_CMD_CHANGE_STATE, &val,
+					   sizeof(val), true);
 
-	if (ret)
-		return ret;
+		if (ret)
+			return ret;
+	}
 
-	if (dev->wlan.hw_rro) {
+	if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3_1) {
+		/* Disable RRO 3.1 Drv */
+		wed_clr(dev, MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG,
+			MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG_RX_DRV_EN);
+
+		mtk_wed_poll_busy(dev, MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG,
+				  MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG_RX_DRV_BUSY);
+
+		/* Disable RRO 3.1 Prefetch */
+		wed_clr(dev, MTK_WED_WPDMA_RRO3_1_RX_PREF_CFG,
+			MTK_WED_WPDMA_RRO3_1_RX_PREF_EN);
+
+		mtk_wed_poll_busy(dev, MTK_WED_WPDMA_RRO3_1_RX_PREF_CFG,
+				  MTK_WED_WPDMA_RRO3_1_RX_PREF_BUSY);
+
+		/* Reset RRO 3.1 Drv */
+		mtk_wed_reset(dev, MTK_WED_RESET_WPDMA_RRO3_1_RX_D_DRV);
+	} else if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3) {
+		/* Reset RRO IND CMD*/
 		wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_WED_RX_IND_CMD_EN);
 		mtk_wed_poll_busy(dev, MTK_WED_RRO_RX_HW_STS,
 				  MTK_WED_RX_IND_CMD_BUSY);
+		if (mtk_wed_is_v3_1(dev->hw)) {
+			wed_set(dev, MTK_WED_RX_IND_CMD_CNT0, MTK_WED_RX_IND_CMD_DBG_CNT_RST);
+			wed_clr(dev, MTK_WED_RX_IND_CMD_CNT0, MTK_WED_RX_IND_CMD_DBG_CNT_RST);
+		}
 		mtk_wed_reset(dev, MTK_WED_RESET_RRO_RX_TO_PG);
 	}
 
-	wed_clr(dev, MTK_WED_WPDMA_RX_D_GLO_CFG, MTK_WED_WPDMA_RX_D_RX_DRV_EN);
-	ret = mtk_wed_poll_busy(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
-				MTK_WED_WPDMA_RX_D_RX_DRV_BUSY);
-	if (!ret && mtk_wed_is_v3_or_greater(dev->hw))
-		ret = mtk_wed_poll_busy(dev, MTK_WED_WPDMA_RX_D_PREF_CFG,
-					 MTK_WED_WPDMA_RX_D_PREF_BUSY);
-	if (ret) {
-		mtk_wed_reset(dev, MTK_WED_RESET_WPDMA_INT_AGENT);
-		mtk_wed_reset(dev, MTK_WED_RESET_WPDMA_RX_D_DRV);
-	} else {
-		if (mtk_wed_is_v3_or_greater(dev->hw)) {
-			/*1.a. Disable Prefetch HW*/
-			wed_clr(dev, MTK_WED_WPDMA_RX_D_PREF_CFG, MTK_WED_WPDMA_RX_D_PREF_EN);
-			mtk_wed_poll_busy(dev, MTK_WED_WPDMA_RX_D_PREF_CFG,
-					  MTK_WED_WPDMA_RX_D_PREF_BUSY);
-			wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX,
-				MTK_WED_WPDMA_RX_D_RST_DRV_IDX_ALL);
-		}
+	if (!mtk_wed_is_v3_1(dev->hw)) {
+		wed_clr(dev, MTK_WED_WPDMA_RX_D_GLO_CFG, MTK_WED_WPDMA_RX_D_RX_DRV_EN);
+		ret = mtk_wed_poll_busy(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
+					MTK_WED_WPDMA_RX_D_RX_DRV_BUSY);
+		if (!ret && mtk_wed_is_v3(dev->hw))
+			ret = mtk_wed_poll_busy(dev, MTK_WED_WPDMA_RX_D_PREF_CFG,
+						MTK_WED_WPDMA_RX_D_PREF_BUSY);
+		if (ret) {
+			mtk_wed_reset(dev, MTK_WED_RESET_WPDMA_INT_AGENT);
+			mtk_wed_reset(dev, MTK_WED_RESET_WPDMA_RX_D_DRV);
+		} else {
+			if (mtk_wed_is_v3(dev->hw)) {
+				/*1.a. Disable Prefetch HW*/
+				wed_clr(dev, MTK_WED_WPDMA_RX_D_PREF_CFG,
+					MTK_WED_WPDMA_RX_D_PREF_EN);
+				mtk_wed_poll_busy(dev, MTK_WED_WPDMA_RX_D_PREF_CFG,
+						  MTK_WED_WPDMA_RX_D_PREF_BUSY);
+				wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX,
+					MTK_WED_WPDMA_RX_D_RST_DRV_IDX_ALL);
+			}
 
-		wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX,
-			MTK_WED_WPDMA_RX_D_RST_CRX_IDX |
-			MTK_WED_WPDMA_RX_D_RST_DRV_IDX);
+			wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX,
+				MTK_WED_WPDMA_RX_D_RST_CRX_IDX |
+				MTK_WED_WPDMA_RX_D_RST_DRV_IDX);
 
-		wed_set(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
-			MTK_WED_WPDMA_RX_D_RST_INIT_COMPLETE |
-			MTK_WED_WPDMA_RX_D_FSM_RETURN_IDLE);
-		wed_clr(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
-			MTK_WED_WPDMA_RX_D_RST_INIT_COMPLETE |
-			MTK_WED_WPDMA_RX_D_FSM_RETURN_IDLE);
+			wed_set(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
+				MTK_WED_WPDMA_RX_D_RST_INIT_COMPLETE |
+				MTK_WED_WPDMA_RX_D_FSM_RETURN_IDLE);
+			wed_clr(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
+				MTK_WED_WPDMA_RX_D_RST_INIT_COMPLETE |
+				MTK_WED_WPDMA_RX_D_FSM_RETURN_IDLE);
 
-		wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX, 0);
-	}
+			wed_w32(dev, MTK_WED_WPDMA_RX_D_RST_IDX, 0);
+		}
 
-	/* reset rro qm */
-	wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_RX_RRO_QM_EN);
-	ret = mtk_wed_poll_busy(dev, MTK_WED_CTRL,
-				MTK_WED_CTRL_RX_RRO_QM_BUSY);
-	if (ret) {
-		mtk_wed_reset(dev, MTK_WED_RESET_RX_RRO_QM);
-	} else {
-		wed_set(dev, MTK_WED_RROQM_RST_IDX,
-			MTK_WED_RROQM_RST_IDX_MIOD |
-			MTK_WED_RROQM_RST_IDX_FDBK);
-		wed_w32(dev, MTK_WED_RROQM_RST_IDX, 0);
+		/* reset rro qm */
+		wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_RX_RRO_QM_EN);
+		ret = mtk_wed_poll_busy(dev, MTK_WED_CTRL,
+					MTK_WED_CTRL_RX_RRO_QM_BUSY);
+		if (ret) {
+			mtk_wed_reset(dev, MTK_WED_RESET_RX_RRO_QM);
+		} else {
+			wed_set(dev, MTK_WED_RROQM_RST_IDX,
+				MTK_WED_RROQM_RST_IDX_MIOD |
+				MTK_WED_RROQM_RST_IDX_FDBK);
+			wed_w32(dev, MTK_WED_RROQM_RST_IDX, 0);
+		}
 	}
 
 	if (dev->wlan.hw_rro) {
-		/* Disable RRO MSDU Page Drv */
-		wed_clr(dev, MTK_WED_RRO_MSDU_PG_RING2_CFG, MTK_WED_RRO_MSDU_PG_DRV_EN);
-
+		if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3) {
+			/* Disable RRO MSDU Page Drv */
+			wed_clr(dev, dev->hw->soc->regmap.msdu_pg_ring2_cfg,
+				MTK_WED_RRO_MSDU_PG_DRV_EN);
+
+			/* RRO MSDU Page Drv Reset */
+			wed_w32(dev, dev->hw->soc->regmap.msdu_pg_ring2_cfg,
+				MTK_WED_RRO_MSDU_PG_DRV_CLR);
+			mtk_wed_poll_busy(dev, dev->hw->soc->regmap.msdu_pg_ring2_cfg,
+					  MTK_WED_RRO_MSDU_PG_DRV_CLR);
+		}
 		/* Disable RRO Data Drv */
 		wed_clr(dev, MTK_WED_RRO_RX_D_CFG(2), MTK_WED_RRO_RX_D_DRV_EN);
 
-		/* RRO MSDU Page Drv Reset */
-		wed_w32(dev, MTK_WED_RRO_MSDU_PG_RING2_CFG, MTK_WED_RRO_MSDU_PG_DRV_CLR);
-		mtk_wed_poll_busy(dev, MTK_WED_RRO_MSDU_PG_RING2_CFG,
-				  MTK_WED_RRO_MSDU_PG_DRV_CLR);
-
 		/* RRO Data Drv Reset */
 		wed_w32(dev, MTK_WED_RRO_RX_D_CFG(2), MTK_WED_RRO_RX_D_DRV_CLR);
 		mtk_wed_poll_busy(dev, MTK_WED_RRO_RX_D_CFG(2),
@@ -1640,7 +1821,7 @@ mtk_wed_rx_reset(struct mtk_wed_device *dev)
 	wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_RX_ROUTE_QM_EN);
 	ret = mtk_wed_poll_busy(dev, MTK_WED_CTRL,
 				MTK_WED_CTRL_RX_ROUTE_QM_BUSY);
-	if (ret) {
+	if (ret || mtk_wed_is_v3_1(dev->hw)) {
 		mtk_wed_reset(dev, MTK_WED_RESET_RX_ROUTE_QM);
 	} else if (mtk_wed_is_v3_or_greater(dev->hw)) {
 		wed_set(dev, MTK_WED_RTQM_RST, BIT(0));
@@ -1667,7 +1848,7 @@ mtk_wed_rx_reset(struct mtk_wed_device *dev)
 	ret = mtk_wed_poll_busy(dev, MTK_WED_GLO_CFG,
 				MTK_WED_GLO_CFG_RX_DMA_BUSY);
 	wed_clr(dev, MTK_WED_GLO_CFG, MTK_WED_GLO_CFG_RX_DMA_EN);
-	if (ret) {
+	if (ret || mtk_wed_is_v3_1(dev->hw)) {
 		mtk_wed_reset(dev, MTK_WED_RESET_WED_RX_DMA);
 	} else {
 		wed_set(dev, MTK_WED_RESET_IDX,
@@ -1681,23 +1862,23 @@ mtk_wed_rx_reset(struct mtk_wed_device *dev)
 			  MTK_WED_CTRL_WED_RX_BM_BUSY);
 	mtk_wed_reset(dev, MTK_WED_RESET_RX_BM);
 
-	if (dev->wlan.hw_rro) {
+	if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3) {
 		wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_WED_RX_PG_BM_EN);
 		mtk_wed_poll_busy(dev, MTK_WED_CTRL,
 				  MTK_WED_CTRL_WED_RX_PG_BM_BUSY);
-		wed_set(dev, MTK_WED_RESET, MTK_WED_RESET_RX_PG_BM);
-		wed_clr(dev, MTK_WED_RESET, MTK_WED_RESET_RX_PG_BM);
+		mtk_wed_reset(dev, MTK_WED_RESET_RX_PG_BM);
 	}
 
-	/* wo change to enable state */
-	val = WO_STATE_ENABLE;
-	ret = mtk_wed_mcu_send_msg(wo, MTK_WED_MODULE_ID_WO,
-				   MTK_WED_WO_CMD_CHANGE_STATE, &val,
-				   sizeof(val), true);
-
-	if (ret)
-		return ret;
+	if (dev->hw->soc->wo_support) {
+		/* wo change to enable state */
+		val = WO_STATE_ENABLE;
+		ret = mtk_wed_mcu_send_msg(wo, MTK_WED_MODULE_ID_WO,
+					   MTK_WED_WO_CMD_CHANGE_STATE, &val,
+					   sizeof(val), true);
 
+		if (ret)
+			return ret;
+	}
 	/* wed_rx_ring_reset */
 	for (i = 0; i < ARRAY_SIZE(dev->rx_ring); i++) {
 		if (!dev->rx_ring[i].desc)
@@ -1712,38 +1893,14 @@ mtk_wed_rx_reset(struct mtk_wed_device *dev)
 	return 0;
 }
 
-
 static void
-mtk_wed_reset_dma(struct mtk_wed_device *dev)
+mtk_wed_reset_wdma_rx_dma(struct mtk_wed_device *dev)
 {
 	bool busy = false;
 	u32 val;
-	int i;
 
-	for (i = 0; i < ARRAY_SIZE(dev->tx_ring); i++) {
-		if (!dev->tx_ring[i].desc)
-			continue;
-
-		mtk_wed_ring_reset(&dev->tx_ring[i], MTK_WED_TX_RING_SIZE,
-				   true);
-	}
-
-	/* 1.Reset WED Tx DMA */
-	wed_clr(dev, MTK_WED_GLO_CFG, MTK_WED_GLO_CFG_TX_DMA_EN);
-	busy = mtk_wed_poll_busy(dev, MTK_WED_GLO_CFG,
-				 MTK_WED_GLO_CFG_TX_DMA_BUSY);
-
-	if (busy) {
-		mtk_wed_reset(dev, MTK_WED_RESET_WED_TX_DMA);
-	} else {
-		wed_w32(dev, MTK_WED_RESET_IDX,
-			dev->hw->soc->regmap.reset_idx_tx_mask);
-		wed_w32(dev, MTK_WED_RESET_IDX, 0);
-	}
-
-	/* 2. Reset WDMA Rx DMA/Driver_Engine */
 	busy = !!mtk_wdma_rx_reset(dev);
-	if (mtk_wed_is_v3_or_greater(dev->hw)) {
+	if (mtk_wed_is_v3(dev->hw)) {
 		val = MTK_WED_WDMA_GLO_CFG_RX_DIS_FSM_AUTO_IDLE |
 		      wed_r32(dev, MTK_WED_WDMA_GLO_CFG);
 		val &= ~MTK_WED_WDMA_GLO_CFG_RX_DRV_EN;
@@ -1764,7 +1921,7 @@ mtk_wed_reset_dma(struct mtk_wed_device *dev)
 		mtk_wed_reset(dev, MTK_WED_RESET_WDMA_INT_AGENT);
 		mtk_wed_reset(dev, MTK_WED_RESET_WDMA_RX_DRV);
 	} else {
-		if (mtk_wed_is_v3_or_greater(dev->hw)) {
+		if (mtk_wed_is_v3(dev->hw)) {
 			/*1.a. Disable Prefetch HW*/
 			wed_clr(dev, MTK_WED_WDMA_RX_PREF_CFG,
 				MTK_WED_WDMA_RX_PREF_EN);
@@ -1792,6 +1949,7 @@ mtk_wed_reset_dma(struct mtk_wed_device *dev)
 			wed_w32(dev, MTK_WED_WDMA_RESET_IDX,
 				MTK_WED_WDMA_RESET_IDX_RX_ALL);
 		}
+
 		wed_w32(dev, MTK_WED_WDMA_RESET_IDX,
 			MTK_WED_WDMA_RESET_IDX_RX | MTK_WED_WDMA_RESET_IDX_DRV);
 		wed_w32(dev, MTK_WED_WDMA_RESET_IDX, 0);
@@ -1802,8 +1960,106 @@ mtk_wed_reset_dma(struct mtk_wed_device *dev)
 		wed_clr(dev, MTK_WED_WDMA_GLO_CFG,
 			MTK_WED_WDMA_GLO_CFG_RST_INIT_COMPLETE);
 	}
+}
+
+static void
+mtk_wed_reset_wdma_rx_dma_v3_1(struct mtk_wed_device *dev)
+{
+	bool busy = false;
+	u32 val, status;
+
+	val = MTK_WED_WDMA_GLO_CFG_FSM_RETURN_IDLE |
+	      wed_r32(dev, MTK_WED_WDMA_GLO_CFG);
+	val &= ~MTK_WED_WDMA_GLO_CFG_RX_DRV_EN;
+	wed_w32(dev, MTK_WED_WDMA_GLO_CFG, val);
+
+	busy = mtk_wed_poll_busy(dev, MTK_WED_WDMA_GLO_CFG,
+				 MTK_WED_WDMA_GLO_CFG_RX_DRV_BUSY);
+	busy = mtk_wed_poll_busy(dev, MTK_WED_WDMA_RX_PREF_CFG,
+				 MTK_WED_WDMA_RX_PREF_BUSY);
+	/* Recycle mode settings prepare */
+	wed_clr(dev, MTK_WED_WDMA_GLO_CFG,
+		MTK_WED_WDMA_GLO_CFG_FSM_RETURN_IDLE |
+		MTK_WED_WDMA_GLO_CFG_RECYCLE_DESC);
+
+	val = MTK_WED_WDMA_GLO_CFG_DYNAMIC_SKIP_DMAD_PREP |
+	      wed_r32(dev, MTK_WED_WDMA_GLO_CFG);
+	val &= ~MTK_WED_WDMA_GLO_CFG_DYNAMIC_SKIP_DMAD_THRES;
+	wed_w32(dev, MTK_WED_WDMA_GLO_CFG, val);
+
+	/* Enable WDMA_RX_DRV recycle mode */
+	wed_set(dev, MTK_WED_WDMA_GLO_CFG,
+		MTK_WED_WDMA_GLO_CFG_DYNAMIC_DMAD_RECYCLE |
+		MTK_WED_WDMA_GLO_CFG_RX_DRV_EN);
+
+	val = MTK_WDMA_GLO_CFG_RX_DMA_BUSY |
+	      MTK_WDMA_GLO_CFG_TX_DMA_BUSY;
+
+	busy = readx_poll_timeout(mtk_wdma_read_reset, dev, status,
+				  !(status & val), 0, 10000);
+
+	wed_clr(dev, MTK_WED_WDMA_GLO_CFG,
+		MTK_WED_WDMA_GLO_CFG_RX_DRV_EN);
+
+	busy = mtk_wed_poll_busy(dev, MTK_WED_WDMA_GLO_CFG,
+				 MTK_WED_WDMA_GLO_CFG_RX_DRV_BUSY);
+	busy = mtk_wed_poll_busy(dev, MTK_WED_WDMA_RX_PREF_CFG,
+				 MTK_WED_WDMA_RX_PREF_BUSY);
+	/* Disable WDMA_RX_DRV recycle mode */
+	val = MTK_WED_WDMA_GLO_CFG_RECYCLE_DESC |
+	      wed_r32(dev, MTK_WED_WDMA_GLO_CFG);
+	val &= ~MTK_WED_WDMA_GLO_CFG_DYNAMIC_DMAD_RECYCLE;
+	wed_w32(dev, MTK_WED_WDMA_GLO_CFG, val);
+
+	val = MTK_WED_WDMA_GLO_CFG_DYNAMIC_SKIP_DMAD_THRES |
+	      wed_r32(dev, MTK_WED_WDMA_GLO_CFG);
+	val &= ~MTK_WED_WDMA_GLO_CFG_DYNAMIC_SKIP_DMAD_PREP;
+	wed_w32(dev, MTK_WED_WDMA_GLO_CFG, val);
+	wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_WDMA_INT_AGENT_EN);
+
+	mtk_wed_poll_busy(dev, MTK_WED_CTRL,
+			  MTK_WED_CTRL_WDMA_INT_AGENT_BUSY);
+	mtk_wed_reset(dev, MTK_WED_RESET_WDMA_INT_AGENT);
+	mtk_wed_reset(dev, MTK_WED_RESET_WDMA_RX_DRV);
+
+	mtk_wdma_rx_reset(dev);
+}
+
+static void
+mtk_wed_reset_dma(struct mtk_wed_device *dev)
+{
+	bool busy = false;
+	u32 val, status;
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(dev->tx_ring); i++) {
+		if (!dev->tx_ring[i].desc)
+			continue;
+
+		mtk_wed_ring_reset(&dev->tx_ring[i], MTK_WED_TX_RING_SIZE,
+				   true);
+	}
+
+	/* 1.Reset WED Tx DMA */
+	wed_clr(dev, MTK_WED_GLO_CFG, MTK_WED_GLO_CFG_TX_DMA_EN);
+	busy = mtk_wed_poll_busy(dev, MTK_WED_GLO_CFG,
+				 MTK_WED_GLO_CFG_TX_DMA_BUSY);
+
+	if (busy) {
+		mtk_wed_reset(dev, MTK_WED_RESET_WED_TX_DMA);
+	} else {
+		wed_w32(dev, MTK_WED_RESET_IDX,
+			dev->hw->soc->regmap.reset_idx_tx_mask);
+		wed_w32(dev, MTK_WED_RESET_IDX, 0);
+	}
+
+	/* 2. Reset WDMA Rx DMA/Driver_Engine */
+	if (mtk_wed_is_v3_1(dev->hw))
+		mtk_wed_reset_wdma_rx_dma_v3_1(dev);
+	else
+		mtk_wed_reset_wdma_rx_dma(dev);
 
-	/* 3. Reset WED WPDMA Tx Driver Engine */
+	/* 3. Reset WED Tx BM */
 	wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_WED_TX_FREE_AGENT_EN);
 
 	for (i = 0; i < 100; i++) {
@@ -1833,11 +2089,12 @@ mtk_wed_reset_dma(struct mtk_wed_device *dev)
 		mtk_wed_poll_busy(dev, MTK_WED_WPDMA_GLO_CFG,
 				  MTK_WED_WPDMA_GLO_CFG_RX_DRV_BUSY);
 
-	if (busy) {
+	if (busy || mtk_wed_is_v3_1(dev->hw)) {
+		wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_WPDMA_INT_AGENT_EN);
 		mtk_wed_reset(dev, MTK_WED_RESET_WPDMA_INT_AGENT);
 		mtk_wed_reset(dev, MTK_WED_RESET_WPDMA_TX_DRV);
 		mtk_wed_reset(dev, MTK_WED_RESET_WPDMA_RX_DRV);
-		if (mtk_wed_is_v3_or_greater(dev->hw))
+		if (mtk_wed_is_v3(dev->hw))
 			wed_w32(dev, MTK_WED_RX1_CTRL2, 0);
 	} else {
 		wed_w32(dev, MTK_WED_WPDMA_RESET_IDX,
@@ -1850,7 +2107,7 @@ mtk_wed_reset_dma(struct mtk_wed_device *dev)
 	if (mtk_wed_is_v1(dev->hw))
 		return;
 
-	if (!busy) {
+	if (!busy && !mtk_wed_is_v3_1(dev->hw)) {
 		wed_w32(dev, MTK_WED_RESET_IDX, MTK_WED_RESET_WPDMA_IDX_RX);
 		wed_w32(dev, MTK_WED_RESET_IDX, 0);
 	}
@@ -1858,6 +2115,11 @@ mtk_wed_reset_dma(struct mtk_wed_device *dev)
 	if (mtk_wed_is_v3_or_greater(dev->hw)) {
 		/* reset amsdu engine */
 		wed_clr(dev, MTK_WED_CTRL, MTK_WED_CTRL_TX_AMSDU_EN);
+		if (mtk_wed_is_v3_1(dev->hw)) {
+			wed_set(dev, MTK_WED_AMSDU_DBG_CFG, MTK_WED_AMSDU_DBG_CFG_EN);
+			wed_set(dev, MTK_WED_AMSDU_DBG_CNT, MTK_WED_AMSDU_DBG_CNT_CLR);
+			wed_clr(dev, MTK_WED_AMSDU_DBG_CFG, MTK_WED_AMSDU_DBG_CFG_EN);
+		}
 		mtk_wed_reset(dev, MTK_WED_RESET_TX_AMSDU);
 	}
 
@@ -2034,16 +2296,23 @@ mtk_wed_configure_irq(struct mtk_wed_device *dev, u32 irq_mask)
 				    dev->wlan.txfree_tbit));
 
 		if (mtk_wed_get_rx_capa(dev)) {
-			wed_w32(dev, MTK_WED_WPDMA_INT_CTRL_RX,
-				MTK_WED_WPDMA_INT_CTRL_RX0_EN |
-				MTK_WED_WPDMA_INT_CTRL_RX0_CLR |
-				MTK_WED_WPDMA_INT_CTRL_RX1_EN |
-				MTK_WED_WPDMA_INT_CTRL_RX1_CLR |
-				FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RX0_DONE_TRIG,
-					   dev->wlan.rx_tbit[0]) |
-				FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RX1_DONE_TRIG,
-					   dev->wlan.rx_tbit[1]));
-
+			if (mtk_wed_is_v3_1(dev->hw)) {
+				wed_w32(dev, MTK_WED_INT_CTRL,
+					FIELD_PREP(MTK_WED_INT_CTRL_RX0_DONE_TRIG,
+						   dev->wlan.rx_tbit[0]) |
+					FIELD_PREP(MTK_WED_INT_CTRL_RX1_DONE_TRIG,
+						   dev->wlan.rx_tbit[1]));
+			} else {
+				wed_w32(dev, MTK_WED_WPDMA_INT_CTRL_RX,
+					MTK_WED_WPDMA_INT_CTRL_RX0_EN |
+					MTK_WED_WPDMA_INT_CTRL_RX0_CLR |
+					MTK_WED_WPDMA_INT_CTRL_RX1_EN |
+					MTK_WED_WPDMA_INT_CTRL_RX1_CLR |
+					FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RX0_DONE_TRIG,
+						   dev->wlan.rx_tbit[0]) |
+					FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RX1_DONE_TRIG,
+						   dev->wlan.rx_tbit[1]));
+			}
 			wdma_mask |= FIELD_PREP(MTK_WDMA_INT_MASK_TX_DONE,
 						GENMASK(1, 0));
 		}
@@ -2100,22 +2369,31 @@ mtk_wed_dma_enable(struct mtk_wed_device *dev)
 		return;
 	}
 
-	wed_set(dev, MTK_WED_WPDMA_GLO_CFG,
-		MTK_WED_WPDMA_GLO_CFG_RX_DRV_R0_PKT_PROC |
-		MTK_WED_WPDMA_GLO_CFG_RX_DRV_R0_CRX_SYNC);
+	if (!mtk_wed_is_v3_1(dev->hw))
+		wed_set(dev, MTK_WED_WPDMA_GLO_CFG,
+			MTK_WED_WPDMA_GLO_CFG_RX_DRV_R0_PKT_PROC |
+			MTK_WED_WPDMA_GLO_CFG_RX_DRV_R0_CRX_SYNC);
 
 	if (mtk_wed_is_v3_or_greater(dev->hw)) {
-		wed_set(dev, MTK_WED_WDMA_RX_PREF_CFG,
-			FIELD_PREP(MTK_WED_WDMA_RX_PREF_BURST_SIZE, 0x10) |
-			FIELD_PREP(MTK_WED_WDMA_RX_PREF_LOW_THRES, 0x8));
-		wed_clr(dev, MTK_WED_WDMA_RX_PREF_CFG,
-			MTK_WED_WDMA_RX_PREF_DDONE2_EN);
+		if (mtk_wed_is_v3_1(dev->hw)) {
+			wed_set(dev, MTK_WED_WDMA_RX_PREF_CFG,
+				MTK_WED_WDMA_RX_PREF_DDONE2_EN);
+
+			wed_set(dev, MTK_WED_WPDMA_GLO_CFG,
+				MTK_WED_WPDMA_GLO_CFG_TXD_VER);
+		} else {
+			wed_set(dev, MTK_WED_WDMA_RX_PREF_CFG,
+				FIELD_PREP(MTK_WED_WDMA_RX_PREF_BURST_SIZE, 0x10) |
+				FIELD_PREP(MTK_WED_WDMA_RX_PREF_LOW_THRES, 0x8));
+
+			wed_clr(dev, MTK_WED_WDMA_RX_PREF_CFG,
+				MTK_WED_WDMA_RX_PREF_DDONE2_EN);
+		}
 		wed_set(dev, MTK_WED_WDMA_RX_PREF_CFG, MTK_WED_WDMA_RX_PREF_EN);
 
-		wed_set(dev, MTK_WED_WPDMA_GLO_CFG,
-			MTK_WED_WPDMA_GLO_CFG_TX_DDONE_CHK_LAST);
 		wed_set(dev, MTK_WED_WPDMA_GLO_CFG,
 			MTK_WED_WPDMA_GLO_CFG_TX_DDONE_CHK |
+			MTK_WED_WPDMA_GLO_CFG_TX_DDONE_CHK_LAST |
 			MTK_WED_WPDMA_GLO_CFG_RX_DRV_EVENT_PKT_FMT_CHK |
 			MTK_WED_WPDMA_GLO_CFG_RX_DRV_UNS_VER_FORCE_4);
 
@@ -2134,18 +2412,21 @@ mtk_wed_dma_enable(struct mtk_wed_device *dev)
 		MTK_WED_WDMA_GLO_CFG_TX_DRV_EN |
 		MTK_WED_WDMA_GLO_CFG_TX_DDONE_CHK);
 
-	wed_clr(dev, MTK_WED_WPDMA_RX_D_GLO_CFG, MTK_WED_WPDMA_RX_D_RXD_READ_LEN);
-	wed_set(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
-		MTK_WED_WPDMA_RX_D_RX_DRV_EN |
-		FIELD_PREP(MTK_WED_WPDMA_RX_D_RXD_READ_LEN, 0x18) |
-		FIELD_PREP(MTK_WED_WPDMA_RX_D_INIT_PHASE_RXEN_SEL, 0x2));
+	if (!mtk_wed_is_v3_1(dev->hw)) {
+		wed_clr(dev, MTK_WED_WPDMA_RX_D_GLO_CFG, MTK_WED_WPDMA_RX_D_RXD_READ_LEN);
+		wed_set(dev, MTK_WED_WPDMA_RX_D_GLO_CFG,
+			MTK_WED_WPDMA_RX_D_RX_DRV_EN |
+			FIELD_PREP(MTK_WED_WPDMA_RX_D_RXD_READ_LEN, 0x18) |
+			FIELD_PREP(MTK_WED_WPDMA_RX_D_INIT_PHASE_RXEN_SEL, 0x2));
+	}
 
-	if (mtk_wed_is_v3_or_greater(dev->hw)) {
+	if (mtk_wed_is_v3(dev->hw))
 		wed_set(dev, MTK_WED_WPDMA_RX_D_PREF_CFG,
 			MTK_WED_WPDMA_RX_D_PREF_EN |
 			FIELD_PREP(MTK_WED_WPDMA_RX_D_PREF_BURST_SIZE, 0x10) |
 			FIELD_PREP(MTK_WED_WPDMA_RX_D_PREF_LOW_THRES, 0x8));
 
+	if (mtk_wed_is_v3_or_greater(dev->hw)) {
 		wed_set(dev, MTK_WED_RRO_RX_D_CFG(2), MTK_WED_RRO_RX_D_DRV_EN);
 		wdma_set(dev, MTK_WDMA_PREF_TX_CFG, MTK_WDMA_PREF_TX_CFG_PREF_EN);
 		wdma_set(dev, MTK_WDMA_WRBK_TX_CFG, MTK_WDMA_WRBK_TX_CFG_WRBK_EN);
@@ -2155,10 +2436,10 @@ mtk_wed_dma_enable(struct mtk_wed_device *dev)
 		struct mtk_wed_ring *ring = &dev->rx_ring[i];
 		u32 val;
 
-		if(!(ring->flags & MTK_WED_RING_CONFIGURED))
+		if (!(ring->flags & MTK_WED_RING_CONFIGURED))
 			continue;
 
-		if(mtk_wed_check_wfdma_rx_fill(dev, ring)) {
+		if (!mtk_wed_is_v3_1(dev->hw) && mtk_wed_check_wfdma_rx_fill(dev, ring)) {
 			dev_err(dev->hw->dev,
 				"mtk_wed%d: rx(%d) dma enable failed!\n",
 				dev->hw->index, i);
@@ -2190,14 +2471,20 @@ mtk_wed_start_hw_rro(struct mtk_wed_device *dev, u32 irq_mask, bool reset)
 		return;
 
 	if (reset) {
-		wed_set(dev, MTK_WED_RRO_MSDU_PG_RING2_CFG,
-			MTK_WED_RRO_MSDU_PG_DRV_EN);
+		if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3)
+			wed_set(dev, dev->hw->soc->regmap.msdu_pg_ring2_cfg,
+				MTK_WED_RRO_MSDU_PG_DRV_EN);
+		else if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3_1) {
+			wed_set(dev, MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG,
+				MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG_RX_DRV_EN);
+		}
 		return;
 	}
 
-	wed_set(dev, MTK_WED_RRO_RX_D_CFG(2), MTK_WED_RRO_MSDU_PG_DRV_CLR);
-	wed_w32(dev, MTK_WED_RRO_MSDU_PG_RING2_CFG,
-		MTK_WED_RRO_MSDU_PG_DRV_CLR);
+	wed_set(dev, MTK_WED_RRO_RX_D_CFG(2), MTK_WED_RRO_RX_D_DRV_CLR);
+	if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3)
+		wed_w32(dev, dev->hw->soc->regmap.msdu_pg_ring2_cfg,
+			MTK_WED_RRO_MSDU_PG_DRV_CLR);
 
 	wed_w32(dev, MTK_WED_WPDMA_INT_CTRL_RRO_RX,
 		MTK_WED_WPDMA_INT_CTRL_RRO_RX0_EN |
@@ -2209,26 +2496,38 @@ mtk_wed_start_hw_rro(struct mtk_wed_device *dev, u32 irq_mask, bool reset)
 		FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RRO_RX1_DONE_TRIG,
 			   dev->wlan.rro_rx_tbit[1]));
 
-	wed_w32(dev, MTK_WED_WPDMA_INT_CTRL_RRO_MSDU_PG,
-		MTK_WED_WPDMA_INT_CTRL_RRO_PG0_EN |
-		MTK_WED_WPDMA_INT_CTRL_RRO_PG0_CLR |
-		MTK_WED_WPDMA_INT_CTRL_RRO_PG1_EN |
-		MTK_WED_WPDMA_INT_CTRL_RRO_PG1_CLR |
-		MTK_WED_WPDMA_INT_CTRL_RRO_PG2_EN |
-		MTK_WED_WPDMA_INT_CTRL_RRO_PG2_CLR |
-		FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RRO_PG0_DONE_TRIG,
-			   dev->wlan.rx_pg_tbit[0]) |
-		FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RRO_PG1_DONE_TRIG,
-			   dev->wlan.rx_pg_tbit[1])|
-		FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RRO_PG2_DONE_TRIG,
-			   dev->wlan.rx_pg_tbit[2]));
+	if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3_1)
+		wed_set(dev, MTK_WED_WPDMA_INT_CTRL_RX,
+			MTK_WED_WPDMA_INT_CTRL_RX_RRO_3_1_EN |
+			MTK_WED_WPDMA_INT_CTRL_RX_RRO_3_1_CLR |
+			FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RX_RRO_3_1_TRIG,
+				   dev->wlan.rro_3_1_rx_tbit));
+
+	if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3)
+		wed_w32(dev, MTK_WED_WPDMA_INT_CTRL_RRO_MSDU_PG,
+			MTK_WED_WPDMA_INT_CTRL_RRO_PG0_EN |
+			MTK_WED_WPDMA_INT_CTRL_RRO_PG0_CLR |
+			MTK_WED_WPDMA_INT_CTRL_RRO_PG1_EN |
+			MTK_WED_WPDMA_INT_CTRL_RRO_PG1_CLR |
+			MTK_WED_WPDMA_INT_CTRL_RRO_PG2_EN |
+			MTK_WED_WPDMA_INT_CTRL_RRO_PG2_CLR |
+			FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RRO_PG0_DONE_TRIG,
+				   dev->wlan.rx_pg_tbit[0]) |
+			FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RRO_PG1_DONE_TRIG,
+				   dev->wlan.rx_pg_tbit[1]) |
+			FIELD_PREP(MTK_WED_WPDMA_INT_CTRL_RRO_PG2_DONE_TRIG,
+				   dev->wlan.rx_pg_tbit[2]));
 
 	/*
 	 * RRO_MSDU_PG_RING2_CFG1_FLD_DRV_EN should be enabled after
 	 * WM FWDL completed, otherwise RRO_MSDU_PG ring may broken
 	 */
-	wed_set(dev, MTK_WED_RRO_MSDU_PG_RING2_CFG,
-		MTK_WED_RRO_MSDU_PG_DRV_EN);
+	if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3)
+		wed_set(dev, dev->hw->soc->regmap.msdu_pg_ring2_cfg,
+			MTK_WED_RRO_MSDU_PG_DRV_EN);
+	else if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3_1)
+		wed_set(dev, MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG,
+			MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG_RX_DRV_EN);
 
 	for (i = 0; i < MTK_WED_RX_QUEUES; i++) {
 		struct mtk_wed_ring *ring = &dev->rx_rro_ring[i];
@@ -2245,6 +2544,8 @@ mtk_wed_start_hw_rro(struct mtk_wed_device *dev, u32 irq_mask, bool reset)
 	for (i = 0; i < MTK_WED_RX_PAGE_QUEUES; i++){
 		struct mtk_wed_ring *ring = &dev->rx_page_ring[i];
 
+		if (dev->wlan.hw_rro == MTK_WED_HWRRO_V3_1)
+			return;
 		if(!(ring->flags & MTK_WED_RING_CONFIGURED))
 			continue;
 
@@ -2357,6 +2658,21 @@ mtk_wed_ind_rx_ring_setup(struct mtk_wed_device *dev, void __iomem *regs)
 	return 0;
 }
 
+static void
+mtk_wed_rro_3_1_rx_ring_setup(struct mtk_wed_device *dev, void __iomem *regs)
+{
+	struct mtk_wed_ring *ring = &dev->rx_rro_3_1_ring;
+
+	ring->wpdma = regs;
+
+	wed_w32(dev, MTK_WED_WPDMA_RRO3_1_RX_D_RX + MTK_WED_RING_OFS_BASE,
+		readl(regs));
+	wed_w32(dev, MTK_WED_WPDMA_RRO3_1_RX_D_RX + MTK_WED_RING_OFS_COUNT,
+		readl(regs + MTK_WED_RING_OFS_COUNT));
+
+	ring->flags |= MTK_WED_RING_CONFIGURED;
+}
+
 static void
 mtk_wed_start(struct mtk_wed_device *dev, u32 irq_mask)
 {
@@ -2391,7 +2707,7 @@ mtk_wed_start(struct mtk_wed_device *dev, u32 irq_mask)
 
 		val |= BIT(0) | (BIT(1) * !!dev->hw->index);
 		regmap_write(dev->hw->mirror, dev->hw->index * 4, val);
-	} else if (mtk_wed_get_rx_capa(dev)) {
+	} else if (mtk_wed_get_rx_capa(dev) && dev->hw->soc->wo_support) {
 		/* driver set mid ready and only once */
 		wed_w32(dev, MTK_WED_EXT_INT_MASK1,
 			MTK_WED_EXT_INT_STATUS_WPDMA_MID_RDY);
@@ -2401,7 +2717,7 @@ mtk_wed_start(struct mtk_wed_device *dev, u32 irq_mask)
 		wed_r32(dev, MTK_WED_EXT_INT_MASK1);
 		wed_r32(dev, MTK_WED_EXT_INT_MASK2);
 
-		if (mtk_wed_is_v3_or_greater(dev->hw)) {
+		if (mtk_wed_is_v3(dev->hw)) {
 			wed_w32(dev, MTK_WED_EXT_INT_MASK3,
 				MTK_WED_EXT_INT_STATUS_WPDMA_MID_RDY);
 			wed_r32(dev, MTK_WED_EXT_INT_MASK3);
@@ -2409,7 +2725,6 @@ mtk_wed_start(struct mtk_wed_device *dev, u32 irq_mask)
 
 		if (mtk_wed_rro_cfg(dev))
 			return;
-
 	}
 
 	mtk_wed_set_512_support(dev, dev->wlan.wcid_512);
@@ -2447,8 +2762,6 @@ mtk_wed_attach(struct mtk_wed_device *dev)
 	device = dev->wlan.bus_type == MTK_WED_BUS_PCIE ?
 				       &dev->wlan.pci_dev->dev
 				       : &dev->wlan.platform_dev->dev;
-	dev_info(device, "attaching wed device %d version %d\n",
-		 hw->index, hw->version);
 
 	dev->hw = hw;
 	dev->dev = hw->dev;
@@ -2465,6 +2778,11 @@ mtk_wed_attach(struct mtk_wed_device *dev)
 	    of_dma_is_coherent(hw->eth->dev->of_node))
 		mtk_eth_set_dma_device(hw->eth, hw->dev);
 
+	dev_info(device, "attaching wed device %d version %ld.%ld\n",
+		 hw->index,
+		 FIELD_GET(MTK_WED_REV_ID_MAJOR, wed_r32(dev, MTK_WED_REV_ID)),
+		 FIELD_GET(MTK_WED_REV_ID_MINOR, wed_r32(dev, MTK_WED_REV_ID)));
+
 	ret = mtk_wed_tx_buffer_alloc(dev);
 	if (ret)
 		goto out;
@@ -2473,7 +2791,7 @@ mtk_wed_attach(struct mtk_wed_device *dev)
 	if (ret)
 		goto out;
 
-	if (mtk_wed_get_rx_capa(dev)) {
+	if (mtk_wed_get_rx_capa(dev) && dev->hw->soc->wo_support) {
 		ret = mtk_wed_rro_alloc(dev);
 		if (ret)
 			goto out;
@@ -2614,6 +2932,8 @@ mtk_wed_rx_ring_setup(struct mtk_wed_device *dev, int idx, void __iomem *regs,
 	ring->wpdma = regs;
 	ring->flags |= MTK_WED_RING_CONFIGURED;
 
+	if (mtk_wed_is_v3_1(dev->hw))
+		return 0;
 	/* WPDMA ->  WED */
 	wpdma_rx_w32(dev, idx, MTK_WED_RING_OFS_BASE, ring->desc_phys);
 	wpdma_rx_w32(dev, idx, MTK_WED_RING_OFS_COUNT, MTK_WED_RX_RING_SIZE);
@@ -2725,6 +3045,20 @@ static int mtk_wed_eth_setup_tc(struct mtk_wed_device *wed, struct net_device *d
 	}
 }
 
+static int
+mtk_wed_get_hw_version(void)
+{
+	struct mtk_wed_hw *hw = hw_list[0];
+
+	RCU_LOCKDEP_WARN(!rcu_read_lock_held(),
+			 "mtk_wed_get_hw_version without holding the RCU read lock");
+
+	if (!hw)
+		return 0;
+	else
+		return hw->version;
+}
+
 void mtk_wed_add_hw(struct device_node *np, struct mtk_eth *eth,
 		    void __iomem *wdma, phys_addr_t wdma_phy,
 		    int index)
@@ -2746,10 +3080,12 @@ void mtk_wed_add_hw(struct device_node *np, struct mtk_eth *eth,
 		.detach = mtk_wed_detach,
 		.setup_tc = mtk_wed_eth_setup_tc,
 		.ppe_check = mtk_wed_ppe_check,
+		.get_hw_version = mtk_wed_get_hw_version,
 		.start_hw_rro = mtk_wed_start_hw_rro,
 		.rro_rx_ring_setup = mtk_wed_rro_rx_ring_setup,
 		.msdu_pg_rx_ring_setup = mtk_wed_msdu_pg_rx_ring_setup,
 		.ind_rx_ring_setup = mtk_wed_ind_rx_ring_setup,
+		.rro_3_1_rx_ring_setup = mtk_wed_rro_3_1_rx_ring_setup,
 	};
 	struct device_node *eth_np = eth->dev->of_node;
 	struct platform_device *pdev;
@@ -2791,15 +3127,17 @@ void mtk_wed_add_hw(struct device_node *np, struct mtk_eth *eth,
 	hw->wdma_phy = wdma_phy;
 	hw->index = index;
 	hw->irq = irq;
-	hw->version = MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V3) ?
-		      3 : MTK_HAS_CAPS(eth->soc->caps, MTK_NETSYS_V2) ? 2 : 1;
 
-	switch (hw->version) {
+	switch (eth->soc->version) {
 	case 2:
 		hw->soc = &mt7986_data;
+		hw->version = MTK_WED_HW_V2;
 		break;
 	case 3:
-		hw->soc = &mt7988_data;
+		hw->soc = MTK_HAS_CAPS(eth->soc->caps, MT7987_CAPS) ?
+			  &mt7987_data : &mt7988_data;
+		hw->version = MTK_HAS_CAPS(eth->soc->caps, MT7987_CAPS) ?
+				MTK_WED_HW_V3_1 : MTK_WED_HW_V3;
 		break;
 	default:
 	case 1:
@@ -2807,6 +3145,7 @@ void mtk_wed_add_hw(struct device_node *np, struct mtk_eth *eth,
 				"mediatek,pcie-mirror");
 		hw->hifsys = syscon_regmap_lookup_by_phandle(eth_np,
 				"mediatek,hifsys");
+		hw->version = MTK_WED_HW_V1;
 		if (IS_ERR(hw->mirror) || IS_ERR(hw->hifsys)) {
 			kfree(hw);
 			goto unlock;
diff --git a/drivers/net/ethernet/mediatek/mtk_wed.h b/drivers/net/ethernet/mediatek/mtk_wed.h
index f5e30ce..c652cc7 100644
--- a/drivers/net/ethernet/mediatek/mtk_wed.h
+++ b/drivers/net/ethernet/mediatek/mtk_wed.h
@@ -35,6 +35,7 @@
 
 #define MTK_WED_TX_BM_DMA_SIZE		65536
 #define MTK_WED_TX_BM_PKT_CNT		32768
+#define MTK_WED_TX_BM_PKT_CNT_V3_1		9216
 
 #define MTK_WED_MODULE_ID_WO		1
 
@@ -47,9 +48,11 @@ struct mtk_wed_soc_data {
 		u32 wpdma_rx_ring0;
 		u32 reset_idx_tx_mask;
 		u32 reset_idx_rx_mask;
+		u32 msdu_pg_ring2_cfg;
 	} regmap;
 	u32 tx_ring_desc_size;
 	u32 wdma_desc_size;
+	u8 wo_support;
 };
 
 struct mtk_wed_hw {
@@ -98,22 +101,27 @@ struct mtk_wed_amsdu {
 #ifdef CONFIG_NET_MEDIATEK_SOC_WED
 static inline bool mtk_wed_is_v1(struct mtk_wed_hw *hw)
 {
-	return hw->version == 1;
+	return hw->version == MTK_WED_HW_V1;
 }
 
 static inline bool mtk_wed_is_v2(struct mtk_wed_hw *hw)
 {
-	return hw->version == 2;
+	return hw->version == MTK_WED_HW_V2;
 }
 
 static inline bool mtk_wed_is_v3(struct mtk_wed_hw *hw)
 {
-	return hw->version == 3;
+	return hw->version == MTK_WED_HW_V3;
 }
 
 static inline bool mtk_wed_is_v3_or_greater(struct mtk_wed_hw *hw)
 {
-	return hw->version > 2;
+	return hw->version > MTK_WED_HW_V2;
+}
+
+static inline bool mtk_wed_is_v3_1(struct mtk_wed_hw *hw)
+{
+	return hw->version == MTK_WED_HW_V3_1;
 }
 
 static inline void
@@ -212,7 +220,7 @@ wpdma_txfree_w32(struct mtk_wed_device *dev, u32 reg, u32 val)
 
 static inline u32 mtk_wed_get_pci_base(struct mtk_wed_device *dev)
 {
-	if (!mtk_wed_is_v3_or_greater(dev->hw))
+	if (!mtk_wed_is_v3(dev->hw))
 		return MTK_WED_PCIE_BASE;
 
 	switch (dev->hw->index) {
diff --git a/drivers/net/ethernet/mediatek/mtk_wed_debugfs.c b/drivers/net/ethernet/mediatek/mtk_wed_debugfs.c
index 93563e1..f67028d 100644
--- a/drivers/net/ethernet/mediatek/mtk_wed_debugfs.c
+++ b/drivers/net/ethernet/mediatek/mtk_wed_debugfs.c
@@ -295,7 +295,8 @@ wed_txinfo_show(struct seq_file *s, void *data)
 		return 0;
 
 	switch(dev->hw->version) {
-	case 3:
+	case MTK_WED_HW_V3:
+	case MTK_WED_HW_V3_1:
 		regs = regs_new_v3;
 		break;
 	default:
@@ -477,6 +478,42 @@ wed_rxinfo_show(struct seq_file *s, void *data)
 		DUMP_END()
 	};
 
+	static const struct reg_dump regs_v3_1[] = {
+		DUMP_STR("WED RX INT info"),
+		DUMP_WED(WED_PCIE_INT_CTRL),
+		DUMP_WED(WED_PCIE_INT_REC),
+		DUMP_WED(WED_WPDMA_INT_STA_REC),
+		DUMP_WED(WED_WPDMA_INT_MON),
+		DUMP_WED(WED_WPDMA_INT_CTRL),
+		DUMP_WED(WED_WPDMA_INT_CTRL_TX),
+		DUMP_WED(WED_WPDMA_INT_CTRL_RX),
+		DUMP_WED(WED_WPDMA_INT_CTRL_TX_FREE),
+		DUMP_WED(WED_WPDMA_STATUS),
+		DUMP_WED(WED_WPDMA_D_ST),
+
+		DUMP_STR("WED RX"),
+		DUMP_WED_RING_RX_TYPE2(WED_RING_RX_DATA(0)),
+
+		DUMP_STR("WED WPDMA RRO3.1 RX"),
+		DUMP_WED(WED_WPDMA_RRO3_1_RX_D_RX_MIB),
+		DUMP_WED_RING_RX_TYPE1(WED_WPDMA_RRO3_1_RX_D_RX),
+		DUMP_STR("WED WDMA TX"),
+		DUMP_WED_RING(WED_WDMA_RING_TX),
+		DUMP_WED(WED_WDMA_TX_MIB),
+
+		DUMP_STR("WDMA TX"),
+		DUMP_WDMA(WDMA_GLO_CFG),
+		DUMP_WDMA_RING(WDMA_RING_TX(0)),
+		DUMP_WDMA_RING(WDMA_RING_TX(1)),
+
+		DUMP_STR("WED RX BM"),
+		DUMP_WED(WED_RX_BM_BASE),
+		DUMP_WED(WED_RX_BM_PTR),
+		DUMP_WED_MASK(WED_RX_BM_PTR, WED_RX_BM_PTR_HEAD),
+		DUMP_WED_MASK(WED_RX_BM_PTR, WED_RX_BM_PTR_TAIL),
+		DUMP_END()
+	};
+
 	static const struct reg_dump *regs_new_v2[] = {
 		&regs_common[0],
 		&regs_v2[0],
@@ -489,6 +526,12 @@ wed_rxinfo_show(struct seq_file *s, void *data)
 		NULL,
 	};
 
+	static const struct reg_dump *regs_new_v3_1[] = {
+		&regs_v3_1[0],
+		&regs_v3[0],
+		NULL,
+	};
+
 	struct mtk_wed_hw *hw = s->private;
 	struct mtk_wed_device *dev = hw->wed_dev;
 	const struct reg_dump **regs;
@@ -497,12 +540,15 @@ wed_rxinfo_show(struct seq_file *s, void *data)
 		return 0;
 
 	switch(dev->hw->version) {
-	case 2:
+	case MTK_WED_HW_V2:
 		regs = regs_new_v2;
 		break;
-	case 3:
+	case MTK_WED_HW_V3:
 		regs = regs_new_v3;
 		break;
+	case MTK_WED_HW_V3_1:
+		regs = regs_new_v3_1;
+		break;
 	default:
 		return 0;
 	}
diff --git a/drivers/net/ethernet/mediatek/mtk_wed_mcu.c b/drivers/net/ethernet/mediatek/mtk_wed_mcu.c
index cd14d47..b4c259b 100644
--- a/drivers/net/ethernet/mediatek/mtk_wed_mcu.c
+++ b/drivers/net/ethernet/mediatek/mtk_wed_mcu.c
@@ -91,7 +91,7 @@ mtk_wed_mcu_msg_update(struct mtk_wed_device *dev, int id, void *data, int len)
 {
 	struct mtk_wed_wo *wo = dev->hw->wed_wo;
 
-	if (!mtk_wed_get_rx_capa(dev))
+	if (!mtk_wed_get_rx_capa(dev) || !dev->hw->soc->wo_support)
 		return 0;
 
 	if (WARN_ON(!wo))
diff --git a/drivers/net/ethernet/mediatek/mtk_wed_regs.h b/drivers/net/ethernet/mediatek/mtk_wed_regs.h
index ed0e560..8d1b21c 100644
--- a/drivers/net/ethernet/mediatek/mtk_wed_regs.h
+++ b/drivers/net/ethernet/mediatek/mtk_wed_regs.h
@@ -39,6 +39,7 @@ struct mtk_wdma_desc {
 #define MTK_WED_RESET_RX_PG_BM				BIT(2)
 #define MTK_WED_RESET_RRO_RX_TO_PG			BIT(3)
 #define MTK_WED_RESET_TX_FREE_AGENT			BIT(4)
+#define MTK_WED_RESET_WPDMA_RRO3_1_RX_D_DRV		BIT(7)
 #define MTK_WED_RESET_WPDMA_TX_DRV			BIT(8)
 #define MTK_WED_RESET_WPDMA_RX_DRV			BIT(9)
 #define MTK_WED_RESET_WPDMA_RX_D_DRV			BIT(10)
@@ -179,7 +180,7 @@ struct mtk_wdma_desc {
 #define MTK_WED_TX_TKID_DYN_THR				0x0e0
 #define MTK_WED_TX_TKID_DYN_THR_LO			GENMASK(6, 0)
 #define MTK_WED_TX_TKID_DYN_THR_HI			GENMASK(22, 16)
-
+#define MTK_WED_TX_TKID_DYN_THR_HI_V3		GENMASK(23, 16)
 #define MTK_WED_TX_TKID_STATUS				0x0e4
 #define MTK_WED_TX_TKID_RECYC				0x0e8
 
@@ -257,6 +258,7 @@ struct mtk_wdma_desc {
 #define MTK_WED_WPDMA_GLO_CFG_RX_DDONE2_WR		BIT(21)
 #define MTK_WED_WPDMA_GLO_CFG_TX_TKID_KEEP		BIT(24)
 #define MTK_WED_WPDMA_GLO_CFG_TX_DDONE_CHK_LAST		BIT(25)
+#define MTK_WED_WPDMA_GLO_CFG_TXD_VER				BIT(26)
 #define MTK_WED_WPDMA_GLO_CFG_TX_DMAD_DW3_PREV		BIT(28)
 #define MTK_WED_WPDMA_GLO_CFG_TX_DDONE_CHK		BIT(30)
 
@@ -309,12 +311,16 @@ struct mtk_wdma_desc {
 #define MTK_WED_WPDMA_INT_CTRL_RX1_EN			BIT(8)
 #define MTK_WED_WPDMA_INT_CTRL_RX1_CLR			BIT(9)
 #define MTK_WED_WPDMA_INT_CTRL_RX1_DONE_TRIG		GENMASK(14, 10)
+#define MTK_WED_WPDMA_INT_CTRL_RX_RRO_3_1_EN		BIT(15)
+#define MTK_WED_WPDMA_INT_CTRL_RX_RRO_3_1_CLR		BIT(16)
+#define MTK_WED_WPDMA_INT_CTRL_RX_RRO_3_1_TRIG		GENMASK(21, 17)
 
 #define MTK_WED_WPDMA_INT_CTRL_TX_FREE			0x538
 #define MTK_WED_WPDMA_INT_CTRL_TX_FREE_DONE_EN		BIT(0)
 #define MTK_WED_WPDMA_INT_CTRL_TX_FREE_DONE_CLR		BIT(1)
 #define MTK_WED_WPDMA_INT_CTRL_TX_FREE_DONE_TRIG	GENMASK(6, 2)
 
+#define MTK_WED_PCIE_INT_CLR				0x550
 #define MTK_WED_PCIE_CFG_BASE				0x560
 #define MTK_WED_PCIE_CFG_INTM				0x564
 #define MTK_WED_PCIE_CFG_MSIS				0x568
@@ -347,6 +353,7 @@ struct mtk_wdma_desc {
 #endif
 
 #define MTK_WED_WPDMA_RING_TX(_n)			(0x600 + (_n) * 0x10)
+#define MTK_WED_WPDMA_RING_TX_BASE_PTR_H		GENMASK(23, 16)
 #define MTK_WED_WPDMA_RING_RX(_n)			(0x700 + (_n) * 0x10)
 #define MTK_WED_WPDMA_RING_RX_DATA(_n)			(0x730 + (_n) * 0x10)
 
@@ -419,6 +426,7 @@ struct mtk_wdma_desc {
 #define MTK_WED_WDMA_GLO_CFG_TX_DDONE_CHK		BIT(1)
 #define MTK_WED_WDMA_GLO_CFG_RX_DRV_EN			BIT(2)
 #define MTK_WED_WDMA_GLO_CFG_RX_DRV_BUSY		BIT(3)
+#define MTK_WED_WDMA_GLO_CFG_RECYCLE_DESC		BIT(4)
 #define MTK_WED_WDMA_GLO_CFG_BT_SIZE			GENMASK(5, 4)
 #define MTK_WED_WDMA_GLO_CFG_TX_WB_DDONE		BIT(6)
 #define MTK_WED_WDMA_GLO_CFG_RX_DIS_FSM_AUTO_IDLE	BIT(13)
@@ -433,6 +441,7 @@ struct mtk_wdma_desc {
 #define MTK_WED_WDMA_GLO_CFG_DYNAMIC_SKIP_DMAD_PREP	BIT(24)
 #define MTK_WED_WDMA_GLO_CFG_DYNAMIC_DMAD_RECYCLE	BIT(25)
 #define MTK_WED_WDMA_GLO_CFG_RST_INIT_COMPLETE		BIT(26)
+#define MTK_WED_WDMA_GLO_CFG_DYNAMIC_SKIP_DMAD_THRES	BIT(28)
 #define MTK_WED_WDMA_GLO_CFG_RXDRV_CLKGATE_BYPASS	BIT(30)
 
 #define MTK_WED_WDMA_RESET_IDX				0xa08
@@ -468,6 +477,7 @@ struct mtk_wdma_desc {
 
 #define MTK_WED_RX_BM_RX_DMAD				0xd80
 #define MTK_WED_RX_BM_RX_DMAD_SDL0			GENMASK(13, 0)
+#define MTK_WED_RX_BM_RX_DMAD_BASE_PTR_H		GENMASK(23, 16)
 
 #define MTK_WED_RX_BM_BASE				0xd84
 #define MTK_WED_RX_BM_INIT_PTR				0xd88
@@ -749,12 +759,14 @@ struct mtk_wdma_desc {
 #define MTK_WED_RRO_MSDU_PG_DRV_EN			BIT(31)
 
 #define MTK_WED_RRO_MSDU_PG_CTRL0(_n)			(0xe5c + (_n) * 0xc)
+#define MTK_WED_RRO_MSDU_PG_BASE_PTR_H			GENMASK(23, 16)
 #define MTK_WED_RRO_MSDU_PG_CTRL1(_n)			(0xe60 + (_n) * 0xc)
 #define MTK_WED_RRO_MSDU_PG_CTRL2(_n)			(0xe64 + (_n) * 0xc)
 
 #define MTK_WED_RRO_RX_D_RX(_n)				(0xe80 + (_n) * 0x10)
 #define MTK_WED_RRO_RX_D_RX_CNT(_n)			(0xe84 + (_n) * 0x10)
 #define MTK_WED_RRO_RX_D_RX_MAX_CNT			GENMASK(11, 0)
+#define MTK_WED_RRO_RX_D_RX_BASE_PTR_H			GENMASK(23, 16)
 #define MTK_WED_RRO_RX_D_RX_MAGIC_CNT			GENMASK(31, 28)
 
 #define MTK_WED_RRO_RX_MAGIC_CNT			BIT(13)
@@ -765,6 +777,7 @@ struct mtk_wdma_desc {
 
 #define MTK_WED_RRO_PG_BM_RX_DMAM			0xeb0
 #define MTK_WED_RRO_PG_BM_RX_SDL0			GENMASK(13, 0)
+#define MTK_WED_RRO_PG_BM_RX_BASE_PTR_H			GENMASK(23, 16)
 
 #define MTK_WED_RRO_PG_BM_BASE				0xeb4
 #define MTK_WED_RRO_PG_BM_INIT_PTR			0xeb8
@@ -803,6 +816,7 @@ struct mtk_wdma_desc {
 #define MTK_WED_RX_IND_CMD_BUSY			GENMASK(31, 0)
 
 #define MTK_WED_RX_IND_CMD_CNT0				0xf20
+#define MTK_WED_RX_IND_CMD_DBG_CNT_RST			BIT(30)
 #define MTK_WED_RX_IND_CMD_DBG_CNT_EN			BIT(31)
 
 #define MTK_WED_RX_IND_CMD_CNT(_n)			(0xf20 + (_n) * 0x4)
@@ -897,10 +911,49 @@ struct mtk_wdma_desc {
 
 #define MTK_WED_MON_AMSDU_HIFTXD_FETCH_BUFF(_n)		(0x1e90 + (_n - 1) * 0x4)
 #define MTK_WED_MON_AMSDU_HIFTXD_FETCH_MSDU(_n)		(0x1ec4 + (_n - 1) * 0x4)
+#define MTK_WED_AMSDU_DBG_CFG				0x1fe0
+#define MTK_WED_AMSDU_DBG_CFG_EN			BIT(18)
+#define MTK_WED_AMSDU_DBG_CNT				0x1fe4
+#define MTK_WED_AMSDU_DBG_CNT_CLR			BIT(0)
 
 #define MTK_WED_PCIE_BASE			0x11280000
 
 #define MTK_WED_PCIE_BASE0			0x11300000
 #define MTK_WED_PCIE_BASE1			0x11310000
 #define MTK_WED_PCIE_BASE2			0x11290000
+
+/* wed 3.1*/
+#define MTK_WED_INT_CTRL					0x218
+#define MTK_WED_INT_CTRL_RX0_DONE_TRIG		GENMASK(4, 0)
+#define MTK_WED_INT_CTRL_RX1_DONE_TRIG		GENMASK(12, 8)
+#define MTK_WED_HIFTXD_BASE_L(_n)		(0x320 + (_n) * 0x4)
+#define MTK_WED_TX_BM_RANGE_CFG			0x098
+#define MTK_WED_WPDMA_CFG_TX0_CIDX		0x588
+#define MTK_WED_WPDMA_CFG_TX_FREE_CIDX		0x58c
+#define MTK_WED_WPDMA_CFG_TX_FREE_DIDX		0x594
+#define MTK_WED_WPDMA_CFG_TX1_CIDX		0x5A0
+#define MTK_WED_WPDMA_CFG_TX0_DIDX		0x5b0
+#define MTK_WED_WPDMA_CFG_TX1_DIDX		0x5cc
+#define MTK_WED_WPDMA_RRO3_1_RX_D_RX		0x814
+#define MTK_WED_WPDMA_RRO3_1_RX_D_RX_BASE_PTR_H		GENMASK(23, 16)
+#define MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG		0x824
+#define MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG_RX_DRV_EN	BIT(0)
+#define MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG_RX_DRV_BUSY	BIT(1)
+#define MTK_WED_WPDMA_RRO3_1_RX_GLO_CFG_BASE	0x834
+#define MTK_WED_WPDMA_RRO3_1_RX_D_RX_MIB	0x840
+#define MTK_WED_WPDMA_RRO3_1_RX_PREF_CFG	0x850
+#define MTK_WED_WPDMA_RRO3_1_RX_PREF_EN		BIT(0)
+#define MTK_WED_WPDMA_RRO3_1_RX_PREF_BUSY	BIT(1)
+#define MTK_WED_WPDMA_RRO3_1_RX_RING_CIDX	0x864
+#define MTK_WED_WPDMA_RRO3_1_RX_RING_DIDX	0x888
+#define MTK_WED_RRO_CTRL			0x8fc
+#define MTK_WED_RRO_CTRL_VER_SEL		BIT(0)
+#define MTK_WED_WPDMA_RRO_RX_RING_CIDX(_n)	(0xea0 + (_n) * 0x4)
+#define MTK_WED_WPDMA_RRO_RX_RING_DIDX(_n)	(0xef8 + (_n) * 0x4)
+#define MTK_WED_RRO_MSDU_PG_RING_CFG_CIDX(_n)	(0xde8 + (_n) * 0x8)
+#define MTK_WED_RX_BM_RANGE_CFG			0xda0
+#define MTK_WED_RX_BM_RANGE_SW_BUF		GENMASK(15, 0)
+#define MTK_WED_RRO_MSDU_PG_RING_CFG_DIDX(_n)	(0xe44 + (_n) * 0x8)
+#define MTK_WED_RRO_PG_BM_RANGE_CFG		0xec8
+#define MTK_WED_RRO_PG_BM_RANGE_SW_BUF		GENMASK(15, 0)
 #endif
diff --git a/drivers/net/ethernet/mediatek/mtk_wed_wo.c b/drivers/net/ethernet/mediatek/mtk_wed_wo.c
index cca29e7..f851300 100644
--- a/drivers/net/ethernet/mediatek/mtk_wed_wo.c
+++ b/drivers/net/ethernet/mediatek/mtk_wed_wo.c
@@ -512,6 +512,9 @@ int mtk_wed_wo_init(struct mtk_wed_hw *hw)
 	struct mtk_wed_wo *wo;
 	int ret = 0;
 
+	if (!hw->soc->wo_support)
+		return 0;
+
 	wo = kzalloc(sizeof(struct mtk_wed_wo), GFP_KERNEL);
 	if (!wo)
 		return -ENOMEM;
@@ -550,7 +553,7 @@ void mtk_wed_wo_exit(struct mtk_wed_hw *hw)
 {
 	struct mtk_wed_wo *wo = hw->wed_wo;
 
-	if(!wo)
+	if(!hw->soc->wo_support || !wo)
 		return;
 
 	wed_wo_hardware_exit(wo);
diff --git a/include/linux/soc/mediatek/mtk_wed.h b/include/linux/soc/mediatek/mtk_wed.h
index 83a4b8b..c994e98 100644
--- a/include/linux/soc/mediatek/mtk_wed.h
+++ b/include/linux/soc/mediatek/mtk_wed.h
@@ -51,11 +51,25 @@ struct mtk_wed_bm_desc {
 	__le32 token;
 } __packed __aligned(4);
 
-enum mtk_wed_bus_tye{
+enum mtk_wed_bus_tye {
 	MTK_WED_BUS_PCIE,
 	MTK_WED_BUS_AXI,
 };
 
+enum mtk_wed_hwrro_mode {
+	MTK_WED_HWRRO_DISABLE,
+	MTK_WED_HWRRO_V3,
+	MTK_WED_HWRRO_V3_1,
+};
+
+enum mtk_wed_hw_version {
+	MTK_WED_DISABLE,
+	MTK_WED_HW_V1,
+	MTK_WED_HW_V2,
+	MTK_WED_HW_V3,
+	MTK_WED_HW_V3_1 = 5,
+};
+
 #define MTK_WED_RING_CONFIGURED		BIT(0)
 struct mtk_wed_ring {
 	struct mtk_wdma_desc *desc;
@@ -103,6 +117,7 @@ struct mtk_wed_device {
 	struct mtk_wed_ring rx_rro_ring[MTK_WED_RX_QUEUES];
 	struct mtk_wed_ring rx_page_ring[MTK_WED_RX_PAGE_QUEUES];
 	struct mtk_wed_ring ind_cmd_ring;
+	struct mtk_wed_ring rx_rro_3_1_ring;
 
 	struct {
 		int size;
@@ -146,18 +161,20 @@ struct mtk_wed_device {
 		u32 wpdma_phys;
 		u32 wpdma_int;
 		u32 wpdma_mask;
-		u32 wpdma_tx;
+		u32 wpdma_tx[MTK_WED_TX_QUEUES];
 		u32 wpdma_txfree;
 		u32 wpdma_rx_glo;
 		u32 wpdma_rx[MTK_WED_RX_QUEUES];
 		u32 wpdma_rx_rro[MTK_WED_RX_QUEUES];
 		u32 wpdma_rx_pg;
+		u32 wpdma_rro_3_1_rx;
 
 		u8 tx_tbit[MTK_WED_TX_QUEUES];
 		u8 rx_tbit[MTK_WED_RX_QUEUES];
 		u8 rro_rx_tbit[MTK_WED_RX_QUEUES];
 		u8 rx_pg_tbit[MTK_WED_RX_PAGE_QUEUES];
 		u8 txfree_tbit;
+		u8 rro_3_1_rx_tbit;
 
 		u16 token_start;
 		unsigned int nbuf;
@@ -167,7 +184,7 @@ struct mtk_wed_device {
 		unsigned int amsdu_max_len;
 
 		bool wcid_512;
-		bool hw_rro;
+		enum mtk_wed_hwrro_mode hw_rro;
 		bool msi;
 
 		u8 amsdu_max_subframes;
@@ -222,6 +239,7 @@ struct mtk_wed_ops {
 
 	u32 (*irq_get)(struct mtk_wed_device *dev, u32 mask);
 	void (*irq_set_mask)(struct mtk_wed_device *dev, u32 mask);
+	int (*get_hw_version)(void);
 	void (*start_hw_rro)(struct mtk_wed_device *dev, u32 irq_mask, bool reset);
 	void (*rro_rx_ring_setup)(struct mtk_wed_device *dev, int ring,
 				  void __iomem *regs);
@@ -229,6 +247,8 @@ struct mtk_wed_ops {
 				      void __iomem *regs);
 	int (*ind_rx_ring_setup)(struct mtk_wed_device *dev,
 				 void __iomem *regs);
+	void (*rro_3_1_rx_ring_setup)(struct mtk_wed_device *dev,
+				      void __iomem *regs);
 };
 
 extern const struct mtk_wed_ops __rcu *mtk_soc_wed_ops;
@@ -253,14 +273,38 @@ mtk_wed_device_attach(struct mtk_wed_device *dev)
 	return ret;
 }
 
+static inline int
+mtk_wed_device_get_hw_version(void)
+{
+	int ret = MTK_WED_DISABLE;
+	const struct mtk_wed_ops *ops;
+#ifdef CONFIG_NET_MEDIATEK_SOC_WED
+	rcu_read_lock();
+	ops = rcu_dereference(mtk_soc_wed_ops);
+	if (ops)
+		ret = ops->get_hw_version();
+
+	rcu_read_unlock();
+#endif
+
+	return ret;
+}
+
 static inline bool
 mtk_wed_get_rx_capa(struct mtk_wed_device *dev)
 {
 #ifdef CONFIG_NET_MEDIATEK_SOC_WED
-	if (dev->version == 3)
+	switch (dev->version) {
+	case MTK_WED_HW_V3:
+	case MTK_WED_HW_V3_1:
 		return dev->wlan.hw_rro;
+	case MTK_WED_HW_V2:
+		return true;
+	case MTK_WED_HW_V1:
+	default:
+		return false;
+	}
 
-	return dev->version != 1;
 #else
 	return false;
 #endif
@@ -270,7 +314,7 @@ static inline bool
 mtk_wed_is_amsdu_supported(struct mtk_wed_device *dev)
 {
 #ifdef CONFIG_NET_MEDIATEK_SOC_WED
-	return dev->version == 3;
+	return dev->version > MTK_WED_HW_V2;
 #else
 	return false;
 #endif
@@ -310,6 +354,8 @@ mtk_wed_is_amsdu_supported(struct mtk_wed_device *dev)
 	(_dev)->ops->msdu_pg_rx_ring_setup(_dev, _ring, _regs)
 #define mtk_wed_device_ind_rx_ring_setup(_dev, _regs) \
 	(_dev)->ops->ind_rx_ring_setup(_dev, _regs)
+#define mtk_wed_device_rro_3_1_rx_ring_setup(_dev, _regs) \
+	(_dev)->ops->rro_3_1_rx_ring_setup(_dev, _regs)
 #else
 static inline bool mtk_wed_device_active(struct mtk_wed_device *dev)
 {
@@ -333,6 +379,7 @@ static inline bool mtk_wed_device_active(struct mtk_wed_device *dev)
 #define mtk_wed_device_rro_rx_ring_setup(_dev, _ring, _regs) -ENODEV
 #define mtk_wed_device_msdu_pg_rx_ring_setup(_dev, _ring, _regs)  -ENODEV
 #define mtk_wed_device_ind_rx_ring_setup(_dev, _regs)  -ENODEV
+#define mtk_wed_device_rro_3_1_rx_ring_setup(_dev, _regs) -ENODEV
 #endif
 
 #endif
-- 
2.45.2

