From 2981641936880b965df6160ba71f08312f9d4826 Mon Sep 17 00:00:00 2001
From: Bo-Cun Chen <bc-bocun.chen@mediatek.com>
Date: Tue, 7 Oct 2025 14:26:32 +0800
Subject: [PATCH] net: ethernet: mtk_eth_soc: support proprietary debugfs

---
 drivers/net/ethernet/mediatek/Makefile      |    2 +-
 drivers/net/ethernet/mediatek/mtk_eth_dbg.c | 2390 +++++++++++++++++++
 drivers/net/ethernet/mediatek/mtk_eth_dbg.h |  458 ++++
 drivers/net/ethernet/mediatek/mtk_eth_soc.c |   47 +-
 drivers/net/ethernet/mediatek/mtk_eth_soc.h |   58 +
 5 files changed, 2944 insertions(+), 11 deletions(-)
 create mode 100644 drivers/net/ethernet/mediatek/mtk_eth_dbg.c
 create mode 100644 drivers/net/ethernet/mediatek/mtk_eth_dbg.h

diff --git a/drivers/net/ethernet/mediatek/Makefile b/drivers/net/ethernet/mediatek/Makefile
index ddbb7f4..24f7d04 100644
--- a/drivers/net/ethernet/mediatek/Makefile
+++ b/drivers/net/ethernet/mediatek/Makefile
@@ -4,7 +4,7 @@
 #
 
 obj-$(CONFIG_NET_MEDIATEK_SOC) += mtk_eth.o
-mtk_eth-y := mtk_eth_soc.o mtk_eth_path.o mtk_ppe.o mtk_ppe_debugfs.o mtk_ppe_offload.o
+mtk_eth-y := mtk_eth_soc.o mtk_eth_path.o mtk_eth_dbg.o mtk_ppe.o mtk_ppe_debugfs.o mtk_ppe_offload.o
 mtk_eth-$(CONFIG_NET_MEDIATEK_SOC_WED) += mtk_wed.o mtk_wed_mcu.o mtk_wed_wo.o
 ifdef CONFIG_DEBUG_FS
 mtk_eth-$(CONFIG_NET_MEDIATEK_SOC_WED) += mtk_wed_debugfs.o
diff --git a/drivers/net/ethernet/mediatek/mtk_eth_dbg.c b/drivers/net/ethernet/mediatek/mtk_eth_dbg.c
new file mode 100644
index 0000000..551c3f4
--- /dev/null
+++ b/drivers/net/ethernet/mediatek/mtk_eth_dbg.c
@@ -0,0 +1,2390 @@
+/*
+ *   Copyright (C) 2025 MediaTek Inc.
+ *
+ *   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; version 2 of the License
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   Copyright (C) 2009-2016 John Crispin <blogic@openwrt.org>
+ *   Copyright (C) 2009-2016 Felix Fietkau <nbd@openwrt.org>
+ *   Copyright (C) 2013-2016 Michael Lee <igvtee@gmail.com>
+ */
+#include <linux/of_address.h>
+#include <linux/of_mdio.h>
+#include <linux/debugfs.h>
+#include <linux/kernel.h>
+#include <linux/phylink.h>
+#include <linux/proc_fs.h>
+
+#include "mtk_eth_soc.h"
+#include "mtk_eth_dbg.h"
+#include "mtk_wed_regs.h"
+
+static int mtk_qdma_pppq_show(struct seq_file *m, void *v)
+{
+	struct mtk_eth *eth = m->private;
+	int i;
+
+	seq_puts(m, "Usage of the QDMA PPPQ for the HW path:\n");
+	for (i = 0; i < MTK_QDMA_NUM_QUEUES; i++)
+		seq_printf(m, "qdma_txq%d:	%5d Mbps %8d refcnt\n",
+			   i, eth->qdma_shaper.speed[i],
+			   atomic_read(&eth->qdma_shaper.refcnt[i]));
+	seq_printf(m, "qdma_thres:	%5d Mbps\n", eth->qdma_shaper.threshold);
+
+	return 0;
+}
+
+static int mtk_qdma_pppq_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mtk_qdma_pppq_show, inode->i_private);
+}
+
+static ssize_t mtk_qdma_pppq_write(struct file *file, const char __user *buffer,
+				   size_t count, loff_t *data)
+{
+	struct seq_file *m = (struct seq_file *)file->private_data;
+	struct mtk_eth *eth = m->private;
+	char buf[8] = {0};
+	u32 threshold;
+	int len = count;
+
+	if ((len > 8) || copy_from_user(buf, buffer, len))
+		return -EFAULT;
+
+	if (sscanf(buf, "%6d", &threshold) != 1)
+		return -EFAULT;
+
+	if (threshold > 10000 || threshold < 0) {
+		pr_warn("Threshold must be between 0 and 10000 Mbps\n");
+		return -EINVAL;
+	}
+
+	eth->qdma_shaper.threshold = threshold;
+
+	return len;
+}
+
+static const struct file_operations mtk_eth_debugfs_qdma_pppq_fops = {
+	.owner = THIS_MODULE,
+	.open = mtk_qdma_pppq_open,
+	.read = seq_read,
+	.write = mtk_qdma_pppq_write,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static ssize_t mtk_qdma_sched_read(struct file *file, char __user *user_buf,
+				   size_t count, loff_t *ppos)
+{
+	struct mtk_qdma_ctx *qdma_ctx = file->private_data;
+	struct mtk_eth *eth = qdma_ctx->eth;
+	const struct mtk_soc_data *soc = eth->soc;
+	long id = qdma_ctx->id;
+	char *buf;
+	unsigned int len = 0, buf_len = 1500;
+	u32 qdma_tx_sch, sch_reg;
+	int enable, scheduling, max_rate, scheduler, i;
+	ssize_t ret_cnt;
+
+	buf = kzalloc(buf_len, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	if (mtk_is_netsys_v2_or_greater(eth))
+		qdma_tx_sch = mtk_r32(eth, soc->reg_map->qdma.tx_sch_rate + (id >> 1) * 0x4);
+	else
+		qdma_tx_sch = mtk_r32(eth, soc->reg_map->qdma.tx_sch_rate);
+
+	if (id & 0x1)
+		qdma_tx_sch >>= 16;
+
+	qdma_tx_sch &= MTK_QDMA_TX_SCH;
+	enable = FIELD_GET(MTK_QDMA_TX_SCH_RATE_EN, qdma_tx_sch);
+	scheduling = FIELD_GET(MTK_QDMA_TX_SCH_MAX_WFQ, qdma_tx_sch);
+	max_rate = FIELD_GET(MTK_QDMA_TX_SCH_RATE_MAN, qdma_tx_sch);
+	qdma_tx_sch = FIELD_GET(MTK_QDMA_TX_SCH_RATE_EXP, qdma_tx_sch);
+	while (qdma_tx_sch--)
+		max_rate *= 10;
+
+	len += scnprintf(buf + len, buf_len - len,
+			 "EN\tScheduling\tMAX\tQueue#\n%d\t%s%16d\t", enable,
+			 (scheduling == 1) ? "WRR" : "SP", max_rate);
+
+	for (i = 0; i < MTK_QDMA_NUM_QUEUES; i++) {
+		mtk_w32(eth, (i / MTK_QTX_PER_PAGE), soc->reg_map->qdma.page);
+		sch_reg = mtk_r32(eth, soc->reg_map->qdma.qtx_sch +
+				       (i % MTK_QTX_PER_PAGE) * MTK_QTX_OFFSET);
+		if (mtk_is_netsys_v2_or_greater(eth))
+			scheduler = FIELD_GET(MTK_QTX_SCH_TX_SEL_V2, sch_reg);
+		else
+			scheduler = FIELD_GET(MTK_QTX_SCH_TX_SEL, sch_reg);
+		if (id == scheduler)
+			len += scnprintf(buf + len, buf_len - len, "%d  ", i);
+	}
+
+	len += scnprintf(buf + len, buf_len - len, "\n");
+	if (len > buf_len)
+		len = buf_len;
+
+	ret_cnt = simple_read_from_buffer(user_buf, count, ppos, buf, len);
+
+	kfree(buf);
+
+	return ret_cnt;
+}
+
+static ssize_t mtk_qdma_sched_write(struct file *file, const char __user *buf,
+				    size_t length, loff_t *offset)
+{
+	struct mtk_qdma_ctx *qdma_ctx = file->private_data;
+	struct mtk_eth *eth = qdma_ctx->eth;
+	const struct mtk_soc_data *soc = eth->soc;
+	long id = qdma_ctx->id;
+	char line[64] = {0}, scheduling[32];
+	int enable, rate, exp = 0, shift = 0;
+	size_t size;
+	u32 qdma_tx_sch, val = 0;
+
+	if (length >= sizeof(line))
+		return -EINVAL;
+
+	if (copy_from_user(line, buf, length))
+		return -EFAULT;
+
+	if (sscanf(line, "%1d %3s %9d", &enable, scheduling, &rate) != 3)
+		return -EFAULT;
+
+	if (mtk_is_netsys_v3_or_greater(eth)) {
+		if (rate > 10000000 || rate < 0)
+			return -EINVAL;
+	} else {
+		if (rate > 1000000 || rate < 0)
+			return -EINVAL;
+	}
+
+	while (rate > 127) {
+		rate /= 10;
+		exp++;
+	}
+
+	line[length] = '\0';
+
+	if (enable)
+		val |= MTK_QDMA_TX_SCH_RATE_EN;
+	if (strcmp(scheduling, "sp") != 0)
+		val |= MTK_QDMA_TX_SCH_MAX_WFQ;
+	val |= FIELD_PREP(MTK_QDMA_TX_SCH_RATE_MAN, rate);
+	val |= FIELD_PREP(MTK_QDMA_TX_SCH_RATE_EXP, exp);
+	if (id & 0x1)
+		shift = 16;
+
+	if (mtk_is_netsys_v2_or_greater(eth))
+		qdma_tx_sch = mtk_r32(eth, soc->reg_map->qdma.tx_sch_rate + (id >> 1) * 0x4);
+	else
+		qdma_tx_sch = mtk_r32(eth, soc->reg_map->qdma.tx_sch_rate);
+
+	qdma_tx_sch &= ~(MTK_QDMA_TX_SCH << shift);
+	qdma_tx_sch |= val << shift;
+	if (mtk_is_netsys_v2_or_greater(eth))
+		mtk_w32(eth, qdma_tx_sch, soc->reg_map->qdma.tx_sch_rate + (id >> 1) * 0x4);
+	else
+		mtk_w32(eth, qdma_tx_sch, soc->reg_map->qdma.tx_sch_rate);
+
+	size = strlen(line);
+	*offset += size;
+
+	return length;
+}
+
+static const struct file_operations mtk_eth_debugfs_qdma_sched_fops = {
+	.owner = THIS_MODULE,
+	.open = simple_open,
+	.read = mtk_qdma_sched_read,
+	.write = mtk_qdma_sched_write,
+	.llseek = default_llseek,
+};
+
+static ssize_t mtk_qdma_queue_read(struct file *file, char __user *user_buf,
+				   size_t count, loff_t *ppos)
+{
+	struct mtk_qdma_ctx *qdma_ctx = file->private_data;
+	struct mtk_eth *eth = qdma_ctx->eth;
+	const struct mtk_soc_data *soc = eth->soc;
+	long id = qdma_ctx->id;
+	char *buf;
+	unsigned int len = 0, buf_len = 1500;
+	u32 qtx_sch, qtx_cfg;
+	int scheduler;
+	int min_rate_en, min_rate, min_rate_exp;
+	int max_rate_en, max_weight, max_rate, max_rate_exp;
+	ssize_t ret_cnt;
+
+	buf = kzalloc(buf_len, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	mtk_w32(eth, (id / MTK_QTX_PER_PAGE), soc->reg_map->qdma.page);
+	qtx_cfg = mtk_r32(eth, soc->reg_map->qdma.qtx_cfg +
+			       (id % MTK_QTX_PER_PAGE) * MTK_QTX_OFFSET);
+	qtx_sch = mtk_r32(eth, soc->reg_map->qdma.qtx_sch +
+			       (id % MTK_QTX_PER_PAGE) * MTK_QTX_OFFSET);
+	if (mtk_is_netsys_v2_or_greater(eth))
+		scheduler = FIELD_GET(MTK_QTX_SCH_TX_SEL_V2, qtx_sch);
+	else
+		scheduler = FIELD_GET(MTK_QTX_SCH_TX_SEL, qtx_sch);
+
+	min_rate_en = FIELD_GET(MTK_QTX_SCH_MIN_RATE_EN, qtx_sch);
+	if (mtk_is_netsys_v3_or_greater(eth) && (eth->soc->caps != MT7988_CAPS)) {
+		min_rate = FIELD_GET(MTK_QTX_SCH_MIN_RATE_MAN_V3, qtx_sch);
+		min_rate_exp = FIELD_GET(MTK_QTX_SCH_MIN_RATE_EXP_V3, qtx_sch);
+		max_rate_en = FIELD_GET(MTK_QTX_SCH_MAX_RATE_EN_V3, qtx_sch);
+		max_weight = FIELD_GET(MTK_QTX_SCH_MAX_RATE_WEIGHT_V3, qtx_sch);
+		max_rate = FIELD_GET(MTK_QTX_SCH_MAX_RATE_MAN_V3, qtx_sch);
+		max_rate_exp = FIELD_GET(MTK_QTX_SCH_MAX_RATE_EXP_V3, qtx_sch);
+	} else {
+		min_rate = FIELD_GET(MTK_QTX_SCH_MIN_RATE_MAN, qtx_sch);
+		min_rate_exp = FIELD_GET(MTK_QTX_SCH_MIN_RATE_EXP, qtx_sch);
+		max_rate_en = FIELD_GET(MTK_QTX_SCH_MAX_RATE_EN, qtx_sch);
+		max_weight = FIELD_GET(MTK_QTX_SCH_MAX_RATE_WEIGHT, qtx_sch);
+		max_rate = FIELD_GET(MTK_QTX_SCH_MAX_RATE_MAN, qtx_sch);
+		max_rate_exp = FIELD_GET(MTK_QTX_SCH_MAX_RATE_EXP, qtx_sch);
+	}
+
+	while (min_rate_exp--)
+		min_rate *= 10;
+
+	while (max_rate_exp--)
+		max_rate *= 10;
+
+	len += scnprintf(buf + len, buf_len - len,
+			 "scheduler: %d\nhw resv: %d\nsw resv: %d\n", scheduler,
+			 (qtx_cfg >> 8) & 0xff, qtx_cfg & 0xff);
+
+	if (mtk_is_netsys_v2_or_greater(eth)) {
+		/* Switch to debug mode */
+		mtk_m32(eth, MTK_MIB_ON_QTX_CFG, MTK_MIB_ON_QTX_CFG, soc->reg_map->qdma.qtx_mib_if);
+		mtk_m32(eth, MTK_VQTX_MIB_EN, MTK_VQTX_MIB_EN, soc->reg_map->qdma.qtx_mib_if);
+		qtx_cfg = mtk_r32(eth, soc->reg_map->qdma.qtx_cfg +
+				       (id % MTK_QTX_PER_PAGE) * MTK_QTX_OFFSET);
+		qtx_sch = mtk_r32(eth, soc->reg_map->qdma.qtx_sch +
+				       (id % MTK_QTX_PER_PAGE) * MTK_QTX_OFFSET);
+		len += scnprintf(buf + len, buf_len - len,
+				 "packet count: %u\n", qtx_cfg);
+		len += scnprintf(buf + len, buf_len - len,
+				 "packet drop: %u\n\n", qtx_sch);
+
+		/* Recover to normal mode */
+		mtk_m32(eth, MTK_MIB_ON_QTX_CFG, 0, soc->reg_map->qdma.qtx_mib_if);
+		mtk_m32(eth, MTK_VQTX_MIB_EN, 0, soc->reg_map->qdma.qtx_mib_if);
+	}
+
+	len += scnprintf(buf + len, buf_len - len,
+			 "      EN     RATE     WEIGHT\n");
+	len += scnprintf(buf + len, buf_len - len,
+			 "----------------------------\n");
+	len += scnprintf(buf + len, buf_len - len,
+			 "max%5d%9d%9d\n", max_rate_en, max_rate, max_weight);
+	len += scnprintf(buf + len, buf_len - len,
+			 "min%5d%9d        -\n", min_rate_en, min_rate);
+
+	if (len > buf_len)
+		len = buf_len;
+
+	ret_cnt = simple_read_from_buffer(user_buf, count, ppos, buf, len);
+
+	kfree(buf);
+
+	return ret_cnt;
+}
+
+static ssize_t mtk_qdma_queue_write(struct file *file, const char __user *buf,
+				    size_t length, loff_t *offset)
+{
+	struct mtk_qdma_ctx *qdma_ctx = file->private_data;
+	struct mtk_eth *eth = qdma_ctx->eth;
+	const struct mtk_soc_data *soc = eth->soc;
+	long id = qdma_ctx->id;
+	char line[64] = {0};
+	int max_enable, max_rate, max_exp = 0;
+	int min_enable, min_rate, min_exp = 0;
+	int weight;
+	int resv;
+	int scheduler;
+	size_t size;
+	u32 qtx_sch = 0, qtx_cfg = 0;
+
+	mtk_w32(eth, (id / MTK_QTX_PER_PAGE), soc->reg_map->qdma.page);
+	if (length >= sizeof(line))
+		return -EINVAL;
+
+	if (copy_from_user(line, buf, length))
+		return -EFAULT;
+
+	if (sscanf(line, "%d %d %d %d %d %d %d", &scheduler, &min_enable, &min_rate,
+		   &max_enable, &max_rate, &weight, &resv) != 7)
+		return -EFAULT;
+
+	line[length] = '\0';
+
+	if (mtk_is_netsys_v3_or_greater(eth)) {
+		if (max_rate > 10000000 || max_rate < 0 ||
+		    min_rate > 10000000 || min_rate < 0)
+			return -EINVAL;
+	} else {
+		if (max_rate > 1000000 || max_rate < 0 ||
+		    min_rate > 1000000 || min_rate < 0)
+			return -EINVAL;
+	}
+
+	while (max_rate > 127) {
+		max_rate /= 10;
+		max_exp++;
+	}
+
+	while (min_rate > 127) {
+		min_rate /= 10;
+		min_exp++;
+	}
+
+	if (mtk_is_netsys_v2_or_greater(eth))
+		qtx_sch |= FIELD_PREP(MTK_QTX_SCH_TX_SEL_V2, scheduler);
+	else
+		qtx_sch |= FIELD_PREP(MTK_QTX_SCH_TX_SEL, scheduler);
+
+	qtx_sch |= FIELD_PREP(MTK_QTX_SCH_LEAKY_BUCKET_SIZE, 3);
+
+	if (min_enable)
+		qtx_sch |= MTK_QTX_SCH_MIN_RATE_EN;
+	if (mtk_is_netsys_v3_or_greater(eth) && (eth->soc->caps != MT7988_CAPS)) {
+		qtx_sch |= FIELD_PREP(MTK_QTX_SCH_MIN_RATE_MAN_V3, min_rate);
+		qtx_sch |= FIELD_PREP(MTK_QTX_SCH_MIN_RATE_EXP_V3, min_exp);
+		if (max_enable)
+			qtx_sch |= MTK_QTX_SCH_MAX_RATE_EN_V3;
+		qtx_sch |= FIELD_PREP(MTK_QTX_SCH_MAX_RATE_WEIGHT_V3, weight);
+		qtx_sch |= FIELD_PREP(MTK_QTX_SCH_MAX_RATE_MAN_V3, max_rate);
+		qtx_sch |= FIELD_PREP(MTK_QTX_SCH_MAX_RATE_EXP_V3, max_exp);
+	} else {
+		qtx_sch |= FIELD_PREP(MTK_QTX_SCH_MIN_RATE_MAN, min_rate);
+		qtx_sch |= FIELD_PREP(MTK_QTX_SCH_MIN_RATE_EXP, min_exp);
+		if (max_enable)
+			qtx_sch |= MTK_QTX_SCH_MAX_RATE_EN;
+		qtx_sch |= FIELD_PREP(MTK_QTX_SCH_MAX_RATE_WEIGHT, weight);
+		qtx_sch |= FIELD_PREP(MTK_QTX_SCH_MAX_RATE_MAN, max_rate);
+		qtx_sch |= FIELD_PREP(MTK_QTX_SCH_MAX_RATE_EXP, max_exp);
+	}
+	mtk_w32(eth, qtx_sch, soc->reg_map->qdma.qtx_sch +
+			      (id % MTK_QTX_PER_PAGE) * MTK_QTX_OFFSET);
+
+	qtx_cfg = mtk_r32(eth, soc->reg_map->qdma.qtx_cfg +
+			       (id % MTK_QTX_PER_PAGE) * MTK_QTX_OFFSET);
+	qtx_cfg &= 0xffff0000;
+	qtx_cfg |= FIELD_PREP(MTK_QTX_CFG_HW_RESV, resv);
+	qtx_cfg |= FIELD_PREP(MTK_QTX_CFG_SW_RESV, resv);
+	mtk_w32(eth, qtx_cfg, soc->reg_map->qdma.qtx_cfg +
+			      (id % MTK_QTX_PER_PAGE) * MTK_QTX_OFFSET);
+
+	size = strlen(line);
+	*offset += size;
+
+	return length;
+}
+
+static const struct file_operations mtk_eth_debugfs_qdma_queue_fops = {
+	.owner = THIS_MODULE,
+	.open = simple_open,
+	.read = mtk_qdma_queue_read,
+	.write = mtk_qdma_queue_write,
+	.llseek = default_llseek,
+};
+
+static void mt7530_mdio_w32(struct mtk_eth *eth, u16 reg, u32 val)
+{
+	mutex_lock(&eth->mii_bus->mdio_lock);
+
+	if (eth->debugfs->mt7530.mmio_base)
+		__raw_writel(val, eth->debugfs->mt7530.mmio_base + reg);
+	else {
+		_mtk_mdio_write_c22(eth, 0x1f, 0x1f, (reg >> 6) & 0x3ff);
+		_mtk_mdio_write_c22(eth, 0x1f, (reg >> 2) & 0xf, val & 0xffff);
+		_mtk_mdio_write_c22(eth, 0x1f, 0x10, val >> 16);
+	}
+
+	mutex_unlock(&eth->mii_bus->mdio_lock);
+}
+
+static u32 mt7530_mdio_r32(struct mtk_eth *eth, u32 reg)
+{
+	u16 high, low;
+	u32 ret;
+
+	mutex_lock(&eth->mii_bus->mdio_lock);
+
+	if (eth->debugfs->mt7530.mmio_base) {
+		ret = __raw_readl(eth->debugfs->mt7530.mmio_base + reg);
+		mutex_unlock(&eth->mii_bus->mdio_lock);
+		return ret;
+	}
+	_mtk_mdio_write_c22(eth, 0x1f, 0x1f, (reg >> 6) & 0x3ff);
+	low = _mtk_mdio_read_c22(eth, 0x1f, (reg >> 2) & 0xf);
+	high = _mtk_mdio_read_c22(eth, 0x1f, 0x10);
+
+	mutex_unlock(&eth->mii_bus->mdio_lock);
+
+	return (high << 16) | (low & 0xffff);
+}
+
+static bool mt7530_sw_detect(struct mtk_eth *eth)
+{
+	struct device_node *np;
+	u32 sw_id;
+	u32 rev;
+
+	/* mt7988 with built-in 7531 */
+	np = of_find_compatible_node(NULL, NULL, "mediatek,mt7988-switch");
+	if (np) {
+		of_node_put(np);
+		return 1;
+	}
+	/* external 753x */
+	rev = mt7530_mdio_r32(eth, 0x781c);
+	sw_id = (rev & 0xffff0000) >> 16;
+	if (sw_id == 0x7530 || sw_id == 0x7531)
+		return 1;
+
+	return 0;
+}
+
+static bool mt7530_is_exist(struct mtk_eth *eth)
+{
+	return eth->debugfs->mt7530.exist;
+}
+
+void mtk_switch_w32(struct mtk_eth *eth, u32 val, unsigned int reg)
+{
+	mtk_w32(eth, val, reg + 0x10000);
+}
+EXPORT_SYMBOL(mtk_switch_w32);
+
+u32 mtk_switch_r32(struct mtk_eth *eth, unsigned int reg)
+{
+	return mtk_r32(eth, reg + 0x10000);
+}
+EXPORT_SYMBOL(mtk_switch_r32);
+
+static int mtk_eth_debugfs_c22_phyregs_show(struct seq_file *m, void *private)
+{
+	struct mtk_eth *eth = m->private;
+	struct fwnode_handle *phy_fwnode;
+	struct phy_device *phydev;
+	struct mtk_mac *mac;
+	u32 data;
+	int i, j;
+
+	for (i = 0 ; i < MTK_MAX_DEVS ; i++) {
+		mac = eth->mac[i];
+		if (!mac ||
+		    of_phy_is_fixed_link(mac->of_node))
+			continue;
+
+		phy_fwnode = fwnode_get_phy_node(of_fwnode_handle(mac->of_node));
+		if (IS_ERR(phy_fwnode))
+			continue;
+
+		phydev = fwnode_phy_find_device(phy_fwnode);
+		/* We're done with the phy_node handle */
+		fwnode_handle_put(phy_fwnode);
+		if (!phydev)
+			continue;
+
+		j = 0;
+		while (j < 30) {
+			data = mdiobus_read(eth->mii_bus, phydev->mdio.addr, j);
+
+			seq_printf(m, "phy=%d, reg=0x%02x, data=0x%04x\n",
+				   phydev->mdio.addr, j, data);
+			j++;
+		}
+	}
+	return 0;
+}
+DEFINE_SHOW_ATTRIBUTE(mtk_eth_debugfs_c22_phyregs);
+
+static int mtk_eth_debugfs_mt7530sw_regs_show(struct seq_file *m, void *private)
+{
+	struct mtk_eth *eth = m->private;
+	u32  offset, data;
+	int i;
+	struct mt7530_ranges {
+		u32 start;
+		u32 end;
+	} ranges[] = {
+		{0x0, 0xac},
+		{0x1000, 0x10e0},
+		{0x1100, 0x1140},
+		{0x1200, 0x1240},
+		{0x1300, 0x1340},
+		{0x1400, 0x1440},
+		{0x1500, 0x1540},
+		{0x1600, 0x1640},
+		{0x1800, 0x1848},
+		{0x1900, 0x1948},
+		{0x1a00, 0x1a48},
+		{0x1b00, 0x1b48},
+		{0x1c00, 0x1c48},
+		{0x1d00, 0x1d48},
+		{0x1e00, 0x1e48},
+		{0x1f60, 0x1ffc},
+		{0x2000, 0x212c},
+		{0x2200, 0x222c},
+		{0x2300, 0x232c},
+		{0x2400, 0x242c},
+		{0x2500, 0x252c},
+		{0x2600, 0x262c},
+		{0x3000, 0x3014},
+		{0x30c0, 0x30f8},
+		{0x3100, 0x3114},
+		{0x3200, 0x3214},
+		{0x3300, 0x3314},
+		{0x3400, 0x3414},
+		{0x3500, 0x3514},
+		{0x3600, 0x3614},
+		{0x4000, 0x40d4},
+		{0x4100, 0x41d4},
+		{0x4200, 0x42d4},
+		{0x4300, 0x43d4},
+		{0x4400, 0x44d4},
+		{0x4500, 0x45d4},
+		{0x4600, 0x46d4},
+		{0x4f00, 0x461c},
+		{0x7000, 0x7038},
+		{0x7120, 0x7124},
+		{0x7800, 0x7804},
+		{0x7810, 0x7810},
+		{0x7830, 0x7830},
+		{0x7a00, 0x7a7c},
+		{0x7b00, 0x7b04},
+		{0x7e00, 0x7e04},
+		{0x7ffc, 0x7ffc},
+	};
+
+	if (!mt7530_is_exist(eth)) {
+		seq_puts(m, "no switch found\n");
+		return -EOPNOTSUPP;
+	}
+	for (i = 0 ; i < ARRAY_SIZE(ranges) ; i++) {
+		for (offset = ranges[i].start;
+		     offset <= ranges[i].end; offset += 4) {
+			data =  mt7530_mdio_r32(eth, offset);
+			seq_printf(m, "mt7530 switch reg=0x%08x, data=0x%08x\n",
+				   offset, data);
+		}
+	}
+
+	return 0;
+}
+DEFINE_SHOW_ATTRIBUTE(mtk_eth_debugfs_mt7530sw_regs);
+
+static ssize_t mtk_eth_debugfs_reset_write(struct file *file, const char __user *ptr,
+					   size_t len, loff_t *off)
+{
+	struct mtk_eth *eth = file->private_data;
+	unsigned long dbg_level = 0;
+	char buf[8] = "";
+	int count = len;
+
+	len = min((size_t)count, sizeof(buf) - 1);
+	if (copy_from_user(buf, ptr, len))
+		return -EFAULT;
+
+	buf[len] = '\0';
+	if (kstrtoul(buf, 0, &dbg_level))
+		return -EINVAL;
+
+	switch (dbg_level) {
+	case 0:
+		atomic_set(&eth->reset.force, 0);
+		break;
+	case 1:
+		if (atomic_read(&eth->reset.force) && !test_bit(MTK_RESETTING, &eth->state))
+			schedule_work(&eth->pending_work);
+		else
+			pr_info(" stat:disable\n");
+		break;
+	case 2:
+		atomic_set(&eth->reset.force, 1);
+		break;
+	default:
+		pr_info("Usage: echo [level] > /sys/kernel/debug/mtketh/reset\n");
+		pr_info("Commands:   [level]\n");
+		pr_info("		0	disable FE force reset\n");
+		pr_info("		1	trigger FE and WDMA force reset\n");
+		pr_info("		2	enable FE force reset\n");
+		break;
+	}
+
+	return count;
+}
+
+static const struct file_operations mtk_eth_debugfs_reset_fops = {
+	.owner = THIS_MODULE,
+	.open = simple_open,
+	.write = mtk_eth_debugfs_reset_write,
+	.llseek = noop_llseek,
+};
+
+static void mii_mgr_read_combine(struct mtk_eth *eth, u32 phy_addr, u32 phy_register,
+			  u32 *read_data)
+{
+	if (mt7530_is_exist(eth) && phy_addr == 31)
+		*read_data = mt7530_mdio_r32(eth, phy_register);
+
+	else
+		*read_data = mdiobus_read(eth->mii_bus, phy_addr, phy_register);
+}
+
+static void mii_mgr_write_combine(struct mtk_eth *eth, u16 phy_addr, u16 phy_register,
+			   u32 write_data)
+{
+	if (mt7530_is_exist(eth) && phy_addr == 31)
+		mt7530_mdio_w32(eth, phy_register, write_data);
+
+	else
+		mdiobus_write(eth->mii_bus, phy_addr, phy_register, write_data);
+}
+
+static void mii_mgr_read_cl45(struct mtk_eth *eth, u16 port, u16 devad, u16 reg, u16 *data)
+{
+	*data = mdiobus_c45_read(eth->mii_bus, port, devad, reg);
+}
+
+static void mii_mgr_write_cl45(struct mtk_eth *eth, u16 port, u16 devad, u16 reg, u16 data)
+{
+	mdiobus_c45_write(eth->mii_bus, port, devad, reg, data);
+}
+
+int mtk_eth_debugfs_priv_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
+{
+	struct mtk_mac *mac = netdev_priv(dev);
+	struct mtk_eth *eth = mac->hw;
+	struct mtk_mii_ioctl_data mii;
+	struct mtk_esw_reg reg;
+	u16 val;
+
+	switch (cmd) {
+	case MTKETH_MII_READ:
+		if (copy_from_user(&mii, ifr->ifr_data, sizeof(mii)))
+			goto err_copy;
+		mii_mgr_read_combine(eth, mii.phy_id, mii.reg_num,
+				     &mii.val_out);
+		if (copy_to_user(ifr->ifr_data, &mii, sizeof(mii)))
+			goto err_copy;
+
+		return 0;
+	case MTKETH_MII_WRITE:
+		if (copy_from_user(&mii, ifr->ifr_data, sizeof(mii)))
+			goto err_copy;
+		mii_mgr_write_combine(eth, mii.phy_id, mii.reg_num,
+				      mii.val_in);
+		return 0;
+	case MTKETH_MII_READ_CL45:
+		if (copy_from_user(&mii, ifr->ifr_data, sizeof(mii)))
+			goto err_copy;
+		mii_mgr_read_cl45(eth,
+				  mdio_phy_id_prtad(mii.phy_id),
+				  mdio_phy_id_devad(mii.phy_id),
+				  mii.reg_num,
+				  &val);
+		mii.val_out = val;
+		if (copy_to_user(ifr->ifr_data, &mii, sizeof(mii)))
+			goto err_copy;
+
+		return 0;
+	case MTKETH_MII_WRITE_CL45:
+		if (copy_from_user(&mii, ifr->ifr_data, sizeof(mii)))
+			goto err_copy;
+		val = mii.val_in;
+		mii_mgr_write_cl45(eth,
+				  mdio_phy_id_prtad(mii.phy_id),
+				  mdio_phy_id_devad(mii.phy_id),
+				  mii.reg_num,
+				  val);
+		return 0;
+	case MTKETH_ESW_REG_READ:
+		if (!mt7530_is_exist(eth))
+			return -EOPNOTSUPP;
+		if (copy_from_user(&reg, ifr->ifr_data, sizeof(reg)))
+			goto err_copy;
+		if (reg.off > REG_ESW_MAX)
+			return -EINVAL;
+		reg.val = mtk_switch_r32(eth, reg.off);
+
+		if (copy_to_user(ifr->ifr_data, &reg, sizeof(reg)))
+			goto err_copy;
+
+		return 0;
+	case MTKETH_ESW_REG_WRITE:
+		if (!mt7530_is_exist(eth))
+			return -EOPNOTSUPP;
+		if (copy_from_user(&reg, ifr->ifr_data, sizeof(reg)))
+			goto err_copy;
+		if (reg.off > REG_ESW_MAX)
+			return -EINVAL;
+		mtk_switch_w32(eth, reg.val, reg.off);
+
+		return 0;
+	default:
+		break;
+	}
+
+	return -EOPNOTSUPP;
+err_copy:
+	return -EFAULT;
+}
+
+static void mtk_gdm_reg_dump_v3(struct seq_file *m, struct mtk_eth *eth,
+				u32 gdm_id, u32 mib_base)
+{
+	seq_printf(m, "| GDMA%d_RX_GBCNT  : %010u (Rx Good Bytes)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base));
+	seq_printf(m, "| GDMA%d_RX_GPCNT  : %010u (Rx Good Pkts)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x08));
+	seq_printf(m, "| GDMA%d_RX_OERCNT : %010u (overflow error)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x10));
+	seq_printf(m, "| GDMA%d_RX_FERCNT : %010u (FCS error)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x14));
+	seq_printf(m, "| GDMA%d_RX_SERCNT : %010u (too short)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x18));
+	seq_printf(m, "| GDMA%d_RX_LERCNT : %010u (too long)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x1C));
+	seq_printf(m, "| GDMA%d_RX_CERCNT : %010u (checksum error)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x20));
+	seq_printf(m, "| GDMA%d_RX_FCCNT  : %010u (flow control)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x24));
+	seq_printf(m, "| GDMA%d_RX_VDPCNT : %010u (VID drop)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x28));
+	seq_printf(m, "| GDMA%d_RX_PFCCNT : %010u (priority flow control)\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x2C));
+	seq_printf(m, "| GDMA%d_TX_GBCNT  : %010u (Tx Good Bytes)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x40));
+	seq_printf(m, "| GDMA%d_TX_GPCNT  : %010u (Tx Good Pkts)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x48));
+	seq_printf(m, "| GDMA%d_TX_SKIPCNT: %010u (abort count)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x50));
+	seq_printf(m, "| GDMA%d_TX_COLCNT : %010u (collision count)|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x54));
+	seq_printf(m, "| GDMA%d_TX_OERCNT : %010u (overflow error)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x58));
+	seq_printf(m, "| GDMA%d_TX_FCCNT  : %010u (flow control)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x60));
+	seq_printf(m, "| GDMA%d_TX_PFCCNT : %010u (priority flow control)\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x64));
+	seq_puts(m, "|						|\n");
+}
+
+static void mtk_gdm_reg_dump_v2(struct seq_file *m, struct mtk_eth *eth,
+				u32 gdm_id, u32 mib_base)
+{
+	seq_printf(m, "| GDMA%d_RX_GBCNT  : %010u (Rx Good Bytes)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base));
+	seq_printf(m, "| GDMA%d_RX_GPCNT  : %010u (Rx Good Pkts)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x08));
+	seq_printf(m, "| GDMA%d_RX_OERCNT : %010u (overflow error)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x10));
+	seq_printf(m, "| GDMA%d_RX_FERCNT : %010u (FCS error)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x14));
+	seq_printf(m, "| GDMA%d_RX_SERCNT : %010u (too short)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x18));
+	seq_printf(m, "| GDMA%d_RX_LERCNT : %010u (too long)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x1C));
+	seq_printf(m, "| GDMA%d_RX_CERCNT : %010u (checksum error)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x20));
+	seq_printf(m, "| GDMA%d_RX_FCCNT  : %010u (flow control)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x24));
+	seq_printf(m, "| GDMA%d_TX_SKIPCNT: %010u (abort count)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x28));
+	seq_printf(m, "| GDMA%d_TX_COLCNT : %010u (collision count)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x2C));
+	seq_printf(m, "| GDMA%d_TX_GBCNT  : %010u (Tx Good Bytes)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x30));
+	seq_printf(m, "| GDMA%d_TX_GPCNT  : %010u (Tx Good Pkts)	|\n",
+		   gdm_id, mtk_r32(eth, mib_base + 0x38));
+	seq_puts(m, "|						|\n");
+}
+
+static void mtk_gdm_cnt_read(struct seq_file *m, struct mtk_eth *eth)
+{
+	const struct mtk_reg_map *reg_map = eth->soc->reg_map;
+	struct mtk_hw_stats *hw_stats;
+	u32 i, mib_base;
+
+	seq_puts(m, "\n			<<CPU>>\n");
+	seq_puts(m, "			   |\n");
+	seq_puts(m, "+-----------------------------------------------+\n");
+	seq_puts(m, "|		  <<PSE>>		        |\n");
+	seq_puts(m, "+-----------------------------------------------+\n");
+	seq_puts(m, "			   |\n");
+	seq_puts(m, "+-----------------------------------------------+\n");
+	seq_puts(m, "|		  <<GDMA>>		        |\n");
+
+	for (i = 0; i < MTK_MAX_DEVS; i++) {
+		if (!eth->mac[i] || !eth->mac[i]->hw_stats)
+			continue;
+
+		hw_stats = eth->mac[i]->hw_stats;
+
+		mib_base = reg_map->gdm1_cnt + hw_stats->reg_offset * i;
+
+		if (mtk_is_netsys_v3_or_greater(eth))
+			mtk_gdm_reg_dump_v3(m, eth, i + 1, mib_base);
+		else
+			mtk_gdm_reg_dump_v2(m, eth, i + 1, mib_base);
+	}
+
+	seq_puts(m, "+-----------------------------------------------+\n");
+}
+
+static void mt7530_dump_each_port(struct seq_file *m, struct mtk_eth *eth, u32 base)
+{
+	u32 pkt_cnt = 0;
+	int i = 0;
+
+	for (i = 0; i < 7; i++) {
+		if (mtk_is_netsys_v3_or_greater(eth)) {
+			if ((base == 0x402C) && (i == 6))
+				base = 0x408C;
+			else if ((base == 0x408C) && (i == 6))
+				base = 0x402C;
+		}
+		pkt_cnt = mt7530_mdio_r32(eth, (base) + (i * 0x100));
+		seq_printf(m, "%8u ", pkt_cnt);
+	}
+	seq_puts(m, "\n");
+}
+
+static int mtk_esw_cnt_read(struct seq_file *m, void *v)
+{
+	struct mtk_eth *eth = m->private;
+
+	mtk_gdm_cnt_read(m, eth);
+
+	if (!mt7530_is_exist(eth))
+		return 0;
+
+	seq_printf(m, "===================== %8s %8s %8s %8s %8s %8s %8s\n",
+		   "Port0", "Port1", "Port2", "Port3", "Port4", "Port5",
+		   "Port6");
+	seq_puts(m, "Tx Drop Packet      :");
+	mt7530_dump_each_port(m, eth, 0x4000);
+	seq_puts(m, "Tx CRC Error        :");
+	mt7530_dump_each_port(m, eth, 0x4004);
+	seq_puts(m, "Tx Unicast Packet   :");
+	mt7530_dump_each_port(m, eth, 0x4008);
+	seq_puts(m, "Tx Multicast Packet :");
+	mt7530_dump_each_port(m, eth, 0x400C);
+	seq_puts(m, "Tx Broadcast Packet :");
+	mt7530_dump_each_port(m, eth, 0x4010);
+	seq_puts(m, "Tx Collision Event  :");
+	mt7530_dump_each_port(m, eth, 0x4014);
+	seq_puts(m, "Tx Pause Packet     :");
+	mt7530_dump_each_port(m, eth, 0x402C);
+	seq_puts(m, "Rx Drop Packet      :");
+	mt7530_dump_each_port(m, eth, 0x4060);
+	seq_puts(m, "Rx Filtering Packet :");
+	mt7530_dump_each_port(m, eth, 0x4064);
+	seq_puts(m, "Rx Unicast Packet   :");
+	mt7530_dump_each_port(m, eth, 0x4068);
+	seq_puts(m, "Rx Multicast Packet :");
+	mt7530_dump_each_port(m, eth, 0x406C);
+	seq_puts(m, "Rx Broadcast Packet :");
+	mt7530_dump_each_port(m, eth, 0x4070);
+	seq_puts(m, "Rx Alignment Error  :");
+	mt7530_dump_each_port(m, eth, 0x4074);
+	seq_puts(m, "Rx CRC Error	    :");
+	mt7530_dump_each_port(m, eth, 0x4078);
+	seq_puts(m, "Rx Undersize Error  :");
+	mt7530_dump_each_port(m, eth, 0x407C);
+	seq_puts(m, "Rx Fragment Error   :");
+	mt7530_dump_each_port(m, eth, 0x4080);
+	seq_puts(m, "Rx Oversize Error   :");
+	mt7530_dump_each_port(m, eth, 0x4084);
+	seq_puts(m, "Rx Jabber Error     :");
+	mt7530_dump_each_port(m, eth, 0x4088);
+	seq_puts(m, "Rx Pause Packet     :");
+	mt7530_dump_each_port(m, eth, 0x408C);
+	mt7530_mdio_w32(eth, 0x4fe0, 0xf0);
+	mt7530_mdio_w32(eth, 0x4fe0, 0x800000f0);
+
+	seq_puts(m, "\n");
+
+	return 0;
+}
+
+static int mtk_esw_cnt_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mtk_esw_cnt_read, inode->i_private);
+}
+
+static const struct file_operations mtk_eth_debugfs_esw_cnt_fops = {
+	.owner = THIS_MODULE,
+	.open = mtk_esw_cnt_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release
+};
+
+static void mtk_mac_mib_dump(struct seq_file *m, u32 gdm_id)
+{
+	struct mtk_eth *eth = m->private;
+
+	PRINT_FORMATTED_MAC_MIB64(m, TX_UC_PKT_CNT);
+	PRINT_FORMATTED_MAC_MIB64(m, TX_UC_BYTE_CNT);
+	PRINT_FORMATTED_MAC_MIB64(m, TX_MC_PKT_CNT);
+	PRINT_FORMATTED_MAC_MIB64(m, TX_MC_BYTE_CNT);
+	PRINT_FORMATTED_MAC_MIB64(m, TX_BC_PKT_CNT);
+	PRINT_FORMATTED_MAC_MIB64(m, TX_BC_BYTE_CNT);
+
+	PRINT_FORMATTED_MAC_MIB64(m, RX_UC_PKT_CNT);
+	PRINT_FORMATTED_MAC_MIB64(m, RX_UC_BYTE_CNT);
+	PRINT_FORMATTED_MAC_MIB64(m, RX_MC_PKT_CNT);
+	PRINT_FORMATTED_MAC_MIB64(m, RX_MC_BYTE_CNT);
+	PRINT_FORMATTED_MAC_MIB64(m, RX_BC_PKT_CNT);
+	PRINT_FORMATTED_MAC_MIB64(m, RX_BC_BYTE_CNT);
+}
+
+static int mtk_eth_debugfs_mac_cnt_show(struct seq_file *m, void *v)
+{
+	int i;
+
+	seq_puts(m, "+------------------------------------+\n");
+	seq_puts(m, "|              <<GMAC>>              |\n");
+
+	for (i = MTK_GMAC1_ID; i < MTK_GMAC_ID_MAX; i++) {
+		mtk_mac_mib_dump(m, i);
+		seq_puts(m, "|                                    |\n");
+	}
+
+	seq_puts(m, "+------------------------------------+\n");
+
+	return 0;
+}
+
+static int mtk_mac_cnt_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mtk_eth_debugfs_mac_cnt_show, inode->i_private);
+}
+DEFINE_SHOW_ATTRIBUTE(mtk_eth_debugfs_mac_cnt);
+
+static void mtk_xfi_mib_dump(struct seq_file *m, u32 gdm_id)
+{
+	struct mtk_eth *eth = m->private;
+
+	PRINT_FORMATTED_XFI_MIB(m, TX_PKT_CNT, GENMASK(31, 0));
+	PRINT_FORMATTED_XFI_MIB(m, TX_ETH_CNT, GENMASK(31, 0));
+	PRINT_FORMATTED_XFI_MIB(m, TX_PAUSE_CNT, GENMASK(15, 0));
+	PRINT_FORMATTED_XFI_MIB(m, TX_BYTE_CNT, GENMASK(31, 0));
+	PRINT_FORMATTED_XFI_MIB64(m, TX_UC_PKT_CNT);
+	PRINT_FORMATTED_XFI_MIB64(m, TX_MC_PKT_CNT);
+	PRINT_FORMATTED_XFI_MIB64(m, TX_BC_PKT_CNT);
+
+	PRINT_FORMATTED_XFI_MIB(m, RX_PKT_CNT, GENMASK(31, 0));
+	PRINT_FORMATTED_XFI_MIB(m, RX_ETH_CNT, GENMASK(31, 0));
+	PRINT_FORMATTED_XFI_MIB(m, RX_PAUSE_CNT, GENMASK(15, 0));
+	PRINT_FORMATTED_XFI_MIB(m, RX_LEN_ERR_CNT, GENMASK(15, 0));
+	PRINT_FORMATTED_XFI_MIB(m, RX_CRC_ERR_CNT, GENMASK(15, 0));
+	if (eth->soc->caps != MT7988_CAPS)
+		PRINT_FORMATTED_XFI_MIB(m, RX_RUNT_PKT_CNT, GENMASK(31, 0));
+	PRINT_FORMATTED_XFI_MIB64(m, RX_UC_PKT_CNT);
+	PRINT_FORMATTED_XFI_MIB64(m, RX_MC_PKT_CNT);
+	PRINT_FORMATTED_XFI_MIB64(m, RX_BC_PKT_CNT);
+	PRINT_FORMATTED_XFI_MIB(m, RX_UC_DROP_CNT, GENMASK(31, 0));
+	PRINT_FORMATTED_XFI_MIB(m, RX_BC_DROP_CNT, GENMASK(31, 0));
+	PRINT_FORMATTED_XFI_MIB(m, RX_MC_DROP_CNT, GENMASK(31, 0));
+	PRINT_FORMATTED_XFI_MIB(m, RX_ALL_DROP_CNT, GENMASK(31, 0));
+}
+
+static int mtk_eth_debugfs_xfi_cnt_show(struct seq_file *m, void *v)
+{
+	struct mtk_eth *eth = m->private;
+	int i;
+
+	seq_puts(m, "+------------------------------------+\n");
+	seq_puts(m, "|             <<XFI MAC>>            |\n");
+
+	for (i = MTK_GMAC2_ID; i < MTK_GMAC_ID_MAX; i++) {
+		mtk_xfi_mib_dump(m, i);
+		mtk_m32(eth, 0x1, 0x1, MTK_XFI_MIB_BASE(i) + MTK_XFI_CNT_CTRL);
+		seq_puts(m, "|                                    |\n");
+	}
+
+	seq_puts(m, "+------------------------------------+\n");
+
+	return 0;
+}
+
+static int mtk_xfi_cnt_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mtk_eth_debugfs_xfi_cnt_show, inode->i_private);
+}
+DEFINE_SHOW_ATTRIBUTE(mtk_eth_debugfs_xfi_cnt);
+
+static int mtk_tx_full_cnt_read(struct seq_file *m, void *v)
+{
+	struct mtk_eth *eth = m->private;
+
+	seq_printf(m, "tx ring full count: %d\n",
+		   atomic_xchg(&eth->tx_ring.full_count, 0));
+
+	return 0;
+}
+
+static int mtk_tx_full_cnt_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mtk_tx_full_cnt_read, inode->i_private);
+}
+
+static const struct file_operations mtk_eth_debugfs_tx_full_cnt_fops = {
+	.owner = THIS_MODULE,
+	.open = mtk_tx_full_cnt_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static int mtk_tx_ring_read(struct seq_file *m, void *v)
+{
+	struct mtk_eth *eth = m->private;
+	struct mtk_tx_ring *ring = &eth->tx_ring;
+	struct mtk_tx_dma_v2 *tx_ring;
+	int i = 0;
+
+	seq_printf(m, "free count = %d\n", (int)atomic_read(&ring->free_count));
+	seq_printf(m, "cpu next free: %d\n",
+		   (int)(ring->next_free - (struct mtk_tx_dma *)ring->dma));
+	seq_printf(m, "cpu last free: %d\n",
+		   (int)(ring->last_free - (struct mtk_tx_dma *)ring->dma));
+	for (i = 0; i < eth->soc->tx.dma_size; i++) {
+		dma_addr_t addr = ring->phys +
+				  (i << eth->soc->tx.desc_shift);
+
+		tx_ring = ring->dma + (i << eth->soc->tx.desc_shift);
+
+		seq_printf(m, "%04d (0x%llx): %08x %08x %08x %08x", i, addr,
+			   tx_ring->txd1, tx_ring->txd2,
+			   tx_ring->txd3, tx_ring->txd4);
+
+		if (mtk_is_netsys_v2_or_greater(eth)) {
+			seq_printf(m, " %08x %08x %08x %08x",
+				   tx_ring->txd5, tx_ring->txd6,
+				   tx_ring->txd7, tx_ring->txd8);
+		}
+
+		seq_puts(m, "\n");
+	}
+
+	return 0;
+}
+
+static int mtk_tx_ring_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mtk_tx_ring_read, inode->i_private);
+}
+
+static const struct file_operations mtk_eth_debugfs_tx_ring_fops = {
+	.owner = THIS_MODULE,
+	.open = mtk_tx_ring_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release
+};
+
+static int mtk_hwtx_ring_show(struct seq_file *m, void *v)
+{
+	struct mtk_eth *eth = m->private;
+	struct mtk_tx_dma_v2 *hwtx_ring;
+	int i = 0;
+
+	for (i = 0; i < eth->soc->tx.fq_dma_size; i++) {
+		dma_addr_t addr = eth->fq_ring.phy_scratch_ring +
+				  (i << eth->soc->tx.desc_shift);
+
+		hwtx_ring = eth->fq_ring.scratch_ring + (i << eth->soc->tx.desc_shift);
+
+		seq_printf(m, "%04d (0x%llx): %08x %08x %08x %08x", i, addr,
+			   hwtx_ring->txd1, hwtx_ring->txd2,
+			   hwtx_ring->txd3, hwtx_ring->txd4);
+
+		if (mtk_is_netsys_v2_or_greater(eth)) {
+			seq_printf(m, " %08x %08x %08x %08x",
+				   hwtx_ring->txd5, hwtx_ring->txd6,
+				   hwtx_ring->txd7, hwtx_ring->txd8);
+		}
+
+		seq_puts(m, "\n");
+	}
+
+	return 0;
+}
+
+static int mtk_hwtx_ring_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mtk_hwtx_ring_show, inode->i_private);
+}
+
+static const struct file_operations mtk_eth_debugfs_hwtx_ring_fops = {
+	.owner = THIS_MODULE,
+	.open = mtk_hwtx_ring_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release
+};
+
+static int mtk_rx_ring_show(struct seq_file *m, void *v)
+{
+	struct mtk_eth *eth = m->private;
+	struct mtk_rx_ring *ring;
+	struct mtk_rx_dma_v2 *rx_ring;
+	int i = 0, j = 0;
+
+	for (j = 0; j < MTK_MAX_RX_RING_NUM; j++) {
+		ring = &eth->rx_ring[j];
+		if (!ring->dma)
+			continue;
+
+		seq_printf(m, "[Ring%d] next to read: %d\n", j,
+			   NEXT_DESP_IDX(ring->calc_idx, eth->soc->rx.dma_size));
+		for (i = 0; i < ring->dma_size; i++) {
+			dma_addr_t addr = ring->phys +
+					  (i << eth->soc->tx.desc_shift);
+
+			rx_ring = ring->dma + (i << eth->soc->rx.desc_shift);
+
+			seq_printf(m, "%04d (0x%llx): %08x %08x %08x %08x", i, addr,
+				   rx_ring->rxd1, rx_ring->rxd2,
+				   rx_ring->rxd3, rx_ring->rxd4);
+
+			if (mtk_is_netsys_v3_or_greater(eth)) {
+				seq_printf(m, " %08x %08x %08x %08x",
+					   rx_ring->rxd5, rx_ring->rxd6,
+					   rx_ring->rxd7, rx_ring->rxd8);
+			}
+
+			seq_puts(m, "\n");
+		}
+	}
+
+	return 0;
+}
+
+static int mtk_rx_ring_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mtk_rx_ring_show, inode->i_private);
+}
+
+static const struct file_operations mtk_eth_debugfs_rx_ring_fops = {
+	.owner = THIS_MODULE,
+	.open = mtk_rx_ring_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release
+};
+
+static inline u32 mtk_dbg_r32(u32 reg)
+{
+	void __iomem *virt_reg;
+	u32 val;
+
+	virt_reg = ioremap(reg, 32);
+	val = __raw_readl(virt_reg);
+	iounmap(virt_reg);
+
+	return val;
+}
+
+static int mtk_dbg_regs_show(struct seq_file *m, void *v)
+{
+	struct mtk_eth *eth = m->private;
+	const struct mtk_reg_map *reg_map = eth->soc->reg_map;
+	u32 i;
+
+	seq_puts(m, "   <<DEBUG REG DUMP>>\n");
+
+	seq_printf(m, "| FE_INT_STA	: %08x |\n",
+		   mtk_r32(eth, MTK_FE_INT_STATUS));
+	if (mtk_is_netsys_v2_or_greater(eth))
+		seq_printf(m, "| FE_INT_STA2	: %08x |\n",
+			   mtk_r32(eth, MTK_FE_INT_STATUS2));
+
+	seq_printf(m, "| PSE_FQFC_CFG	: %08x |\n",
+		   mtk_r32(eth, MTK_PSE_FQFC_CFG));
+	seq_printf(m, "| PSE_IQ_STA1	: %08x |\n",
+		   mtk_r32(eth, reg_map->pse_iq_sta + 0x00));
+	seq_printf(m, "| PSE_IQ_STA2	: %08x |\n",
+		   mtk_r32(eth, reg_map->pse_iq_sta + 0x04));
+
+	if (mtk_is_netsys_v2_or_greater(eth)) {
+		seq_printf(m, "| PSE_IQ_STA3	: %08x |\n",
+			   mtk_r32(eth, reg_map->pse_iq_sta + 0x08));
+		seq_printf(m, "| PSE_IQ_STA4	: %08x |\n",
+			   mtk_r32(eth, reg_map->pse_iq_sta + 0x0c));
+		seq_printf(m, "| PSE_IQ_STA5	: %08x |\n",
+			   mtk_r32(eth, reg_map->pse_iq_sta + 0x10));
+		seq_printf(m, "| PSE_IQ_STA6	: %08x |\n",
+			   mtk_r32(eth, reg_map->pse_iq_sta + 0x14));
+		seq_printf(m, "| PSE_IQ_STA7	: %08x |\n",
+			   mtk_r32(eth, reg_map->pse_iq_sta + 0x18));
+		seq_printf(m, "| PSE_IQ_STA8	: %08x |\n",
+			   mtk_r32(eth, reg_map->pse_iq_sta + 0x1c));
+	}
+
+	seq_printf(m, "| PSE_OQ_STA1	: %08x |\n",
+		   mtk_r32(eth, reg_map->pse_oq_sta + 0x00));
+	seq_printf(m, "| PSE_OQ_STA2	: %08x |\n",
+		   mtk_r32(eth, reg_map->pse_oq_sta + 0x04));
+
+	if (mtk_is_netsys_v2_or_greater(eth)) {
+		seq_printf(m, "| PSE_OQ_STA3	: %08x |\n",
+			   mtk_r32(eth, reg_map->pse_oq_sta + 0x08));
+		seq_printf(m, "| PSE_OQ_STA4	: %08x |\n",
+			   mtk_r32(eth, reg_map->pse_oq_sta + 0x0c));
+		seq_printf(m, "| PSE_OQ_STA5	: %08x |\n",
+			   mtk_r32(eth, reg_map->pse_oq_sta + 0x10));
+		seq_printf(m, "| PSE_OQ_STA6	: %08x |\n",
+			   mtk_r32(eth, reg_map->pse_oq_sta + 0x14));
+		seq_printf(m, "| PSE_OQ_STA7	: %08x |\n",
+			   mtk_r32(eth, reg_map->pse_oq_sta + 0x18));
+		seq_printf(m, "| PSE_OQ_STA8	: %08x |\n",
+			   mtk_r32(eth, reg_map->pse_oq_sta + 0x1c));
+	}
+
+	seq_printf(m, "| PDMA_CRX_IDX	: %08x |\n",
+		   mtk_r32(eth, reg_map->pdma.pcrx_ptr));
+	seq_printf(m, "| PDMA_DRX_IDX	: %08x |\n",
+		   mtk_r32(eth, reg_map->pdma.pdrx_ptr));
+	if (MTK_HAS_CAPS(eth->soc->caps, MTK_RSS)) {
+		for (i = 1; i < eth->soc->rss_num; i++) {
+			seq_printf(m, "| PDMA_CRX_IDX%d	: %08x |\n", i,
+				   mtk_r32(eth, reg_map->pdma.pcrx_ptr +
+					   i * MTK_QRX_OFFSET));
+			seq_printf(m, "| PDMA_DRX_IDX%d	: %08x |\n", i,
+				   mtk_r32(eth, reg_map->pdma.pdrx_ptr +
+					   i * MTK_QRX_OFFSET));
+		}
+	}
+	if (MTK_HAS_CAPS(eth->soc->caps, MTK_HWLRO)) {
+		for (i = 0; i < MTK_HW_LRO_RING_NUM; i++) {
+			seq_printf(m, "| PDMA_CRX_IDX%d	: %08x |\n",
+				   MTK_HW_LRO_RING(i),
+				   mtk_r32(eth, reg_map->pdma.pcrx_ptr +
+					   MTK_HW_LRO_RING(i) * MTK_QRX_OFFSET));
+			seq_printf(m, "| PDMA_DRX_IDX%d	: %08x |\n",
+				   MTK_HW_LRO_RING(i),
+				   mtk_r32(eth, reg_map->pdma.pdrx_ptr +
+					   MTK_HW_LRO_RING(i) * MTK_QRX_OFFSET));
+		}
+	}
+	seq_printf(m, "| QDMA_CTX_IDX	: %08x |\n",
+		   mtk_r32(eth, reg_map->qdma.ctx_ptr));
+	seq_printf(m, "| QDMA_DTX_IDX	: %08x |\n",
+		   mtk_r32(eth, reg_map->qdma.dtx_ptr));
+	seq_printf(m, "| QDMA_FQ_CNT	: %08x |\n",
+		   mtk_r32(eth, reg_map->qdma.fq_count));
+	seq_printf(m, "| QDMA_FWD_CNT	: %08x |\n",
+		   mtk_r32(eth, reg_map->qdma.fwd_count));
+	seq_printf(m, "| QDMA_FSM	: %08x |\n",
+		   mtk_r32(eth, reg_map->qdma.fsm));
+	seq_printf(m, "| FE_PSE_FREE	: %08x |\n",
+		   mtk_r32(eth, MTK_FE_PSE_FREE));
+	seq_printf(m, "| FE_DROP_FQ	: %08x |\n",
+		   mtk_r32(eth, MTK_FE_DROP_FQ));
+	seq_printf(m, "| FE_DROP_FC	: %08x |\n",
+		   mtk_r32(eth, MTK_FE_DROP_FC));
+	seq_printf(m, "| FE_DROP_PPE	: %08x |\n",
+		   mtk_r32(eth, MTK_FE_DROP_PPE));
+	seq_printf(m, "| GDM1_IG_CTRL	: %08x |\n",
+		   mtk_r32(eth, MTK_GDMA_FWD_CFG(0)));
+	seq_printf(m, "| GDM2_IG_CTRL	: %08x |\n",
+		   mtk_r32(eth, MTK_GDMA_FWD_CFG(1)));
+	if (mtk_is_netsys_v3_or_greater(eth)) {
+		seq_printf(m, "| GDM3_IG_CTRL	: %08x |\n",
+			   mtk_r32(eth, MTK_GDMA_FWD_CFG(2)));
+	}
+	seq_printf(m, "| MAC_P1_MCR	: %08x |\n",
+		   mtk_r32(eth, MTK_MAC_MCR(0)));
+	seq_printf(m, "| MAC_P2_MCR	: %08x |\n",
+		   mtk_r32(eth, MTK_MAC_MCR(1)));
+	if (mtk_is_netsys_v3_or_greater(eth)) {
+		seq_printf(m, "| MAC_P3_MCR	: %08x |\n",
+			   mtk_r32(eth, MTK_MAC_MCR(2)));
+	}
+	seq_printf(m, "| MAC_P1_FSM	: %08x |\n",
+		   mtk_r32(eth, MTK_MAC_FSM(0)));
+	seq_printf(m, "| MAC_P2_FSM	: %08x |\n",
+		   mtk_r32(eth, MTK_MAC_FSM(1)));
+	if (mtk_is_netsys_v3_or_greater(eth)) {
+		seq_printf(m, "| MAC_P3_FSM	: %08x |\n",
+			   mtk_r32(eth, MTK_MAC_FSM(2)));
+	}
+
+	if (mtk_is_netsys_v3_or_greater(eth)) {
+		seq_printf(m, "| XMAC_P1_MCR	: %08x |\n",
+			   mtk_r32(eth, MTK_XMAC_MCR(1)));
+		seq_printf(m, "| XMAC_P1_STS	: %08x |\n",
+			   mtk_r32(eth, MTK_HAS_CAPS(eth->soc->caps, MTK_XGMAC_V2) ?
+					MTK_XMAC_STS(1) : MTK_XGMAC_STS(1)));
+		if (MTK_HAS_CAPS(eth->soc->caps, MTK_GMAC3_USXGMII)) {
+			seq_printf(m, "| XMAC_P2_MCR	: %08x |\n",
+				   mtk_r32(eth, MTK_XMAC_MCR(2)));
+			seq_printf(m, "| XMAC_P2_STS	: %08x |\n",
+				   mtk_r32(eth, MTK_HAS_CAPS(eth->soc->caps, MTK_XGMAC_V2) ?
+						MTK_XMAC_STS(2) : MTK_XGMAC_STS(2)));
+		}
+	}
+
+	if (mtk_is_netsys_v2_or_greater(eth)) {
+		seq_printf(m, "| FE_CDM1_FSM	: %08x |\n",
+			   mtk_r32(eth, MTK_FE_CDM1_FSM));
+		seq_printf(m, "| FE_CDM2_FSM	: %08x |\n",
+			   mtk_r32(eth, MTK_FE_CDM2_FSM));
+		seq_printf(m, "| FE_CDM3_FSM	: %08x |\n",
+			   mtk_r32(eth, MTK_FE_CDM3_FSM));
+		seq_printf(m, "| FE_CDM4_FSM	: %08x |\n",
+			   mtk_r32(eth, MTK_FE_CDM4_FSM));
+		seq_printf(m, "| FE_CDM5_FSM	: %08x |\n",
+			   mtk_r32(eth, MTK_FE_CDM5_FSM));
+		seq_printf(m, "| FE_CDM6_FSM	: %08x |\n",
+			   mtk_r32(eth, MTK_FE_CDM6_FSM));
+		seq_printf(m, "| FE_CDM7_FSM	: %08x |\n",
+			   mtk_r32(eth, MTK_FE_CDM7_FSM));
+		seq_printf(m, "| FE_GDM1_FSM	: %08x |\n",
+			   mtk_r32(eth, MTK_FE_GDM1_FSM));
+		seq_printf(m, "| FE_GDM2_FSM	: %08x |\n",
+			   mtk_r32(eth, MTK_FE_GDM2_FSM));
+		if (mtk_is_netsys_v3_or_greater(eth)) {
+			seq_printf(m, "| FE_GDM3_FSM	: %08x |\n",
+				   mtk_r32(eth, MTK_FE_GDM3_FSM));
+		}
+		seq_printf(m, "| SGMII_EFUSE	: %08x |\n",
+			   mtk_dbg_r32(MTK_SGMII_EFUSE));
+		seq_printf(m, "| SGMII0_RX_CNT : %08x |\n",
+			   mtk_dbg_r32(MTK_SGMII_FALSE_CARRIER_CNT(0)));
+		seq_printf(m, "| SGMII1_RX_CNT : %08x |\n",
+			   mtk_dbg_r32(MTK_SGMII_FALSE_CARRIER_CNT(1)));
+		seq_printf(m, "| WED_RTQM_GLO	: %08x |\n",
+			   mtk_dbg_r32(MTK_WED_RTQM_GLO_CFG));
+	}
+
+	mtk_w32(eth, 0xffffffff, MTK_FE_INT_STATUS);
+	if (mtk_is_netsys_v2_or_greater(eth))
+		mtk_w32(eth, 0xffffffff, MTK_FE_INT_STATUS2);
+
+	return 0;
+}
+
+static int mtk_dbg_regs_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mtk_dbg_regs_show, inode->i_private);
+}
+
+static const struct file_operations mtk_eth_debugfs_dbg_regs_fops = {
+	.owner = THIS_MODULE,
+	.open = mtk_dbg_regs_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release
+};
+
+void mtk_eth_debugfs_hwlro_stats_update(struct mtk_eth *eth, u32 ring_no,
+					struct mtk_rx_dma_v2 *rxd)
+{
+	u32 idx, agg_cnt, agg_size;
+
+	if (mtk_is_netsys_v3_or_greater(eth)) {
+		idx = ring_no - 4;
+		agg_cnt = FIELD_GET(RX_DMA_GET_AGG_CNT_V2, rxd->rxd6);
+	} else {
+		idx = ring_no - 1;
+		agg_cnt = FIELD_GET(RX_DMA_GET_AGG_CNT, rxd->rxd2);
+	}
+
+	if (idx >= MTK_HW_LRO_RING_NUM)
+		return;
+
+	agg_size = RX_DMA_GET_PLEN0(rxd->rxd2);
+
+	eth->debugfs->hwlro_stats.agg_size_cnt[idx][agg_size / 5000]++;
+	eth->debugfs->hwlro_stats.agg_num_cnt[idx][agg_cnt]++;
+	eth->debugfs->hwlro_stats.tot_flush_cnt[idx]++;
+	eth->debugfs->hwlro_stats.tot_agg_cnt[idx] += agg_cnt;
+}
+
+void mtk_eth_debugfs_hwlro_flush_stats_update(struct mtk_eth *eth, u32 ring_no,
+					      struct mtk_rx_dma_v2 *rxd)
+{
+	u32 idx, flush_reason;
+
+	if (mtk_is_netsys_v3_or_greater(eth)) {
+		idx = ring_no - 4;
+		flush_reason = FIELD_GET(RX_DMA_GET_FLUSH_RSN_V2, rxd->rxd6);
+	} else {
+		idx = ring_no - 1;
+		flush_reason = FIELD_GET(RX_DMA_GET_REV, rxd->rxd2);
+	}
+
+	if (idx >= MTK_HW_LRO_RING_NUM)
+		return;
+
+	if ((flush_reason & 0x7) == MTK_HW_LRO_AGG_FLUSH)
+		eth->debugfs->hwlro_stats.agg_flush_cnt[idx]++;
+	else if ((flush_reason & 0x7) == MTK_HW_LRO_AGE_FLUSH)
+		eth->debugfs->hwlro_stats.age_flush_cnt[idx]++;
+	else if ((flush_reason & 0x7) == MTK_HW_LRO_NOT_IN_SEQ_FLUSH)
+		eth->debugfs->hwlro_stats.seq_flush_cnt[idx]++;
+	else if ((flush_reason & 0x7) == MTK_HW_LRO_TIMESTAMP_FLUSH)
+		eth->debugfs->hwlro_stats.timestamp_flush_cnt[idx]++;
+	else if ((flush_reason & 0x7) == MTK_HW_LRO_NON_RULE_FLUSH)
+		eth->debugfs->hwlro_stats.norule_flush_cnt[idx]++;
+}
+
+static int mtk_hwlro_stats_show_v1(struct seq_file *m, struct mtk_eth *eth, void *v)
+{
+	int i;
+
+	seq_puts(m, "HW LRO statistic dump:\n");
+
+	/* Agg number count */
+	seq_puts(m, "Cnt:   RING1 | RING2 | RING3 | Total\n");
+	for (i = 0; i <= MTK_HW_LRO_MAX_AGG_CNT; i++) {
+		seq_printf(m, " %02d :      %d        %d        %d        %d\n", i,
+			   eth->debugfs->hwlro_stats.agg_num_cnt[0][i],
+			   eth->debugfs->hwlro_stats.agg_num_cnt[1][i],
+			   eth->debugfs->hwlro_stats.agg_num_cnt[2][i],
+			   (eth->debugfs->hwlro_stats.agg_num_cnt[0][i] +
+			    eth->debugfs->hwlro_stats.agg_num_cnt[1][i] +
+			    eth->debugfs->hwlro_stats.agg_num_cnt[2][i]));
+	}
+
+	/* Total agg count */
+	seq_puts(m, "Total agg:   RING1 | RING2 | RING3 | Total\n");
+	seq_printf(m, "                %d      %d      %d      %d\n",
+		   eth->debugfs->hwlro_stats.tot_agg_cnt[0],
+		   eth->debugfs->hwlro_stats.tot_agg_cnt[1],
+		   eth->debugfs->hwlro_stats.tot_agg_cnt[2],
+		   (eth->debugfs->hwlro_stats.tot_agg_cnt[0] +
+		    eth->debugfs->hwlro_stats.tot_agg_cnt[1] +
+		    eth->debugfs->hwlro_stats.tot_agg_cnt[2]));
+
+	/* Total flush count */
+	seq_puts(m, "Total flush:   RING1 | RING2 | RING3 | Total\n");
+	seq_printf(m, "                %d      %d      %d      %d\n",
+		   eth->debugfs->hwlro_stats.tot_flush_cnt[0],
+		   eth->debugfs->hwlro_stats.tot_flush_cnt[1],
+		   eth->debugfs->hwlro_stats.tot_flush_cnt[2],
+		   (eth->debugfs->hwlro_stats.tot_flush_cnt[0] +
+		    eth->debugfs->hwlro_stats.tot_flush_cnt[1] +
+		    eth->debugfs->hwlro_stats.tot_flush_cnt[2]));
+
+	/* Avg agg count */
+	seq_puts(m, "Avg agg:   RING1 | RING2 | RING3 | Total\n");
+	seq_printf(m, "                %d      %d      %d      %d\n",
+		   (eth->debugfs->hwlro_stats.tot_flush_cnt[0]) ?
+		    (eth->debugfs->hwlro_stats.tot_agg_cnt[0] /
+		     eth->debugfs->hwlro_stats.tot_flush_cnt[0]) : 0,
+		   (eth->debugfs->hwlro_stats.tot_flush_cnt[1]) ?
+		    (eth->debugfs->hwlro_stats.tot_agg_cnt[1] /
+		     eth->debugfs->hwlro_stats.tot_flush_cnt[1]) : 0,
+		   (eth->debugfs->hwlro_stats.tot_flush_cnt[2]) ?
+		    (eth->debugfs->hwlro_stats.tot_agg_cnt[2] /
+		     eth->debugfs->hwlro_stats.tot_flush_cnt[2]) : 0,
+		   (eth->debugfs->hwlro_stats.tot_flush_cnt[0] +
+		    eth->debugfs->hwlro_stats.tot_flush_cnt[1] +
+		    eth->debugfs->hwlro_stats.tot_flush_cnt[2]) ?
+		    ((eth->debugfs->hwlro_stats.tot_agg_cnt[0] +
+		      eth->debugfs->hwlro_stats.tot_agg_cnt[1] +
+		      eth->debugfs->hwlro_stats.tot_agg_cnt[2]) /
+		     (eth->debugfs->hwlro_stats.tot_flush_cnt[0] +
+		      eth->debugfs->hwlro_stats.tot_flush_cnt[1] +
+		      eth->debugfs->hwlro_stats.tot_flush_cnt[2])) : 0);
+
+	/*  Statistics of aggregation size counts */
+	seq_puts(m, "HW LRO flush pkt len:\n");
+	seq_puts(m, " Length  | RING1  | RING2  | RING3  | Total\n");
+	for (i = 0; i < 15; i++) {
+		seq_printf(m, "%05d~%05d: %d      %d      %d      %d\n",
+			   i * 5000, (i + 1) * 5000,
+			   eth->debugfs->hwlro_stats.agg_size_cnt[0][i],
+			   eth->debugfs->hwlro_stats.agg_size_cnt[1][i],
+			   eth->debugfs->hwlro_stats.agg_size_cnt[2][i],
+			   (eth->debugfs->hwlro_stats.agg_size_cnt[0][i] +
+			    eth->debugfs->hwlro_stats.agg_size_cnt[1][i] +
+			    eth->debugfs->hwlro_stats.agg_size_cnt[2][i]));
+	}
+
+	seq_puts(m, "Flush reason:   RING1 | RING2 | RING3 | Total\n");
+	seq_printf(m, "AGG timeout:      %d      %d      %d      %d\n",
+		   eth->debugfs->hwlro_stats.agg_flush_cnt[0],
+		   eth->debugfs->hwlro_stats.agg_flush_cnt[1],
+		   eth->debugfs->hwlro_stats.agg_flush_cnt[2],
+		   (eth->debugfs->hwlro_stats.agg_flush_cnt[0] +
+		    eth->debugfs->hwlro_stats.agg_flush_cnt[1] +
+		    eth->debugfs->hwlro_stats.agg_flush_cnt[2]));
+
+	seq_printf(m, "AGE timeout:      %d      %d      %d      %d\n",
+		   eth->debugfs->hwlro_stats.age_flush_cnt[0],
+		   eth->debugfs->hwlro_stats.age_flush_cnt[1],
+		   eth->debugfs->hwlro_stats.age_flush_cnt[2],
+		   (eth->debugfs->hwlro_stats.age_flush_cnt[0] +
+		    eth->debugfs->hwlro_stats.age_flush_cnt[1] +
+		    eth->debugfs->hwlro_stats.age_flush_cnt[2]));
+
+	seq_printf(m, "Not in-sequence:  %d      %d      %d      %d\n",
+		   eth->debugfs->hwlro_stats.seq_flush_cnt[0],
+		   eth->debugfs->hwlro_stats.seq_flush_cnt[1],
+		   eth->debugfs->hwlro_stats.seq_flush_cnt[2],
+		   (eth->debugfs->hwlro_stats.seq_flush_cnt[0] +
+		    eth->debugfs->hwlro_stats.seq_flush_cnt[1] +
+		    eth->debugfs->hwlro_stats.seq_flush_cnt[2]));
+
+	seq_printf(m, "Timestamp:        %d      %d      %d      %d\n",
+		   eth->debugfs->hwlro_stats.timestamp_flush_cnt[0],
+		   eth->debugfs->hwlro_stats.timestamp_flush_cnt[1],
+		   eth->debugfs->hwlro_stats.timestamp_flush_cnt[2],
+		   (eth->debugfs->hwlro_stats.timestamp_flush_cnt[0] +
+		    eth->debugfs->hwlro_stats.timestamp_flush_cnt[1] +
+		    eth->debugfs->hwlro_stats.timestamp_flush_cnt[2]));
+
+	seq_printf(m, "No LRO rule:      %d      %d      %d      %d\n",
+		   eth->debugfs->hwlro_stats.norule_flush_cnt[0],
+		   eth->debugfs->hwlro_stats.norule_flush_cnt[1],
+		   eth->debugfs->hwlro_stats.norule_flush_cnt[2],
+		   (eth->debugfs->hwlro_stats.norule_flush_cnt[0] +
+		    eth->debugfs->hwlro_stats.norule_flush_cnt[1] +
+		    eth->debugfs->hwlro_stats.norule_flush_cnt[2]));
+
+	return 0;
+}
+
+static int mtk_hwlro_stats_show_v2(struct seq_file *m, struct mtk_eth *eth, void *v)
+{
+	int i;
+
+	seq_puts(m, "HW LRO statistic dump:\n");
+
+	/* Agg number count */
+	seq_puts(m, "Cnt:   RING4 | RING5 | RING6 | RING7 Total\n");
+	for (i = 0; i <= MTK_HW_LRO_MAX_AGG_CNT; i++) {
+		seq_printf(m,
+			   " %02d :      %d        %d        %d        %d        %d\n", i,
+			   eth->debugfs->hwlro_stats.agg_num_cnt[0][i],
+			   eth->debugfs->hwlro_stats.agg_num_cnt[1][i],
+			   eth->debugfs->hwlro_stats.agg_num_cnt[2][i],
+			   eth->debugfs->hwlro_stats.agg_num_cnt[3][i],
+			   (eth->debugfs->hwlro_stats.agg_num_cnt[0][i] +
+			    eth->debugfs->hwlro_stats.agg_num_cnt[1][i] +
+			    eth->debugfs->hwlro_stats.agg_num_cnt[2][i] +
+			    eth->debugfs->hwlro_stats.agg_num_cnt[3][i]));
+	}
+
+	/* Total agg count */
+	seq_puts(m, "Total agg:   RING4 | RING5 | RING6 | RING7 Total\n");
+	seq_printf(m, "                %d      %d      %d      %d      %d\n",
+		   eth->debugfs->hwlro_stats.tot_agg_cnt[0],
+		   eth->debugfs->hwlro_stats.tot_agg_cnt[1],
+		   eth->debugfs->hwlro_stats.tot_agg_cnt[2],
+		   eth->debugfs->hwlro_stats.tot_agg_cnt[3],
+		   (eth->debugfs->hwlro_stats.tot_agg_cnt[0] +
+		    eth->debugfs->hwlro_stats.tot_agg_cnt[1] +
+		    eth->debugfs->hwlro_stats.tot_agg_cnt[2] +
+		    eth->debugfs->hwlro_stats.tot_agg_cnt[3]));
+
+	/* Total flush count */
+	seq_puts(m, "Total flush:   RING4 | RING5 | RING6 | RING7 Total\n");
+	seq_printf(m, "                %d      %d      %d      %d      %d\n",
+		   eth->debugfs->hwlro_stats.tot_flush_cnt[0],
+		   eth->debugfs->hwlro_stats.tot_flush_cnt[1],
+		   eth->debugfs->hwlro_stats.tot_flush_cnt[2],
+		   eth->debugfs->hwlro_stats.tot_flush_cnt[3],
+		   (eth->debugfs->hwlro_stats.tot_flush_cnt[0] +
+		    eth->debugfs->hwlro_stats.tot_flush_cnt[1] +
+		    eth->debugfs->hwlro_stats.tot_flush_cnt[2] +
+		    eth->debugfs->hwlro_stats.tot_flush_cnt[3]));
+
+	/* Avg agg count */
+	seq_puts(m, "Avg agg:   RING4 | RING5 | RING6 | RING7 Total\n");
+	seq_printf(m, "                %d      %d      %d      %d      %d\n",
+		   (eth->debugfs->hwlro_stats.tot_flush_cnt[0]) ?
+		    (eth->debugfs->hwlro_stats.tot_agg_cnt[0] /
+		     eth->debugfs->hwlro_stats.tot_flush_cnt[0]) : 0,
+		   (eth->debugfs->hwlro_stats.tot_flush_cnt[1]) ?
+		    (eth->debugfs->hwlro_stats.tot_agg_cnt[1] /
+		     eth->debugfs->hwlro_stats.tot_flush_cnt[1]) : 0,
+		   (eth->debugfs->hwlro_stats.tot_flush_cnt[2]) ?
+		    (eth->debugfs->hwlro_stats.tot_agg_cnt[2] /
+		     eth->debugfs->hwlro_stats.tot_flush_cnt[2]) : 0,
+		   (eth->debugfs->hwlro_stats.tot_flush_cnt[3]) ?
+		    (eth->debugfs->hwlro_stats.tot_agg_cnt[3] /
+		     eth->debugfs->hwlro_stats.tot_flush_cnt[3]) : 0,
+		   (eth->debugfs->hwlro_stats.tot_flush_cnt[0] +
+		    eth->debugfs->hwlro_stats.tot_flush_cnt[1] +
+		    eth->debugfs->hwlro_stats.tot_flush_cnt[2] +
+		    eth->debugfs->hwlro_stats.tot_flush_cnt[3]) ?
+		    ((eth->debugfs->hwlro_stats.tot_agg_cnt[0] +
+		      eth->debugfs->hwlro_stats.tot_agg_cnt[1] +
+		      eth->debugfs->hwlro_stats.tot_agg_cnt[2] +
+		      eth->debugfs->hwlro_stats.tot_agg_cnt[3]) /
+		     (eth->debugfs->hwlro_stats.tot_flush_cnt[0] +
+		      eth->debugfs->hwlro_stats.tot_flush_cnt[1] +
+		      eth->debugfs->hwlro_stats.tot_flush_cnt[2] +
+		      eth->debugfs->hwlro_stats.tot_flush_cnt[3])) : 0);
+
+	/*  Statistics of aggregation size counts */
+	seq_puts(m, "HW LRO flush pkt len:\n");
+	seq_puts(m, " Length  | RING4  | RING5  | RING6  | RING7 Total\n");
+	for (i = 0; i < 15; i++) {
+		seq_printf(m, "%05d~%05d: %d      %d      %d      %d      %d\n",
+			   i * 5000, (i + 1) * 5000,
+			   eth->debugfs->hwlro_stats.agg_size_cnt[0][i],
+			   eth->debugfs->hwlro_stats.agg_size_cnt[1][i],
+			   eth->debugfs->hwlro_stats.agg_size_cnt[2][i],
+			   eth->debugfs->hwlro_stats.agg_size_cnt[3][i],
+			   (eth->debugfs->hwlro_stats.agg_size_cnt[0][i] +
+			    eth->debugfs->hwlro_stats.agg_size_cnt[1][i] +
+			    eth->debugfs->hwlro_stats.agg_size_cnt[2][i] +
+			    eth->debugfs->hwlro_stats.agg_size_cnt[3][i]));
+	}
+
+	seq_puts(m, "Flush reason:   RING4 | RING5 | RING6 | RING7 Total\n");
+	seq_printf(m, "AGG timeout:      %d      %d      %d      %d      %d\n",
+		   eth->debugfs->hwlro_stats.agg_flush_cnt[0],
+		   eth->debugfs->hwlro_stats.agg_flush_cnt[1],
+		   eth->debugfs->hwlro_stats.agg_flush_cnt[2],
+		   eth->debugfs->hwlro_stats.agg_flush_cnt[3],
+		   (eth->debugfs->hwlro_stats.agg_flush_cnt[0] +
+		    eth->debugfs->hwlro_stats.agg_flush_cnt[1] +
+		    eth->debugfs->hwlro_stats.agg_flush_cnt[2] +
+		    eth->debugfs->hwlro_stats.agg_flush_cnt[3]));
+
+	seq_printf(m, "AGE timeout:      %d      %d      %d      %d      %d\n",
+		   eth->debugfs->hwlro_stats.age_flush_cnt[0],
+		   eth->debugfs->hwlro_stats.age_flush_cnt[1],
+		   eth->debugfs->hwlro_stats.age_flush_cnt[2],
+		   eth->debugfs->hwlro_stats.age_flush_cnt[3],
+		   (eth->debugfs->hwlro_stats.age_flush_cnt[0] +
+		    eth->debugfs->hwlro_stats.age_flush_cnt[1] +
+		    eth->debugfs->hwlro_stats.age_flush_cnt[2] +
+		    eth->debugfs->hwlro_stats.age_flush_cnt[3]));
+
+	seq_printf(m, "Not in-sequence:  %d      %d      %d      %d      %d\n",
+		   eth->debugfs->hwlro_stats.seq_flush_cnt[0],
+		   eth->debugfs->hwlro_stats.seq_flush_cnt[1],
+		   eth->debugfs->hwlro_stats.seq_flush_cnt[2],
+		   eth->debugfs->hwlro_stats.seq_flush_cnt[3],
+		   (eth->debugfs->hwlro_stats.seq_flush_cnt[0] +
+		    eth->debugfs->hwlro_stats.seq_flush_cnt[1] +
+		    eth->debugfs->hwlro_stats.seq_flush_cnt[2] +
+		    eth->debugfs->hwlro_stats.seq_flush_cnt[3]));
+
+	seq_printf(m, "Timestamp:        %d      %d      %d      %d      %d\n",
+		   eth->debugfs->hwlro_stats.timestamp_flush_cnt[0],
+		   eth->debugfs->hwlro_stats.timestamp_flush_cnt[1],
+		   eth->debugfs->hwlro_stats.timestamp_flush_cnt[2],
+		   eth->debugfs->hwlro_stats.timestamp_flush_cnt[3],
+		   (eth->debugfs->hwlro_stats.timestamp_flush_cnt[0] +
+		    eth->debugfs->hwlro_stats.timestamp_flush_cnt[1] +
+		    eth->debugfs->hwlro_stats.timestamp_flush_cnt[2] +
+		    eth->debugfs->hwlro_stats.timestamp_flush_cnt[3]));
+
+	seq_printf(m, "No LRO rule:      %d      %d      %d      %d      %d\n",
+		   eth->debugfs->hwlro_stats.norule_flush_cnt[0],
+		   eth->debugfs->hwlro_stats.norule_flush_cnt[1],
+		   eth->debugfs->hwlro_stats.norule_flush_cnt[2],
+		   eth->debugfs->hwlro_stats.norule_flush_cnt[3],
+		   (eth->debugfs->hwlro_stats.norule_flush_cnt[0] +
+		    eth->debugfs->hwlro_stats.norule_flush_cnt[1] +
+		    eth->debugfs->hwlro_stats.norule_flush_cnt[2] +
+		    eth->debugfs->hwlro_stats.norule_flush_cnt[3]));
+
+	return 0;
+}
+
+static int mtk_hwlro_stats_show(struct seq_file *m, void *v)
+{
+	struct mtk_eth *eth = m->private;
+
+	if (mtk_is_netsys_v3_or_greater(eth))
+		mtk_hwlro_stats_show_v2(m, eth, v);
+	else
+		mtk_hwlro_stats_show_v1(m, eth, v);
+
+	return 0;
+}
+
+static ssize_t mtk_hwlro_stats_write(struct file *file, const char __user *buffer,
+				     size_t count, loff_t *data)
+{
+	struct seq_file *m = (struct seq_file *)file->private_data;
+	struct mtk_eth *eth = m->private;
+	struct mtk_hwlro_stats *hwlro_stats = &eth->debugfs->hwlro_stats;
+
+	memset(hwlro_stats->agg_num_cnt, 0, sizeof(hwlro_stats->agg_num_cnt));
+	memset(hwlro_stats->agg_size_cnt, 0, sizeof(hwlro_stats->agg_size_cnt));
+	memset(hwlro_stats->tot_agg_cnt, 0, sizeof(hwlro_stats->tot_agg_cnt));
+	memset(hwlro_stats->tot_flush_cnt, 0, sizeof(hwlro_stats->tot_flush_cnt));
+	memset(hwlro_stats->agg_flush_cnt, 0, sizeof(hwlro_stats->agg_flush_cnt));
+	memset(hwlro_stats->age_flush_cnt, 0, sizeof(hwlro_stats->age_flush_cnt));
+	memset(hwlro_stats->seq_flush_cnt, 0, sizeof(hwlro_stats->seq_flush_cnt));
+	memset(hwlro_stats->timestamp_flush_cnt, 0,
+	       sizeof(hwlro_stats->timestamp_flush_cnt));
+	memset(hwlro_stats->norule_flush_cnt, 0, sizeof(hwlro_stats->norule_flush_cnt));
+
+	pr_info("clear hw lro cnt table\n");
+
+	return count;
+}
+
+static int mtk_hwlro_stats_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mtk_hwlro_stats_show, inode->i_private);
+}
+
+static const struct file_operations mtk_eth_debugfs_hwlro_stats_fops = {
+	.owner = THIS_MODULE,
+	.open = mtk_hwlro_stats_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.write = mtk_hwlro_stats_write,
+	.release = single_release
+};
+
+static int mtk_hwlro_agg_cnt_ctrl(struct mtk_eth *eth, int cnt)
+{
+	const struct mtk_reg_map *reg_map = eth->soc->reg_map;
+	int i;
+
+	for (i = 1; i <= MTK_HW_LRO_RING_NUM; i++)
+		SET_PDMA_RXRING_MAX_AGG_CNT(eth, i, cnt);
+
+	return 0;
+}
+
+static int mtk_hwlro_agg_time_ctrl(struct mtk_eth *eth, int time)
+{
+	const struct mtk_reg_map *reg_map = eth->soc->reg_map;
+	int i;
+
+	for (i = 1; i <= MTK_HW_LRO_RING_NUM; i++)
+		SET_PDMA_RXRING_AGG_TIME(eth, i, time);
+
+	return 0;
+}
+
+static int mtk_hwlro_age_time_ctrl(struct mtk_eth *eth, int time)
+{
+	const struct mtk_reg_map *reg_map = eth->soc->reg_map;
+	int i;
+
+	for (i = 1; i <= MTK_HW_LRO_RING_NUM; i++)
+		SET_PDMA_RXRING_AGE_TIME(eth, i, time);
+	return 0;
+}
+
+static int mtk_hwlro_threshold_ctrl(struct mtk_eth *eth, int bandwidth)
+{
+	const struct mtk_reg_map *reg_map = eth->soc->reg_map;
+
+	SET_PDMA_LRO_BW_THRESHOLD(eth, bandwidth);
+
+	return 0;
+}
+
+static int mtk_hwlro_ring_enable_ctrl(struct mtk_eth *eth, int enable)
+{
+	const struct mtk_reg_map *reg_map = eth->soc->reg_map;
+	int i;
+
+	pr_info("[%s] %s HW LRO rings\n", __func__, (enable) ? "Enable" : "Disable");
+
+	for (i = 1; i <= MTK_HW_LRO_RING_NUM; i++)
+		SET_PDMA_RXRING_VALID(eth, i, enable);
+
+	return 0;
+}
+
+static int mtk_hwlro_stats_enable_ctrl(struct mtk_eth *eth, int enable)
+{
+	pr_info("[%s] %s HW LRO statistics\n", __func__, (enable) ? "Enable" : "Disable");
+	eth->debugfs->hwlro_stats.ebl = enable;
+
+	return 0;
+}
+
+static const mtk_hwlro_dbg_func hwlro_dbg_func[] = {
+	[0] = mtk_hwlro_agg_cnt_ctrl,
+	[1] = mtk_hwlro_agg_time_ctrl,
+	[2] = mtk_hwlro_age_time_ctrl,
+	[3] = mtk_hwlro_threshold_ctrl,
+	[4] = mtk_hwlro_ring_enable_ctrl,
+	[5] = mtk_hwlro_stats_enable_ctrl,
+};
+
+static ssize_t mtk_hwlro_auto_tlb_write(struct file *file, const char __user *buffer,
+					size_t count, loff_t *data)
+{
+	struct seq_file *m = (struct seq_file *)file->private_data;
+	struct mtk_eth *eth = m->private;
+	char buf[32];
+	char *p_buf;
+	char *p_token = NULL;
+	char *p_delimiter = " \t";
+	long x = 0, y = 0;
+	u32 len = count;
+	int ret;
+
+	if (len >= sizeof(buf)) {
+		pr_info("Input handling fail!\n");
+		return -1;
+	}
+
+	if (copy_from_user(buf, buffer, len))
+		return -EFAULT;
+
+	buf[len] = '\0';
+
+	p_buf = buf;
+	p_token = strsep(&p_buf, p_delimiter);
+	if (!p_token)
+		x = 0;
+	else
+		ret = kstrtol(p_token, 10, &x);
+
+	p_token = strsep(&p_buf, "\t\n ");
+	if (p_token)
+		ret = kstrtol(p_token, 10, &y);
+
+	if (hwlro_dbg_func[x] && (ARRAY_SIZE(hwlro_dbg_func) > x))
+		(*hwlro_dbg_func[x]) (eth, y);
+
+	return count;
+}
+
+static void mtk_hwlro_auto_tlb_dump_v1(struct seq_file *m, u32 index)
+{
+	struct mtk_eth *eth = m->private;
+	const struct mtk_reg_map *reg_map = eth->soc->reg_map;
+	struct mtk_hwlro_alt_v1 alt;
+	__be32 addr;
+	u32 tlb_info[9];
+	u32 dw_len, cnt, priority;
+	u32 entry;
+	int i;
+
+	if (index > 4)
+		index = index - 1;
+	entry = (index * 9) + 1;
+
+	/* read valid entries of the auto-learn table */
+	mtk_w32(eth, entry, MTK_FE_ALT_CF8);
+
+	for (i = 0; i < 9; i++)
+		tlb_info[i] = mtk_r32(eth, MTK_FE_ALT_SEQ_CFC);
+
+	memcpy(&alt, tlb_info, sizeof(struct mtk_hwlro_alt_v1));
+
+	dw_len = alt.alt_info7.dw_len;
+	cnt = alt.alt_info6.cnt;
+
+	if (mtk_r32(eth, MTK_PDMA_LRO_CTRL_DW0) & MTK_LRO_ALT_PKT_CNT_MODE)
+		priority = cnt;		/* packet count */
+	else
+		priority = dw_len;	/* byte count */
+
+	/* dump valid entries of the auto-learn table */
+	if (index >= 4)
+		seq_printf(m, "\n===== TABLE Entry: %d (Act) =====\n", index);
+	else
+		seq_printf(m, "\n===== TABLE Entry: %d (LRU) =====\n", index);
+
+	if (alt.alt_info8.ipv4) {
+		addr = htonl(alt.alt_info1.sip0);
+		seq_printf(m, "SIP = %pI4 (IPv4)\n", &addr);
+	} else {
+		seq_printf(m, "SIP = %08X:%08X:%08X:%08X (IPv6)\n",
+			   alt.alt_info4.sip3, alt.alt_info3.sip2,
+			   alt.alt_info2.sip1, alt.alt_info1.sip0);
+	}
+
+	seq_printf(m, "DIP_ID = %d\n", alt.alt_info8.dip_id);
+	seq_printf(m, "TCP SPORT = %d | TCP DPORT = %d\n",
+		   alt.alt_info0.stp, alt.alt_info0.dtp);
+	seq_printf(m, "VLAN_VID_VLD = %d\n", alt.alt_info6.vlan_vid_vld);
+	seq_printf(m, "VLAN1 = %d | VLAN2 = %d | VLAN3 = %d | VLAN4 =%d\n",
+		   (alt.alt_info5.vlan_vid0 & 0xfff),
+		   ((alt.alt_info5.vlan_vid0 >> 12) & 0xfff),
+		   ((alt.alt_info6.vlan_vid1 << 8) |
+		   ((alt.alt_info5.vlan_vid0 >> 24) & 0xfff)),
+		   ((alt.alt_info6.vlan_vid1 >> 4) & 0xfff));
+	seq_printf(m, "TPUT = %d | FREQ = %d\n", dw_len, cnt);
+	seq_printf(m, "PRIORITY = %d\n", priority);
+}
+
+static void mtk_hwlro_auto_tlb_dump_v2(struct seq_file *m, u32 index)
+{
+	struct mtk_eth *eth = m->private;
+	const struct mtk_reg_map *reg_map = eth->soc->reg_map;
+	struct mtk_hwlro_alt_v2 alt;
+	u32 score = 0, ipv4 = 0;
+	u32 ipv6[4] = { 0 };
+	u32 tlb_info[12];
+	int i;
+
+	/* read valid entries of the auto-learn table */
+	mtk_w32(eth, index << MTK_LRO_ALT_INDEX_OFFSET, reg_map->pdma.lro_alt_dbg);
+
+	for (i = 0; i < 11; i++)
+		tlb_info[i] = mtk_r32(eth, reg_map->pdma.lro_alt_dbg_data);
+
+	memcpy(&alt, tlb_info, sizeof(struct mtk_hwlro_alt_v2));
+
+	if (mtk_r32(eth, MTK_PDMA_LRO_CTRL_DW0) & MTK_LRO_ALT_PKT_CNT_MODE)
+		score = 1;	/* packet count */
+	else
+		score = 0;	/* byte count */
+
+	/* dump valid entries of the auto-learn table */
+	if (alt.alt_info0.valid) {
+		if (index < 5)
+			seq_printf(m,
+				   "\n===== TABLE Entry: %d (onging) =====\n",
+				   index);
+		else
+			seq_printf(m,
+				   "\n===== TABLE Entry: %d (candidate) =====\n",
+				   index);
+
+		if (alt.alt_info1.v4_valid) {
+			ipv4 = (alt.alt_info4.sip0_h << 23) |
+				alt.alt_info5.sip0_l;
+			seq_printf(m, "SIP = 0x%x: (IPv4)\n", ipv4);
+
+			ipv4 = (alt.alt_info8.dip0_h << 23) |
+				alt.alt_info9.dip0_l;
+			seq_printf(m, "DIP = 0x%x: (IPv4)\n", ipv4);
+		} else if (alt.alt_info1.v6_valid) {
+			ipv6[3] = (alt.alt_info1.sip3_h << 23) |
+				   (alt.alt_info2.sip3_l << 9);
+			ipv6[2] = (alt.alt_info2.sip2_h << 23) |
+				   (alt.alt_info3.sip2_l << 9);
+			ipv6[1] = (alt.alt_info3.sip1_h << 23) |
+				   (alt.alt_info4.sip1_l << 9);
+			ipv6[0] = (alt.alt_info4.sip0_h << 23) |
+				   (alt.alt_info5.sip0_l << 9);
+			seq_printf(m, "SIP = 0x%x:0x%x:0x%x:0x%x (IPv6)\n",
+				   ipv6[3], ipv6[2], ipv6[1], ipv6[0]);
+
+			ipv6[3] = (alt.alt_info5.dip3_h << 23) |
+				   (alt.alt_info6.dip3_l << 9);
+			ipv6[2] = (alt.alt_info6.dip2_h << 23) |
+				   (alt.alt_info7.dip2_l << 9);
+			ipv6[1] = (alt.alt_info7.dip1_h << 23) |
+				   (alt.alt_info8.dip1_l << 9);
+			ipv6[0] = (alt.alt_info8.dip0_h << 23) |
+				   (alt.alt_info9.dip0_l << 9);
+			seq_printf(m, "DIP = 0x%x:0x%x:0x%x:0x%x (IPv6)\n",
+				   ipv6[3], ipv6[2], ipv6[1], ipv6[0]);
+		}
+
+		seq_printf(m, "TCP SPORT = %d | TCP DPORT = %d\n",
+			   (alt.alt_info9.sp_h << 7) | (alt.alt_info10.sp_l),
+			   alt.alt_info10.dp);
+	}
+}
+
+static void mtk_hwlro_auto_tlb_dump_v3(struct seq_file *m, u32 index)
+{
+	struct mtk_eth *eth = m->private;
+	u32 val, sport, dport, vld, dip_idx, mode;
+	u32 ipv6[4] = { 0 };
+	u32 ipv4 = 0;
+	int ret;
+
+	/* dump the LRO_DATA table for the specific entry */
+	val = FIELD_PREP(MTK_GLO_MEM_IDX, MTK_LRO_MEM_IDX);
+	val |= FIELD_PREP(MTK_GLO_MEM_ADDR, MTK_LRO_MEM_DATA_BASE + index);
+	val |= FIELD_PREP(MTK_GLO_MEM_CMD, MTK_GLO_MEM_READ);
+	mtk_w32(eth, val, MTK_GLO_MEM_CTRL);
+	/* check if the GLO_MEM access is successful */
+	ret = FIELD_GET(MTK_GLO_MEM_CMD, mtk_r32(eth, MTK_GLO_MEM_CTRL));
+	if (ret != 0) {
+		pr_warn("GLO_MEM read/write error\n");
+		return;
+	}
+
+	/* dump the valid entries of the auto-learn table */
+	vld = FIELD_GET(MTK_LRO_DATA_VLD, mtk_r32(eth, MTK_GLO_MEM_DATA(6)));
+	if (vld) {
+		if (index < 5)
+			seq_printf(m,
+				   "\n===== TABLE Entry: %d (onging) =====\n",
+				   index);
+		else
+			seq_printf(m,
+				   "\n===== TABLE Entry: %d (candidate) =====\n",
+				   index);
+
+		/* determine the DIP index for the specific entry */
+		dip_idx = FIELD_GET(MTK_LRO_DATA_DIP_IDX, mtk_r32(eth, MTK_GLO_MEM_DATA(5)));
+		/* switch to the LRO_DIP table for the specific DIP index */
+		val = FIELD_PREP(MTK_GLO_MEM_IDX, MTK_LRO_MEM_IDX);
+		val |= FIELD_PREP(MTK_GLO_MEM_ADDR, MTK_LRO_MEM_DIP_BASE + dip_idx);
+		val |= FIELD_PREP(MTK_GLO_MEM_CMD, MTK_GLO_MEM_READ);
+		mtk_w32(eth, val, MTK_GLO_MEM_CTRL);
+		/* determine the mode for the specific DIP index */
+		mode = FIELD_GET(MTK_LRO_DIP_MODE, mtk_r32(eth, MTK_GLO_MEM_DATA(4)));
+		/* dump the SIP and DIP */
+		if (mode == MTK_LRO_DIP_IPV4) {
+			val = FIELD_PREP(MTK_GLO_MEM_IDX, MTK_LRO_MEM_IDX);
+			val |= FIELD_PREP(MTK_GLO_MEM_ADDR, MTK_LRO_MEM_DATA_BASE + index);
+			val |= FIELD_PREP(MTK_GLO_MEM_CMD, MTK_GLO_MEM_READ);
+			mtk_w32(eth, val, MTK_GLO_MEM_CTRL);
+			ipv4 = mtk_r32(eth, MTK_GLO_MEM_DATA(1));
+			seq_printf(m, "SIP = 0x%x: (IPv4)\n", ipv4);
+			val = FIELD_PREP(MTK_GLO_MEM_IDX, MTK_LRO_MEM_IDX);
+			val |= FIELD_PREP(MTK_GLO_MEM_ADDR, MTK_LRO_MEM_DIP_BASE + dip_idx);
+			val |= FIELD_PREP(MTK_GLO_MEM_CMD, MTK_GLO_MEM_READ);
+			mtk_w32(eth, val, MTK_GLO_MEM_CTRL);
+			ipv4 = mtk_r32(eth, MTK_GLO_MEM_DATA(0));
+			seq_printf(m, "DIP = 0x%x: (IPv4)\n", ipv4);
+		} else if (mode == MTK_LRO_DIP_IPV6) {
+			val = FIELD_PREP(MTK_GLO_MEM_IDX, MTK_LRO_MEM_IDX);
+			val |= FIELD_PREP(MTK_GLO_MEM_ADDR, MTK_LRO_MEM_DATA_BASE + index);
+			val |= FIELD_PREP(MTK_GLO_MEM_CMD, MTK_GLO_MEM_READ);
+			mtk_w32(eth, val, MTK_GLO_MEM_CTRL);
+			ipv6[3] = mtk_r32(eth, MTK_GLO_MEM_DATA(4));
+			ipv6[2] = mtk_r32(eth, MTK_GLO_MEM_DATA(3));
+			ipv6[1] = mtk_r32(eth, MTK_GLO_MEM_DATA(2));
+			ipv6[0] = mtk_r32(eth, MTK_GLO_MEM_DATA(1));
+			seq_printf(m, "SIP = 0x%x:0x%x:0x%x:0x%x (IPv6)\n",
+				   ipv6[3], ipv6[2], ipv6[1], ipv6[0]);
+			val = FIELD_PREP(MTK_GLO_MEM_IDX, MTK_LRO_MEM_IDX);
+			val |= FIELD_PREP(MTK_GLO_MEM_ADDR, MTK_LRO_MEM_DIP_BASE + dip_idx);
+			val |= FIELD_PREP(MTK_GLO_MEM_CMD, MTK_GLO_MEM_READ);
+			mtk_w32(eth, val, MTK_GLO_MEM_CTRL);
+			ipv6[3] = mtk_r32(eth, MTK_GLO_MEM_DATA(3));
+			ipv6[2] = mtk_r32(eth, MTK_GLO_MEM_DATA(2));
+			ipv6[1] = mtk_r32(eth, MTK_GLO_MEM_DATA(1));
+			ipv6[0] = mtk_r32(eth, MTK_GLO_MEM_DATA(0));
+			seq_printf(m, "DIP = 0x%x:0x%x:0x%x:0x%x (IPv6)\n",
+				   ipv6[3], ipv6[2], ipv6[1], ipv6[0]);
+		}
+
+		/* dump the SPORT and DPORT */
+		val = FIELD_PREP(MTK_GLO_MEM_IDX, MTK_LRO_MEM_IDX);
+		val |= FIELD_PREP(MTK_GLO_MEM_ADDR, MTK_LRO_MEM_DATA_BASE + index);
+		val |= FIELD_PREP(MTK_GLO_MEM_CMD, MTK_GLO_MEM_READ);
+		mtk_w32(eth, val, MTK_GLO_MEM_CTRL);
+		sport = FIELD_GET(MTK_LRO_DATA_SPORT, mtk_r32(eth, MTK_GLO_MEM_DATA(0)));
+		dport = FIELD_GET(MTK_LRO_DATA_DPORT, mtk_r32(eth, MTK_GLO_MEM_DATA(0)));
+		seq_printf(m, "TCP SPORT = %d | TCP DPORT = %d\n", sport, dport);
+	}
+}
+
+static int mtk_hwlro_auto_tlb_show(struct seq_file *m, void *v)
+{
+	struct mtk_eth *eth = m->private;
+	const struct mtk_reg_map *reg_map = eth->soc->reg_map;
+	int i;
+	u32 reg_val;
+	u32 reg_op1, reg_op2, reg_op3, reg_op4;
+	u32 agg_cnt, agg_time, age_time;
+
+	seq_puts(m, "Usage of /proc/mtketh/hwlro_auto_tlb:\n");
+	seq_puts(m, "echo [function] [setting] > /proc/mtketh/hwlro_auto_tlb\n");
+	seq_puts(m, "Functions:\n");
+	seq_puts(m, "[0] = hwlro_agg_cnt_ctrl\n");
+	seq_puts(m, "[1] = hwlro_agg_time_ctrl\n");
+	seq_puts(m, "[2] = hwlro_age_time_ctrl\n");
+	seq_puts(m, "[3] = hwlro_threshold_ctrl\n");
+	seq_puts(m, "[4] = hwlro_ring_enable_ctrl\n");
+	seq_puts(m, "[5] = hwlro_stats_enable_ctrl\n\n");
+
+	if (mtk_is_netsys_v3_or_greater(eth)) {
+		for (i = 1; i <= 8; i++) {
+			if (MTK_HAS_CAPS(eth->soc->caps, MTK_GLO_MEM_ACCESS))
+				mtk_hwlro_auto_tlb_dump_v3(m, i);
+			else
+				mtk_hwlro_auto_tlb_dump_v2(m, i);
+		}
+	} else {
+		/* Read valid entries of the auto-learn table */
+		mtk_w32(eth, 0, MTK_FE_ALT_CF8);
+		reg_val = mtk_r32(eth, MTK_FE_ALT_SEQ_CFC);
+
+		seq_printf(m,
+			   "HW LRO Auto-learn Table: (MTK_FE_ALT_SEQ_CFC=0x%x)\n",
+			   reg_val);
+
+		for (i = 7; i >= 0; i--) {
+			if (reg_val & (1 << i))
+				mtk_hwlro_auto_tlb_dump_v1(m, i);
+		}
+	}
+
+	/* Read the agg_time/age_time/agg_cnt of LRO rings */
+	seq_puts(m, "\nHW LRO Ring Settings\n");
+
+	for (i = 1; i <= MTK_HW_LRO_RING_NUM; i++) {
+		if (MTK_HAS_CAPS(eth->soc->caps, MTK_GLO_MEM_ACCESS)) {
+			reg_val = FIELD_PREP(MTK_GLO_MEM_IDX, MTK_LRO_MEM_IDX);
+			reg_val |= FIELD_PREP(MTK_GLO_MEM_ADDR, MTK_LRO_MEM_CFG_BASE + i);
+			reg_val |= FIELD_PREP(MTK_GLO_MEM_CMD, MTK_GLO_MEM_READ);
+			mtk_w32(eth, reg_val, MTK_GLO_MEM_CTRL);
+			agg_cnt = FIELD_GET(MTK_RING_MAX_AGG_CNT,
+					    mtk_r32(eth, MTK_GLO_MEM_DATA(1)));
+			agg_time = FIELD_GET(MTK_RING_MAX_AGG_TIME_V2,
+					     mtk_r32(eth, MTK_GLO_MEM_DATA(0)));
+			age_time = FIELD_GET(MTK_RING_AGE_TIME,
+					     mtk_r32(eth, MTK_GLO_MEM_DATA(0)));
+		} else {
+			reg_op1 = mtk_r32(eth, MTK_LRO_CTRL_DW1_CFG(i));
+			reg_op2 = mtk_r32(eth, MTK_LRO_CTRL_DW2_CFG(i));
+			reg_op3 = mtk_r32(eth, MTK_LRO_CTRL_DW3_CFG(i));
+
+			agg_cnt = ((reg_op3 & 0x3) << 6) |
+				   ((reg_op2 >> MTK_LRO_RING_AGG_CNT_L_OFFSET) & 0x3f);
+			agg_time = (reg_op2 >> MTK_LRO_RING_AGG_TIME_OFFSET) & 0xffff;
+			age_time = ((reg_op2 & 0x3f) << 10) |
+				    ((reg_op1 >> MTK_LRO_RING_AGE_TIME_L_OFFSET) & 0x3ff);
+		}
+		reg_op4 = mtk_r32(eth, MTK_PDMA_LRO_CTRL_DW2);
+
+		seq_printf(m,
+			   "Ring[%d]: MAX_AGG_CNT=%d, AGG_TIME=%d, AGE_TIME=%d, Threshold=%d\n",
+			   MTK_HW_LRO_RING(i - 1), agg_cnt, agg_time, age_time, reg_op4);
+	}
+
+	seq_puts(m, "\n");
+
+	return 0;
+}
+
+static int mtk_hwlro_auto_tlb_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mtk_hwlro_auto_tlb_show, inode->i_private);
+}
+
+static const struct file_operations mtk_eth_debugfs_hwlro_auto_tlb_fops = {
+	.owner = THIS_MODULE,
+	.open = mtk_hwlro_auto_tlb_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.write = mtk_hwlro_auto_tlb_write,
+	.release = single_release
+};
+
+static int mtk_eth_procfs_init(struct mtk_eth *eth)
+{
+	static const struct proc_ops mtk_eth_procfs_dbg_regs_fops = {
+		.proc_open = mtk_dbg_regs_open,
+		.proc_read = seq_read,
+		.proc_lseek = seq_lseek,
+		.proc_release = single_release
+	};
+	static const struct proc_ops mtk_eth_procfs_tx_full_cnt_fops = {
+		.proc_open = mtk_tx_full_cnt_open,
+		.proc_read = seq_read,
+		.proc_lseek = seq_lseek,
+		.proc_release = single_release,
+	};
+	static const struct proc_ops mtk_eth_procfs_tx_ring_fops = {
+		.proc_open = mtk_tx_ring_open,
+		.proc_read = seq_read,
+		.proc_lseek = seq_lseek,
+		.proc_release = single_release
+	};
+	static const struct proc_ops mtk_eth_procfs_hwtx_ring_fops = {
+		.proc_open = mtk_hwtx_ring_open,
+		.proc_read = seq_read,
+		.proc_lseek = seq_lseek,
+		.proc_release = single_release
+	};
+	static const struct proc_ops mtk_eth_procfs_rx_ring_fops = {
+		.proc_open = mtk_rx_ring_open,
+		.proc_read = seq_read,
+		.proc_lseek = seq_lseek,
+		.proc_release = single_release
+	};
+	static const struct proc_ops mtk_eth_procfs_esw_cnt_fops = {
+		.proc_open = mtk_esw_cnt_open,
+		.proc_read = seq_read,
+		.proc_lseek = seq_lseek,
+		.proc_release = single_release
+	};
+	static const struct proc_ops mtk_eth_procfs_mac_cnt_fops = {
+		.proc_open = mtk_mac_cnt_open,
+		.proc_read = seq_read,
+		.proc_lseek = seq_lseek,
+		.proc_release = single_release
+	};
+	static const struct proc_ops mtk_eth_procfs_xfi_cnt_fops = {
+		.proc_open = mtk_xfi_cnt_open,
+		.proc_read = seq_read,
+		.proc_lseek = seq_lseek,
+		.proc_release = single_release
+	};
+	static const struct proc_ops mtk_eth_procfs_hwlro_stats_fops = {
+		.proc_open = mtk_hwlro_stats_open,
+		.proc_read = seq_read,
+		.proc_lseek = seq_lseek,
+		.proc_write = mtk_hwlro_stats_write,
+		.proc_release = single_release
+	};
+	static const struct proc_ops mtk_eth_procfs_hwlro_auto_tlb_fops = {
+		.proc_open = mtk_hwlro_auto_tlb_open,
+		.proc_read = seq_read,
+		.proc_lseek = seq_lseek,
+		.proc_write = mtk_hwlro_auto_tlb_write,
+		.proc_release = single_release
+	};
+
+	eth->debugfs->procfs_root = proc_mkdir(PROCREG_DIR, NULL);
+	if (!eth->debugfs->procfs_root) {
+		dev_err(eth->dev, "%s: failed to create directory\n", __func__);
+		return -ENOMEM;
+	}
+
+	proc_create_data(PROCREG_DBG_REGS, 0444, eth->debugfs->procfs_root,
+			 &mtk_eth_procfs_dbg_regs_fops, (void *)eth);
+	proc_create_data(PROCREG_TXFULL_CNT, 0444, eth->debugfs->procfs_root,
+			 &mtk_eth_procfs_tx_full_cnt_fops, (void *)eth);
+	proc_create_data(PROCREG_TXRING, 0444, eth->debugfs->procfs_root,
+			 &mtk_eth_procfs_tx_ring_fops, (void *)eth);
+	proc_create_data(PROCREG_HWTXRING, 0444, eth->debugfs->procfs_root,
+			 &mtk_eth_procfs_hwtx_ring_fops, (void *)eth);
+	proc_create_data(PROCREG_RXRING, 0444, eth->debugfs->procfs_root,
+			 &mtk_eth_procfs_rx_ring_fops, (void *)eth);
+	proc_create_data(PROCREG_ESW_CNT, 0444, eth->debugfs->procfs_root,
+			 &mtk_eth_procfs_esw_cnt_fops, (void *)eth);
+	if (mtk_is_netsys_v3_or_greater(eth)) {
+		proc_create_data(PROCREG_MAC_CNT, 0444, eth->debugfs->procfs_root,
+				 &mtk_eth_procfs_mac_cnt_fops, (void *)eth);
+		proc_create_data(PROCREG_XFI_CNT, 0444, eth->debugfs->procfs_root,
+				 &mtk_eth_procfs_xfi_cnt_fops, (void *)eth);
+	}
+	if (eth->hwlro) {
+		proc_create_data(PROCREG_HW_LRO_STATS, 0444, eth->debugfs->procfs_root,
+				 &mtk_eth_procfs_hwlro_stats_fops, (void *)eth);
+		proc_create_data(PROCREG_HW_LRO_AUTO_TLB, 0444, eth->debugfs->procfs_root,
+				 &mtk_eth_procfs_hwlro_auto_tlb_fops, (void *)eth);
+	}
+
+	return 0;
+}
+
+int mtk_eth_debugfs_init(struct mtk_eth *eth)
+{
+	char name[16];
+	int i, ret = 0;
+
+	eth->debugfs = devm_kzalloc(eth->dev, sizeof(*eth->debugfs), GFP_KERNEL);
+	if (!eth->debugfs) {
+		dev_err(eth->dev, "%s: failed to alloc debugfs structure\n", __func__);
+		return -ENOMEM;
+	}
+
+	eth->debugfs->root = debugfs_create_dir("mtketh", NULL);
+	if (!eth->debugfs->root) {
+		dev_err(eth->dev, "%s: failed to create debugfs directory\n", __func__);
+		return -ENOMEM;
+	}
+
+	eth->debugfs->mt7530.mmio_base = eth->esw_base;
+	eth->debugfs->mt7530.exist = mt7530_sw_detect(eth);
+
+	debugfs_create_file("phy_regs", 0444, eth->debugfs->root,
+			    eth, &mtk_eth_debugfs_c22_phyregs_fops);
+	debugfs_create_file("reset", 0444, eth->debugfs->root,
+			    eth, &mtk_eth_debugfs_reset_fops);
+	if (mt7530_is_exist(eth)) {
+		debugfs_create_file("mt7530sw_regs", 0444,
+				    eth->debugfs->root, eth,
+				    &mtk_eth_debugfs_mt7530sw_regs_fops);
+	}
+
+	if (mtk_is_netsys_v3_or_greater(eth))
+		debugfs_create_file("qdma_pppq", 0444, eth->debugfs->root,
+				    eth, &mtk_eth_debugfs_qdma_pppq_fops);
+
+	for (i = 0; i < (mtk_is_netsys_v2_or_greater(eth) ? 4 : 2); i++) {
+		ret = snprintf(name, sizeof(name), "qdma_sch%d", i);
+		if (ret != strlen(name)) {
+			ret = -ENOMEM;
+			goto err;
+		}
+		eth->debugfs->qdma_sched[i].eth = eth;
+		eth->debugfs->qdma_sched[i].id = i;
+		debugfs_create_file(name, 0444, eth->debugfs->root,
+				    (void *)&eth->debugfs->qdma_sched[i],
+				    &mtk_eth_debugfs_qdma_sched_fops);
+	}
+
+	for (i = 0; i < MTK_QDMA_NUM_QUEUES; i++) {
+		ret = snprintf(name, sizeof(name), "qdma_txq%d", i);
+		if (ret != strlen(name)) {
+			ret = -ENOMEM;
+			goto err;
+		}
+		eth->debugfs->qdma_queue[i].eth = eth;
+		eth->debugfs->qdma_queue[i].id = i;
+		debugfs_create_file(name, 0444, eth->debugfs->root,
+				    (void *)&eth->debugfs->qdma_queue[i],
+				    &mtk_eth_debugfs_qdma_queue_fops);
+	}
+
+	debugfs_create_file(PROCREG_DBG_REGS, 0444, eth->debugfs->root,
+			    eth, &mtk_eth_debugfs_dbg_regs_fops);
+	debugfs_create_file(PROCREG_TXFULL_CNT, 0444, eth->debugfs->root,
+			    eth, &mtk_eth_debugfs_tx_full_cnt_fops);
+	debugfs_create_file(PROCREG_TXRING, 0444, eth->debugfs->root,
+			    eth, &mtk_eth_debugfs_tx_ring_fops);
+	debugfs_create_file(PROCREG_HWTXRING, 0444, eth->debugfs->root,
+			    eth, &mtk_eth_debugfs_hwtx_ring_fops);
+	debugfs_create_file(PROCREG_RXRING, 0444, eth->debugfs->root,
+			    eth, &mtk_eth_debugfs_rx_ring_fops);
+	debugfs_create_file(PROCREG_ESW_CNT, 0444, eth->debugfs->root,
+			    eth, &mtk_eth_debugfs_esw_cnt_fops);
+	if (mtk_is_netsys_v3_or_greater(eth)) {
+		debugfs_create_file(PROCREG_MAC_CNT, 0444, eth->debugfs->root,
+				    eth, &mtk_eth_debugfs_mac_cnt_fops);
+		debugfs_create_file(PROCREG_XFI_CNT, 0444, eth->debugfs->root,
+				    eth, &mtk_eth_debugfs_xfi_cnt_fops);
+	}
+	if (eth->hwlro) {
+		memset(&eth->debugfs->hwlro_stats, 0, sizeof(struct mtk_hwlro_stats));
+		debugfs_create_file(PROCREG_HW_LRO_STATS, 0444, eth->debugfs->root,
+				    eth, &mtk_eth_debugfs_hwlro_stats_fops);
+		debugfs_create_file(PROCREG_HW_LRO_AUTO_TLB, 0444, eth->debugfs->root,
+				    eth, &mtk_eth_debugfs_hwlro_auto_tlb_fops);
+	}
+
+	if (mtk_eth_procfs_init(eth)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	return 0;
+
+err:
+	debugfs_remove_recursive(eth->debugfs->root);
+	return ret;
+}
+
+void mtk_eth_debugfs_exit(struct mtk_eth *eth)
+{
+	debugfs_remove_recursive(eth->debugfs->root);
+	remove_proc_subtree(PROCREG_DIR, eth->debugfs->procfs_root);
+}
diff --git a/drivers/net/ethernet/mediatek/mtk_eth_dbg.h b/drivers/net/ethernet/mediatek/mtk_eth_dbg.h
new file mode 100644
index 0000000..0eff3e6
--- /dev/null
+++ b/drivers/net/ethernet/mediatek/mtk_eth_dbg.h
@@ -0,0 +1,458 @@
+/*
+ *   Copyright (C) 2025 MediaTek Inc.
+ *
+ *   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; version 2 of the License
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   Copyright (C) 2009-2016 John Crispin <blogic@openwrt.org>
+ *   Copyright (C) 2009-2016 Felix Fietkau <nbd@openwrt.org>
+ *   Copyright (C) 2013-2016 Michael Lee <igvtee@gmail.com>
+ */
+
+#ifndef MTK_ETH_DBG_H
+#define MTK_ETH_DBG_H
+
+/* Debug Purpose Register */
+#define MTK_SGMII_FALSE_CARRIER_CNT(x)	(0x10060028 + ((x) * 0x10000))
+#define MTK_SGMII_EFUSE			0x11D008C8
+
+#define REG_ESW_MAX			0xFC
+#define MTKETH_ESW_REG_READ		0x89F1
+#define MTKETH_ESW_REG_WRITE		0x89F2
+#define MTKETH_MII_READ			0x89F3
+#define MTKETH_MII_WRITE		0x89F4
+#define MTKETH_MII_READ_CL45		0x89FC
+#define MTKETH_MII_WRITE_CL45		0x89FD
+
+#define PROCREG_ESW_CNT			"esw_cnt"
+#define PROCREG_MAC_CNT			"mac_cnt"
+#define PROCREG_XFI_CNT			"xfi_cnt"
+#define PROCREG_TXFULL_CNT		"tx_full_cnt"
+#define PROCREG_TXRING			"tx_ring"
+#define PROCREG_HWTXRING		"hwtx_ring"
+#define PROCREG_RXRING			"rx_ring"
+#define PROCREG_DIR			"mtketh"
+#define PROCREG_DBG_REGS		"dbg_regs"
+#define PROCREG_RSS_CTRL		"rss_ctrl"
+#define PROCREG_HW_LRO_STATS		"hw_lro_stats"
+#define PROCREG_HW_LRO_AUTO_TLB		"hw_lro_auto_tlb"
+#define PROCREG_RESET_EVENT		"reset_event"
+
+/* GMAC MIB Register */
+#define MTK_MAC_MIB_BASE(x)		(0x10400 + (x * 0x60))
+#define MTK_MAC_RX_UC_PKT_CNT_L		0x00
+#define MTK_MAC_RX_UC_PKT_CNT_H		0x04
+#define MTK_MAC_RX_UC_BYTE_CNT_L	0x08
+#define MTK_MAC_RX_UC_BYTE_CNT_H	0x0C
+#define MTK_MAC_RX_MC_PKT_CNT_L		0x10
+#define MTK_MAC_RX_MC_PKT_CNT_H		0x14
+#define MTK_MAC_RX_MC_BYTE_CNT_L	0x18
+#define MTK_MAC_RX_MC_BYTE_CNT_H	0x1C
+#define MTK_MAC_RX_BC_PKT_CNT_L		0x20
+#define MTK_MAC_RX_BC_PKT_CNT_H		0x24
+#define MTK_MAC_RX_BC_BYTE_CNT_L	0x28
+#define MTK_MAC_RX_BC_BYTE_CNT_H	0x2C
+#define MTK_MAC_TX_UC_PKT_CNT_L		0x30
+#define MTK_MAC_TX_UC_PKT_CNT_H		0x34
+#define MTK_MAC_TX_UC_BYTE_CNT_L	0x38
+#define MTK_MAC_TX_UC_BYTE_CNT_H	0x3C
+#define MTK_MAC_TX_MC_PKT_CNT_L		0x40
+#define MTK_MAC_TX_MC_PKT_CNT_H		0x44
+#define MTK_MAC_TX_MC_BYTE_CNT_L	0x48
+#define MTK_MAC_TX_MC_BYTE_CNT_H	0x4C
+#define MTK_MAC_TX_BC_PKT_CNT_L		0x50
+#define MTK_MAC_TX_BC_PKT_CNT_H		0x54
+#define MTK_MAC_TX_BC_BYTE_CNT_L	0x58
+#define MTK_MAC_TX_BC_BYTE_CNT_H	0x5C
+
+/* XFI MAC MIB Register */
+#define MTK_XFI_MIB_BASE(x)		(MTK_XMAC_MCR(x))
+#define MTK_XFI_CNT_CTRL		0x100
+#define MTK_XFI_TX_PKT_CNT		0x108
+#define MTK_XFI_TX_ETH_CNT		0x114
+#define MTK_XFI_TX_PAUSE_CNT		0x120
+#define MTK_XFI_TX_BYTE_CNT		0x134
+#define MTK_XFI_TX_UC_PKT_CNT_L		0x150
+#define MTK_XFI_TX_UC_PKT_CNT_H		0x154
+#define MTK_XFI_TX_MC_PKT_CNT_L		0x160
+#define MTK_XFI_TX_MC_PKT_CNT_H		0x164
+#define MTK_XFI_TX_BC_PKT_CNT_L		0x170
+#define MTK_XFI_TX_BC_PKT_CNT_H		0x174
+
+#define MTK_XFI_RX_PKT_CNT		0x188
+#define MTK_XFI_RX_ETH_CNT		0x18C
+#define MTK_XFI_RX_PAUSE_CNT		0x190
+#define MTK_XFI_RX_LEN_ERR_CNT		0x194
+#define MTK_XFI_RX_CRC_ERR_CNT		0x198
+#define MTK_XFI_RX_RUNT_PKT_CNT		0x1BC
+#define MTK_XFI_RX_UC_PKT_CNT_L		0x1C0
+#define MTK_XFI_RX_UC_PKT_CNT_H		0x1C4
+#define MTK_XFI_RX_MC_PKT_CNT_L		0x1D0
+#define MTK_XFI_RX_MC_PKT_CNT_H		0x1D4
+#define MTK_XFI_RX_BC_PKT_CNT_L		0x1E0
+#define MTK_XFI_RX_BC_PKT_CNT_H		0x1E4
+#define MTK_XFI_RX_UC_DROP_CNT		0x200
+#define MTK_XFI_RX_BC_DROP_CNT		0x204
+#define MTK_XFI_RX_MC_DROP_CNT		0x208
+#define MTK_XFI_RX_ALL_DROP_CNT		0x20C
+
+#define PRINT_FORMATTED_MAC_MIB64(m, reg)			\
+({								\
+	seq_printf(m, "| MAC%d_%s	: %010llu |\n",		\
+		   gdm_id + 1, #reg,				\
+		   mtk_r32(eth, MTK_MAC_MIB_BASE(gdm_id) +	\
+			   MTK_MAC_##reg##_L) +			\
+		   ((u64)mtk_r32(eth, MTK_MAC_MIB_BASE(gdm_id) +\
+				 MTK_MAC_##reg##_H) << 32));	\
+})
+
+#define PRINT_FORMATTED_XFI_MIB(m, reg, mask)			\
+({								\
+	seq_printf(m, "| XFI%d_%s	: %010lu |\n",		\
+		   gdm_id, #reg,				\
+		   FIELD_GET(mask, mtk_r32(eth,			\
+			     MTK_XFI_MIB_BASE(gdm_id) +		\
+			     MTK_XFI_##reg)));			\
+})
+
+#define PRINT_FORMATTED_XFI_MIB64(m, reg)			\
+({								\
+	seq_printf(m, "| XFI%d_%s	: %010llu |\n",		\
+		   gdm_id, #reg,				\
+		   mtk_r32(eth, MTK_XFI_MIB_BASE(gdm_id) +	\
+			   MTK_XFI_##reg##_L) +			\
+		   ((u64)mtk_r32(eth, MTK_XFI_MIB_BASE(gdm_id) +\
+				 MTK_XFI_##reg##_H) << 32));	\
+})
+
+/* HW LRO flush reason */
+#define MTK_HW_LRO_AGG_FLUSH		(1)
+#define MTK_HW_LRO_AGE_FLUSH		(2)
+#define MTK_HW_LRO_NOT_IN_SEQ_FLUSH	(3)
+#define MTK_HW_LRO_TIMESTAMP_FLUSH	(4)
+#define MTK_HW_LRO_NON_RULE_FLUSH	(5)
+
+#define SET_PDMA_RXRING_MAX_AGG_CNT(eth, x, y)						\
+{											\
+	u32 reg_val1, reg_val2;								\
+	if (MTK_HAS_CAPS(eth->soc->caps, MTK_GLO_MEM_ACCESS)) {				\
+		reg_val1 = FIELD_PREP(MTK_GLO_MEM_IDX, MTK_LRO_MEM_IDX);		\
+		reg_val1 |= FIELD_PREP(MTK_GLO_MEM_ADDR, MTK_LRO_MEM_CFG_BASE + x);	\
+		reg_val1 |= FIELD_PREP(MTK_GLO_MEM_CMD, MTK_GLO_MEM_READ);		\
+		mtk_w32(eth, reg_val1, MTK_GLO_MEM_CTRL);				\
+		reg_val1 = mtk_r32(eth, MTK_GLO_MEM_DATA(1));				\
+		reg_val1 &= ~MTK_RING_MAX_AGG_CNT;					\
+		reg_val1 |= FIELD_PREP(MTK_RING_MAX_AGG_CNT, y);			\
+		mtk_w32(eth, reg_val1, MTK_GLO_MEM_DATA(1));				\
+		reg_val1 = FIELD_PREP(MTK_GLO_MEM_IDX, MTK_LRO_MEM_IDX);		\
+		reg_val1 |= FIELD_PREP(MTK_GLO_MEM_ADDR, MTK_LRO_MEM_CFG_BASE + x);	\
+		reg_val1 |= FIELD_PREP(MTK_GLO_MEM_CMD, MTK_GLO_MEM_WRITE);		\
+		mtk_w32(eth, reg_val1, MTK_GLO_MEM_CTRL);				\
+	} else {									\
+		reg_val1 = mtk_r32(eth, MTK_LRO_CTRL_DW2_CFG(x));			\
+		reg_val2 = mtk_r32(eth, MTK_LRO_CTRL_DW3_CFG(x));			\
+		reg_val1 &= ~MTK_LRO_RING_AGG_CNT_L_MASK;				\
+		reg_val2 &= ~MTK_LRO_RING_AGG_CNT_H_MASK;				\
+		reg_val1 |= ((y) & 0x3f) << MTK_LRO_RING_AGG_CNT_L_OFFSET;		\
+		reg_val2 |= (((y) >> 6) & 0x03) <<					\
+			     MTK_LRO_RING_AGG_CNT_H_OFFSET;				\
+		mtk_w32(eth, reg_val1, MTK_LRO_CTRL_DW2_CFG(x));			\
+		mtk_w32(eth, reg_val2, MTK_LRO_CTRL_DW3_CFG(x));			\
+	}										\
+}
+
+#define SET_PDMA_RXRING_AGG_TIME(eth, x, y)						\
+{											\
+	u32 reg_val;									\
+	if (MTK_HAS_CAPS(eth->soc->caps, MTK_GLO_MEM_ACCESS)) {				\
+		reg_val = FIELD_PREP(MTK_GLO_MEM_IDX, MTK_LRO_MEM_IDX);			\
+		reg_val |= FIELD_PREP(MTK_GLO_MEM_ADDR, MTK_LRO_MEM_CFG_BASE + x);	\
+		reg_val |= FIELD_PREP(MTK_GLO_MEM_CMD, MTK_GLO_MEM_READ);		\
+		mtk_w32(eth, reg_val, MTK_GLO_MEM_CTRL);				\
+		reg_val = mtk_r32(eth, MTK_GLO_MEM_DATA(0));				\
+		reg_val &= ~MTK_RING_MAX_AGG_TIME_V2;					\
+		reg_val |= FIELD_PREP(MTK_RING_MAX_AGG_TIME_V2, y);			\
+		mtk_w32(eth, reg_val, MTK_GLO_MEM_DATA(0));				\
+		reg_val = FIELD_PREP(MTK_GLO_MEM_IDX, MTK_LRO_MEM_IDX);			\
+		reg_val |= FIELD_PREP(MTK_GLO_MEM_ADDR, MTK_LRO_MEM_CFG_BASE + x);	\
+		reg_val |= FIELD_PREP(MTK_GLO_MEM_CMD, MTK_GLO_MEM_WRITE);		\
+		mtk_w32(eth, reg_val, MTK_GLO_MEM_CTRL);				\
+	} else {									\
+		reg_val = mtk_r32(eth, MTK_LRO_CTRL_DW2_CFG(x));			\
+		reg_val &= ~MTK_LRO_RING_AGG_TIME_MASK;					\
+		reg_val |= ((y) & 0xffff) << MTK_LRO_RING_AGG_TIME_OFFSET;		\
+		mtk_w32(eth, reg_val, MTK_LRO_CTRL_DW2_CFG(x));				\
+	}										\
+}
+
+#define SET_PDMA_RXRING_AGE_TIME(eth, x, y)						\
+{											\
+	u32 reg_val1, reg_val2;								\
+	if (MTK_HAS_CAPS(eth->soc->caps, MTK_GLO_MEM_ACCESS)) {				\
+		reg_val1 = FIELD_PREP(MTK_GLO_MEM_IDX, MTK_LRO_MEM_IDX);		\
+		reg_val1 |= FIELD_PREP(MTK_GLO_MEM_ADDR, MTK_LRO_MEM_CFG_BASE + x);	\
+		reg_val1 |= FIELD_PREP(MTK_GLO_MEM_CMD, MTK_GLO_MEM_READ);		\
+		mtk_w32(eth, reg_val1, MTK_GLO_MEM_CTRL);				\
+		reg_val1 = mtk_r32(eth, MTK_GLO_MEM_DATA(0));				\
+		reg_val1 &= ~MTK_RING_AGE_TIME;						\
+		reg_val1 |= FIELD_PREP(MTK_RING_AGE_TIME, y);				\
+		mtk_w32(eth, reg_val1, MTK_GLO_MEM_DATA(0));				\
+		reg_val1 = FIELD_PREP(MTK_GLO_MEM_IDX, MTK_LRO_MEM_IDX);		\
+		reg_val1 |= FIELD_PREP(MTK_GLO_MEM_ADDR, MTK_LRO_MEM_CFG_BASE + x);	\
+		reg_val1 |= FIELD_PREP(MTK_GLO_MEM_CMD, MTK_GLO_MEM_WRITE);		\
+		mtk_w32(eth, reg_val1, MTK_GLO_MEM_CTRL);				\
+	} else {									\
+		reg_val1 = mtk_r32(eth, MTK_LRO_CTRL_DW1_CFG(x));			\
+		reg_val2 = mtk_r32(eth, MTK_LRO_CTRL_DW2_CFG(x));			\
+		reg_val1 &= ~MTK_LRO_RING_AGE_TIME_L_MASK;				\
+		reg_val2 &= ~MTK_LRO_RING_AGE_TIME_H_MASK;				\
+		reg_val1 |= ((y) & 0x3ff) << MTK_LRO_RING_AGE_TIME_L_OFFSET;		\
+		reg_val2 |= (((y) >> 10) & 0x03f) <<					\
+			     MTK_LRO_RING_AGE_TIME_H_OFFSET;				\
+		mtk_w32(eth, reg_val1, MTK_LRO_CTRL_DW1_CFG(x));			\
+		mtk_w32(eth, reg_val2, MTK_LRO_CTRL_DW2_CFG(x));			\
+	}										\
+}
+
+#define SET_PDMA_LRO_BW_THRESHOLD(eth, x)						\
+{											\
+	u32 reg_val;									\
+	if (MTK_HAS_CAPS(eth->soc->caps, MTK_GLO_MEM_ACCESS)) {				\
+		return -EINVAL;								\
+	} else {									\
+		reg_val = mtk_r32(eth, MTK_PDMA_LRO_CTRL_DW2);				\
+		reg_val = (x);								\
+		mtk_w32(eth, reg_val, MTK_PDMA_LRO_CTRL_DW2);				\
+	}										\
+}
+
+#define SET_PDMA_RXRING_VALID(eth, x, y)						\
+{											\
+	u32 reg_val;									\
+	if (MTK_HAS_CAPS(eth->soc->caps, MTK_GLO_MEM_ACCESS)) {				\
+		reg_val = FIELD_PREP(MTK_GLO_MEM_IDX, MTK_LRO_MEM_IDX);			\
+		reg_val |= FIELD_PREP(MTK_GLO_MEM_ADDR, MTK_LRO_MEM_CFG_BASE + x);	\
+		reg_val |= FIELD_PREP(MTK_GLO_MEM_CMD, MTK_GLO_MEM_READ);		\
+		mtk_w32(eth, reg_val, MTK_GLO_MEM_CTRL);				\
+		reg_val = mtk_r32(eth, MTK_GLO_MEM_DATA(1));				\
+		reg_val |= FIELD_PREP(MTK_LRO_DATA_VLD, y);				\
+		mtk_w32(eth, reg_val, MTK_GLO_MEM_DATA(1));				\
+		reg_val = FIELD_PREP(MTK_GLO_MEM_IDX, MTK_LRO_MEM_IDX);			\
+		reg_val |= FIELD_PREP(MTK_GLO_MEM_ADDR, MTK_LRO_MEM_CFG_BASE + x);	\
+		reg_val |= FIELD_PREP(MTK_GLO_MEM_CMD, MTK_GLO_MEM_WRITE);		\
+		mtk_w32(eth, reg_val, MTK_GLO_MEM_CTRL);				\
+	} else {									\
+		reg_val = mtk_r32(eth, MTK_LRO_CTRL_DW2_CFG(x));			\
+		reg_val &= ~(0x1 << MTK_LRO_RING_RX_PORT_VLD_OFFSET);			\
+		reg_val |= ((y) & 0x1) << MTK_LRO_RING_RX_PORT_VLD_OFFSET;		\
+		mtk_w32(eth, reg_val, MTK_LRO_CTRL_DW2_CFG(x));				\
+	}										\
+}
+
+struct mtk_hwlro_alt_v1_info0 {
+	u32 dtp : 16;
+	u32 stp : 16;
+};
+
+struct mtk_hwlro_alt_v1_info1 {
+	u32 sip0 : 32;
+};
+
+struct mtk_hwlro_alt_v1_info2 {
+	u32 sip1 : 32;
+};
+
+struct mtk_hwlro_alt_v1_info3 {
+	u32 sip2 : 32;
+};
+
+struct mtk_hwlro_alt_v1_info4 {
+	u32 sip3 : 32;
+};
+
+struct mtk_hwlro_alt_v1_info5 {
+	u32 vlan_vid0 : 32;
+};
+
+struct mtk_hwlro_alt_v1_info6 {
+	u32 vlan_vid1 : 16;
+	u32 vlan_vid_vld : 4;
+	u32 cnt : 12;
+};
+
+struct mtk_hwlro_alt_v1_info7 {
+	u32 dw_len : 32;
+};
+
+struct mtk_hwlro_alt_v1_info8 {
+	u32 dip_id : 2;
+	u32 ipv6 : 1;
+	u32 ipv4 : 1;
+	u32 resv : 27;
+	u32 valid : 1;
+};
+
+struct mtk_hwlro_alt_v1 {
+	struct mtk_hwlro_alt_v1_info0 alt_info0;
+	struct mtk_hwlro_alt_v1_info1 alt_info1;
+	struct mtk_hwlro_alt_v1_info2 alt_info2;
+	struct mtk_hwlro_alt_v1_info3 alt_info3;
+	struct mtk_hwlro_alt_v1_info4 alt_info4;
+	struct mtk_hwlro_alt_v1_info5 alt_info5;
+	struct mtk_hwlro_alt_v1_info6 alt_info6;
+	struct mtk_hwlro_alt_v1_info7 alt_info7;
+	struct mtk_hwlro_alt_v1_info8 alt_info8;
+};
+
+struct mtk_hwlro_alt_v2_info0 {
+	u32 v2_id_h:3;
+	u32 v1_id:12;
+	u32 v0_id:12;
+	u32 v3_valid:1;
+	u32 v2_valid:1;
+	u32 v1_valid:1;
+	u32 v0_valid:1;
+	u32 valid:1;
+};
+
+struct mtk_hwlro_alt_v2_info1 {
+	u32 sip3_h:9;
+	u32 v6_valid:1;
+	u32 v4_valid:1;
+	u32 v3_id:12;
+	u32 v2_id_l:9;
+};
+
+struct mtk_hwlro_alt_v2_info2 {
+	u32 sip2_h:9;
+	u32 sip3_l:23;
+};
+
+struct mtk_hwlro_alt_v2_info3 {
+	u32 sip1_h:9;
+	u32 sip2_l:23;
+};
+
+struct mtk_hwlro_alt_v2_info4 {
+	u32 sip0_h:9;
+	u32 sip1_l:23;
+};
+
+struct mtk_hwlro_alt_v2_info5 {
+	u32 dip3_h:9;
+	u32 sip0_l:23;
+};
+
+struct mtk_hwlro_alt_v2_info6 {
+	u32 dip2_h:9;
+	u32 dip3_l:23;
+};
+
+struct mtk_hwlro_alt_v2_info7 {
+	u32 dip1_h:9;
+	u32 dip2_l:23;
+};
+
+struct mtk_hwlro_alt_v2_info8 {
+	u32 dip0_h:9;
+	u32 dip1_l:23;
+};
+
+struct mtk_hwlro_alt_v2_info9 {
+	u32 sp_h:9;
+	u32 dip0_l:23;
+};
+
+struct mtk_hwlro_alt_v2_info10 {
+	u32 resv:9;
+	u32 dp:16;
+	u32 sp_l:7;
+};
+
+struct mtk_hwlro_alt_v2 {
+	struct mtk_hwlro_alt_v2_info0 alt_info0;
+	struct mtk_hwlro_alt_v2_info1 alt_info1;
+	struct mtk_hwlro_alt_v2_info2 alt_info2;
+	struct mtk_hwlro_alt_v2_info3 alt_info3;
+	struct mtk_hwlro_alt_v2_info4 alt_info4;
+	struct mtk_hwlro_alt_v2_info5 alt_info5;
+	struct mtk_hwlro_alt_v2_info6 alt_info6;
+	struct mtk_hwlro_alt_v2_info7 alt_info7;
+	struct mtk_hwlro_alt_v2_info8 alt_info8;
+	struct mtk_hwlro_alt_v2_info9 alt_info9;
+	struct mtk_hwlro_alt_v2_info10 alt_info10;
+};
+
+struct mtk_esw_reg {
+	unsigned int off;
+	unsigned int val;
+};
+
+struct mtk_mii_ioctl_data {
+	u16 phy_id;
+	u16 reg_num;
+	unsigned int val_in;
+	unsigned int val_out;
+};
+
+struct mtk_hwlro_stats {
+	u32 agg_num_cnt[4][MTK_HW_LRO_MAX_AGG_CNT + 1];
+	u32 agg_size_cnt[4][16];
+	u32 tot_agg_cnt[4];
+	u32 tot_flush_cnt[4];
+	u32 agg_flush_cnt[4];
+	u32 age_flush_cnt[4];
+	u32 seq_flush_cnt[4];
+	u32 timestamp_flush_cnt[4];
+	u32 norule_flush_cnt[4];
+	u32 ebl;
+};
+
+struct mtk_mt7530_switch {
+	void __iomem	*mmio_base;
+	bool		exist;
+};
+
+struct mtk_qdma_ctx {
+	struct mtk_eth	*eth;
+	long		id;
+};
+
+struct mtk_eth_debugfs {
+	struct dentry			*root;
+	struct proc_dir_entry		*procfs_root;
+	struct mtk_hwlro_stats		hwlro_stats;
+	struct mtk_mt7530_switch	mt7530;
+	struct mtk_qdma_ctx		qdma_sched[4];
+	struct mtk_qdma_ctx		qdma_queue[MTK_QDMA_NUM_QUEUES];
+};
+
+typedef int (*mtk_hwlro_dbg_func) (struct mtk_eth *eth, int par);
+
+extern int _mtk_mdio_write_c22(struct mtk_eth *eth, u32 phy_addr, u32 phy_reg,
+			       u32 write_data);
+extern int _mtk_mdio_write_c45(struct mtk_eth *eth, u32 phy_addr,
+			       u32 devad, u32 phy_reg, u32 write_data);
+extern int _mtk_mdio_read_c22(struct mtk_eth *eth, u32 phy_addr, u32 phy_reg);
+extern int _mtk_mdio_read_c45(struct mtk_eth *eth, u32 phy_addr,
+			      u32 devad, u32 phy_reg);
+extern void mtk_switch_w32(struct mtk_eth *eth, u32 val, unsigned int reg);
+extern u32 mtk_switch_r32(struct mtk_eth *eth, unsigned int reg);
+
+void mtk_eth_debugfs_hwlro_stats_update(struct mtk_eth *eth, u32 ring_no,
+					struct mtk_rx_dma_v2 *rxd);
+void mtk_eth_debugfs_hwlro_flush_stats_update(struct mtk_eth *eth, u32 ring_no,
+					      struct mtk_rx_dma_v2 *rxd);
+
+int mtk_eth_debugfs_priv_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd);
+
+int mtk_eth_debugfs_init(struct mtk_eth *eth);
+void mtk_eth_debugfs_exit(struct mtk_eth *eth);
+
+#endif /* MTK_ETH_DBG_H */
diff --git a/drivers/net/ethernet/mediatek/mtk_eth_soc.c b/drivers/net/ethernet/mediatek/mtk_eth_soc.c
index 0dcb94f..10019be 100644
--- a/drivers/net/ethernet/mediatek/mtk_eth_soc.c
+++ b/drivers/net/ethernet/mediatek/mtk_eth_soc.c
@@ -36,6 +36,7 @@
 #include <net/page_pool/helpers.h>
 
 #include "mtk_eth_soc.h"
+#include "mtk_eth_dbg.h"
 #include "mtk_wed.h"
 
 static int mtk_msg_level = -1;
@@ -87,9 +88,11 @@ static const struct mtk_reg_map mtk_reg_map = {
 		.fc_th		= 0x1a10,
 		.tx_sch_rate	= 0x1a14,
 		.int_grp	= 0x1a20,
+		.fsm		= 0x1a34,
 		.hred		= 0x1a44,
 		.ctx_ptr	= 0x1b00,
 		.dtx_ptr	= 0x1b04,
+		.fwd_count	= 0x1b08,
 		.crx_ptr	= 0x1b10,
 		.drx_ptr	= 0x1b14,
 		.fq_head	= 0x1b20,
@@ -117,6 +120,7 @@ static const struct mtk_reg_map mt7628_reg_map = {
 		.rx_ptr		= 0x0900,
 		.rx_cnt_cfg	= 0x0904,
 		.pcrx_ptr	= 0x0908,
+		.pdrx_ptr	= 0x090c,
 		.glo_cfg	= 0x0a04,
 		.rst_idx	= 0x0a08,
 		.delay_irq	= 0x0a0c,
@@ -134,6 +138,7 @@ static const struct mtk_reg_map mt7986_reg_map = {
 		.rx_ptr		= 0x4100,
 		.rx_cnt_cfg	= 0x4104,
 		.pcrx_ptr	= 0x4108,
+		.pdrx_ptr	= 0x410c,
 		.glo_cfg	= 0x4204,
 		.rst_idx	= 0x4208,
 		.delay_irq	= 0x420c,
@@ -160,9 +165,12 @@ static const struct mtk_reg_map mt7986_reg_map = {
 		.delay_irq	= 0x460c,
 		.fc_th		= 0x4610,
 		.int_grp	= 0x4620,
+		.fsm		= 0x4634,
 		.hred		= 0x4644,
+		.qtx_mib_if	= 0x46bc,
 		.ctx_ptr	= 0x4700,
 		.dtx_ptr	= 0x4704,
+		.fwd_count	= 0x4708,
 		.crx_ptr	= 0x4710,
 		.drx_ptr	= 0x4714,
 		.fq_head	= 0x4720,
@@ -192,6 +200,7 @@ static const struct mtk_reg_map mt7988_reg_map = {
 		.rx_ptr		= 0x6900,
 		.rx_cnt_cfg	= 0x6904,
 		.pcrx_ptr	= 0x6908,
+		.pdrx_ptr	= 0x690c,
 		.glo_cfg	= 0x6a04,
 		.rst_idx	= 0x6a08,
 		.delay_irq	= 0x6a0c,
@@ -223,9 +232,12 @@ static const struct mtk_reg_map mt7988_reg_map = {
 		.delay_irq	= 0x460c,
 		.fc_th		= 0x4610,
 		.int_grp	= 0x4620,
+		.fsm		= 0x4634,
 		.hred		= 0x4644,
+		.qtx_mib_if	= 0x46bc,
 		.ctx_ptr	= 0x4700,
 		.dtx_ptr	= 0x4704,
+		.fwd_count	= 0x4708,
 		.crx_ptr	= 0x4710,
 		.drx_ptr	= 0x4714,
 		.fq_head	= 0x4720,
@@ -360,8 +372,8 @@ static int mtk_mdio_busy_wait(struct mtk_eth *eth)
 	return -ETIMEDOUT;
 }
 
-static int _mtk_mdio_write_c22(struct mtk_eth *eth, u32 phy_addr, u32 phy_reg,
-			       u32 write_data)
+int _mtk_mdio_write_c22(struct mtk_eth *eth, u32 phy_addr, u32 phy_reg,
+			u32 write_data)
 {
 	int ret;
 
@@ -384,8 +396,8 @@ static int _mtk_mdio_write_c22(struct mtk_eth *eth, u32 phy_addr, u32 phy_reg,
 	return 0;
 }
 
-static int _mtk_mdio_write_c45(struct mtk_eth *eth, u32 phy_addr,
-			       u32 devad, u32 phy_reg, u32 write_data)
+int _mtk_mdio_write_c45(struct mtk_eth *eth, u32 phy_addr,
+			u32 devad, u32 phy_reg, u32 write_data)
 {
 	int ret;
 
@@ -420,7 +432,7 @@ static int _mtk_mdio_write_c45(struct mtk_eth *eth, u32 phy_addr,
 	return 0;
 }
 
-static int _mtk_mdio_read_c22(struct mtk_eth *eth, u32 phy_addr, u32 phy_reg)
+int _mtk_mdio_read_c22(struct mtk_eth *eth, u32 phy_addr, u32 phy_reg)
 {
 	int ret;
 
@@ -442,8 +454,8 @@ static int _mtk_mdio_read_c22(struct mtk_eth *eth, u32 phy_addr, u32 phy_reg)
 	return mtk_r32(eth, MTK_PHY_IAC) & PHY_IAC_DATA_MASK;
 }
 
-static int _mtk_mdio_read_c45(struct mtk_eth *eth, u32 phy_addr,
-			      u32 devad, u32 phy_reg)
+int _mtk_mdio_read_c45(struct mtk_eth *eth, u32 phy_addr,
+		       u32 devad, u32 phy_reg)
 {
 	int ret;
 
@@ -2686,6 +2698,12 @@ static int mtk_poll_rx(struct napi_struct *napi, int budget,
 		if (reason == MTK_PPE_CPU_REASON_HIT_UNBIND_RATE_REACHED)
 			mtk_ppe_check_skb(eth->ppe[ppe_idx], skb, hash);
 
+		if (eth->hwlro && eth->debugfs->hwlro_stats.ebl &&
+		    IS_HW_LRO_RING(ring->ring_no)) {
+			mtk_eth_debugfs_hwlro_stats_update(eth, ring->ring_no, &trxd);
+			mtk_eth_debugfs_hwlro_flush_stats_update(eth, ring->ring_no, &trxd);
+		}
+
 		skb_record_rx_queue(skb, 0);
 		napi_gro_receive(napi, skb);
 
@@ -2884,12 +2902,12 @@ static int mtk_poll_tx(struct mtk_eth *eth, int budget)
 
 static void mtk_handle_status_irq(struct mtk_eth *eth)
 {
-	u32 status2 = mtk_r32(eth, MTK_INT_STATUS2);
+	u32 status2 = mtk_r32(eth, MTK_FE_INT_STATUS);
 
 	if (unlikely(status2 & (MTK_GDM1_AF | MTK_GDM2_AF))) {
 		mtk_stats_update(eth);
 		mtk_w32(eth, (MTK_GDM1_AF | MTK_GDM2_AF),
-			MTK_INT_STATUS2);
+			MTK_FE_INT_STATUS);
 	}
 }
 
@@ -4038,7 +4056,7 @@ static void mtk_dma_free(struct mtk_eth *eth)
 
 static bool mtk_hw_reset_check(struct mtk_eth *eth)
 {
-	u32 val = mtk_r32(eth, MTK_INT_STATUS2);
+	u32 val = mtk_r32(eth, MTK_FE_INT_STATUS);
 
 	return (val & MTK_FE_INT_FQ_EMPTY) || (val & MTK_FE_INT_RFIFO_UF) ||
 	       (val & MTK_FE_INT_RFIFO_OV) || (val & MTK_FE_INT_TSO_FAIL) ||
@@ -5439,6 +5457,12 @@ static void mtk_restore_qdma_cfg(struct mtk_eth *eth)
 			reg_map->qdma.tx_sch_rate + 0x4);
 }
 
+static int mtk_siocdevprivate(struct net_device *dev, struct ifreq *ifr,
+			      void __user *data, int cmd)
+{
+	return mtk_eth_debugfs_priv_ioctl(dev, ifr, cmd);
+}
+
 static void mtk_prepare_for_reset(struct mtk_eth *eth)
 {
 	struct mtk_mac *mac;
@@ -5989,6 +6013,7 @@ static const struct net_device_ops mtk_netdev_ops = {
 	.ndo_set_mac_address	= mtk_set_mac_address,
 	.ndo_validate_addr	= eth_validate_addr,
 	.ndo_eth_ioctl		= mtk_do_ioctl,
+	.ndo_siocdevprivate	= mtk_siocdevprivate,
 	.ndo_change_mtu		= mtk_change_mtu,
 	.ndo_tx_timeout		= mtk_tx_timeout,
 	.ndo_get_stats64        = mtk_get_stats64,
@@ -6844,6 +6869,8 @@ static int mtk_probe(struct platform_device *pdev)
 		}
 	}
 
+	mtk_eth_debugfs_init(eth);
+
 	/* we run 2 devices on the same DMA ring so we need a dummy device
 	 * for NAPI to work
 	 */
diff --git a/drivers/net/ethernet/mediatek/mtk_eth_soc.h b/drivers/net/ethernet/mediatek/mtk_eth_soc.h
index d544eb1..ce2f45c 100644
--- a/drivers/net/ethernet/mediatek/mtk_eth_soc.h
+++ b/drivers/net/ethernet/mediatek/mtk_eth_soc.h
@@ -119,6 +119,11 @@
 /* Frame Engine Interrupt Status 2 Register */
 #define MTK_FE_INT_STATUS2	0x28
 
+/* Frame Engine LRO Auto-Learn Table Information */
+#define MTK_FE_ALT_CF8		0x300
+#define MTK_FE_ALT_SGL_CFC	0x304
+#define MTK_FE_ALT_SEQ_CFC	0x308
+
 /* CDMP Ingress Control Register */
 #define MTK_CDMQ_IG_CTRL	0x1400
 #define MTK_CDMQ_STAG_EN	BIT(0)
@@ -269,6 +274,13 @@
 #define MTK_RSS_INDR_TABLE_DW(x)	(reg_map->pdma.rss_glo_cfg + 0x50 +	\
 					 ((x) * 0x4))
 
+/* PDMA HW LRO ALT Debug Registers */
+#define MTK_LRO_ALT_DBG		0xc40
+#define MTK_LRO_ALT_INDEX_OFFSET	(8)
+
+/* PDMA HW LRO ALT Data Registers */
+#define MTK_LRO_ALT_DBG_DATA	0xc44
+
 /* PDMA Global Configuration Register */
 #define MTK_PDMA_LRO_SDL	0x3000
 #define MTK_RX_CFG_SDL_OFFSET	16
@@ -312,7 +324,29 @@
 #define MTK_RING_MAX_AGG_CNT_L		((MTK_HW_LRO_MAX_AGG_CNT & 0x3f) << 26)
 #define MTK_RING_MAX_AGG_CNT_H		((MTK_HW_LRO_MAX_AGG_CNT >> 6) & 0x3)
 
+/* PDMA HW LRO Ring Control Mask */
+#define MTK_LRO_RING_AGG_CNT_H_MASK	GENMASK(1, 0)
+#define MTK_LRO_RING_AGG_TIME_MASK	GENMASK(25, 10)
+#define MTK_LRO_RING_AGG_CNT_L_MASK	GENMASK(31, 26)
+#define MTK_LRO_RING_AGE_TIME_H_MASK	GENMASK(5, 0)
+#define MTK_LRO_RING_AGE_TIME_L_MASK	GENMASK(31, 22)
+
+/* PDMA HW LRO Ring Control 1 Offsets */
+#define MTK_LRO_RING_AGE_TIME_L_OFFSET	(22)
+
+/* PDMA HW LRO Ring Control 2 Offsets */
+#define MTK_LRO_RING_AGE_TIME_H_OFFSET	(0)
+#define MTK_LRO_RING_RX_MODE_OFFSET	(6)
+#define MTK_LRO_RING_RX_PORT_VLD_OFFSET	(8)
+#define MTK_LRO_RING_AGG_TIME_OFFSET	(10)
+#define MTK_LRO_RING_AGG_CNT_L_OFFSET	(26)
+
+/* PDMA HW LRO Ring Control 3 Offsets */
+#define MTK_LRO_RING_AGG_CNT_H_OFFSET	(0)
+
 /* QDMA TX Queue Configuration Registers */
+#define MTK_QTX_CFG_HW_RESV	GENMASK(15, 8)
+#define MTK_QTX_CFG_SW_RESV	GENMASK(7, 0)
 #define MTK_QTX_OFFSET		0x10
 #define QDMA_RES_THRES		4
 
@@ -340,8 +374,19 @@
 /* QDMA Page Configuration Register */
 #define MTK_QTX_PER_PAGE	(16)
 
+/* QDMA Page Configuration Register */
+#define MTK_QTX_PER_PAGE	(16)
+
+/* QDMA TX Queue MIB Interface Register */
+#define MTK_MIB_ON_QTX_CFG	BIT(31)
+#define MTK_VQTX_MIB_EN		BIT(28)
+
 /* QDMA TX Scheduler Rate Control Register */
+#define MTK_QDMA_TX_SCH			GENMASK(15, 0)
 #define MTK_QDMA_TX_SCH_MAX_WFQ		BIT(15)
+#define MTK_QDMA_TX_SCH_RATE_EN		BIT(11)
+#define MTK_QDMA_TX_SCH_RATE_MAN	GENMASK(10, 4)
+#define MTK_QDMA_TX_SCH_RATE_EXP	GENMASK(3, 0)
 
 /* QDMA Global Configuration Register */
 #define MTK_RX_2B_OFFSET	BIT(31)
@@ -476,6 +521,10 @@
 #define RX_DMA_L4_VALID_PDMA	BIT(30)		/* when PDMA is used */
 #define RX_DMA_SPECIAL_TAG	BIT(22)
 
+/* PDMA descriptor rxd2 */
+#define RX_DMA_GET_AGG_CNT	GENMASK(9, 2)
+#define RX_DMA_GET_REV		GENMASK(15, 10)
+
 /* PDMA descriptor rxd5 */
 #define MTK_RXD5_FOE_ENTRY	GENMASK(14, 0)
 #define MTK_RXD5_PPE_CPU_REASON	GENMASK(22, 18)
@@ -488,6 +537,10 @@
 #define RX_DMA_VTAG_V2		BIT(0)
 #define RX_DMA_L4_VALID_V2	BIT(2)
 
+/* PDMA V2 descriptor rxd6 */
+#define RX_DMA_GET_FLUSH_RSN_V2	GENMASK(2, 0)
+#define RX_DMA_GET_AGG_CNT_V2	GENMASK(23, 16)
+
 /* PHY Polling and SMI Master Control registers */
 #define MTK_PPSC		0x10000
 #define PPSC_MDC_CFG		GENMASK(29, 24)
@@ -757,6 +810,7 @@
 #define MTK_FE_CDM4_FSM		0x298
 #define MTK_FE_CDM5_FSM		0x318
 #define MTK_FE_CDM6_FSM		0x328
+#define MTK_FE_CDM7_FSM		0x338
 #define MTK_FE_GDM1_FSM		0x228
 #define MTK_FE_GDM2_FSM		0x22C
 #define MTK_FE_GDM3_FSM		0x23C
@@ -1471,9 +1525,12 @@ struct mtk_reg_map {
 		u32	delay_irq;	/* delay interrupt */
 		u32	fc_th;		/* flow control */
 		u32	int_grp;
+		u32	fsm;
 		u32	hred;		/* interrupt mask */
+		u32	qtx_mib_if;	/* tx queue mib interface */
 		u32	ctx_ptr;	/* tx acquire cpu pointer */
 		u32	dtx_ptr;	/* tx acquire dma pointer */
+		u32	fwd_count;	/* tx forward count */
 		u32	crx_ptr;	/* tx release cpu pointer */
 		u32	drx_ptr;	/* tx release dma pointer */
 		u32	fq_head;	/* fq head pointer */
@@ -1628,6 +1685,7 @@ struct mtk_eth {
 	struct mtk_qdma_shaper		qdma_shaper;
 	struct clk			*clks[MTK_CLK_MAX];
 
+	struct mtk_eth_debugfs		*debugfs;
 	struct mii_bus			*mii_bus;
 	unsigned int			mdc_divider;
 	struct work_struct		pending_work;
-- 
2.45.2

