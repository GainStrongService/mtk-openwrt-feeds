--- a/net/8021q/vlan_dev.c
+++ b/net/8021q/vlan_dev.c
@@ -641,6 +641,9 @@ static netdev_features_t vlan_dev_fix_fe
 	features = netdev_intersect_features(features, lower_features);
 	features |= old_features & (NETIF_F_SOFT_FEATURES | NETIF_F_GSO_SOFTWARE_ALL);
 
+	if (old_features & NETIF_F_HW_ESP)
+		features |= NETIF_F_HW_ESP;
+
 	return features;
 }
 
--- a/net/xfrm/xfrm_output.c
+++ b/net/xfrm/xfrm_output.c
@@ -610,40 +610,6 @@ out:
 }
 EXPORT_SYMBOL_GPL(xfrm_output_resume);
 
-static int xfrm_dev_direct_output(struct sock *sk, struct xfrm_state *x,
-				  struct sk_buff *skb)
-{
-	struct dst_entry *dst = skb_dst(skb);
-	struct net *net = xs_net(x);
-	int err;
-
-	dst = skb_dst_pop(skb);
-	if (!dst) {
-		XFRM_INC_STATS(net, LINUX_MIB_XFRMOUTERROR);
-		kfree_skb(skb);
-		return -EHOSTUNREACH;
-	}
-	skb_dst_set(skb, dst);
-	nf_reset_ct(skb);
-
-	err = skb_dst(skb)->ops->local_out(net, sk, skb);
-	if (unlikely(err != 1)) {
-		kfree_skb(skb);
-		return err;
-	}
-
-	/* In transport mode, network destination is
-	 * directly reachable, while in tunnel mode,
-	 * inner packet network may not be. In packet
-	 * offload type, HW is responsible for hard
-	 * header packet mangling so directly xmit skb
-	 * to netdevice.
-	 */
-	skb->dev = x->xso.dev;
-	__skb_push(skb, skb->dev->hard_header_len);
-	return dev_queue_xmit(skb);
-}
-
 static int xfrm_output2(struct net *net, struct sock *sk, struct sk_buff *skb)
 {
 	return xfrm_output_resume(sk, skb, 1);
@@ -763,18 +729,16 @@ int xfrm_output(struct sock *sk, struct
 			return -EHOSTUNREACH;
 		}
 
-		/* Exclusive direct xmit for tunnel mode, as
-		 * some filtering or matching rules may apply
-		 * in transport mode.
-		 * Locally generated packets also require
-		 * the normal XFRM path for L2 header setup,
-		 * as the hardware needs the L2 header to match
-		 * for encryption, so skip direct output as well.
+		/* This is just a workaround to mark the packet
+		 * which needs to be free with packet offload
 		 */
-		if (x->props.mode == XFRM_MODE_TUNNEL && !skb->sk)
-			return xfrm_dev_direct_output(sk, x, skb);
+		if (skb->inner_protocol == IPPROTO_RSVP) {
+			XFRM_INC_STATS(net, LINUX_MIB_XFRMOUTERROR);
+			kfree_skb(skb);
+			return -EHOSTUNREACH;
+		}
 
-		return xfrm_output_resume(sk, skb, 0);
+		return 0;
 	}
 
 	secpath_reset(skb);
--- a/net/xfrm/xfrm_policy.c
+++ b/net/xfrm/xfrm_policy.c
@@ -3790,6 +3790,13 @@ int __xfrm_policy_check(struct sock *sk,
 	}
 #endif
 
+	/* Inbound HW offload packets, pass the check directly */
+	if (pol->xdo.type == XFRM_DEV_OFFLOAD_PACKET &&
+	    (pol->xdo.dir == XFRM_DEV_OFFLOAD_IN || pol->xdo.dir == XFRM_DEV_OFFLOAD_FWD)) {
+		xfrm_pols_put(pols, npols);
+		return 1;
+	}
+
 	if (pol->action == XFRM_POLICY_ALLOW) {
 		static struct sec_path dummy;
 		struct xfrm_tmpl *tp[XFRM_MAX_DEPTH];
@@ -3798,6 +3805,16 @@ int __xfrm_policy_check(struct sock *sk,
 		int ti = 0;
 		int i, k;
 
+		/* Strongswan install FWD policy for inbound HW offload
+		* packets. But cannot find corresponding packet offload
+		* state here and will be drop. So, we bypass following
+		* check for FWD policy with acction allow.
+		*/
+		if (dir == XFRM_POLICY_FWD) {
+			xfrm_pols_put(pols, npols);
+			return 1;
+		}
+
 		sp = skb_sec_path(skb);
 		if (!sp)
 			sp = &dummy;
