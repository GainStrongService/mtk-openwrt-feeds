From 4c48cf1085396a13cf64a26c5c31d822d8d883e9 Mon Sep 17 00:00:00 2001
From: "chak-kei.lam" <chak-kei.lam@mediatek.com>
Date: Fri, 18 Oct 2024 15:56:49 +0800
Subject: [PATCH] net ethernet mtkhnat add flow entry wifitx protect

---
 drivers/net/ethernet/mediatek/mtk_hnat/hnat.c |  85 +++++-
 drivers/net/ethernet/mediatek/mtk_hnat/hnat.h |  31 ++-
 .../ethernet/mediatek/mtk_hnat/hnat_mcast.c   |   2 +-
 .../ethernet/mediatek/mtk_hnat/hnat_nf_hook.c | 263 ++++++++++--------
 4 files changed, 256 insertions(+), 125 deletions(-)

diff --git a/drivers/net/ethernet/mediatek/mtk_hnat/hnat.c b/drivers/net/ethernet/mediatek/mtk_hnat/hnat.c
index e9508eb..b73e99e 100644
--- a/drivers/net/ethernet/mediatek/mtk_hnat/hnat.c
+++ b/drivers/net/ethernet/mediatek/mtk_hnat/hnat.c
@@ -28,6 +28,7 @@
 struct mtk_hnat *hnat_priv;
 static struct socket *_hnat_roam_sock;
 static struct work_struct _hnat_roam_work;
+static struct delayed_work _hnat_flow_entry_teardown_work;
 
 int (*ra_sw_nat_hook_rx)(struct sk_buff *skb) = NULL;
 EXPORT_SYMBOL(ra_sw_nat_hook_rx);
@@ -545,6 +546,64 @@ static void hnat_roaming_disable(void)
 	pr_info("hnat roaming work disable\n");
 }
 
+static void hnat_flow_entry_teardown_all(u32 ppe_id)
+{
+	struct hnat_flow_entry *flow_entry;
+	struct hlist_head *head;
+	struct hlist_node *n;
+	int index;
+
+	spin_lock_bh(&hnat_priv->flow_entry_lock);
+	for (index = 0; index < DEF_ETRY_NUM / 4; index++) {
+		head = &hnat_priv->foe_flow[ppe_id][index];
+		hlist_for_each_entry_safe(flow_entry, n, head, list) {
+			hnat_flow_entry_delete(flow_entry);
+		}
+	}
+	spin_unlock_bh(&hnat_priv->flow_entry_lock);
+}
+
+static void hnat_flow_entry_teardown_handler(struct work_struct *work)
+{
+	struct hnat_flow_entry *flow_entry;
+	struct hlist_head *head;
+	struct hlist_node *n;
+	unsigned long now = jiffies;
+	int index, i;
+	u32 cnt = 0;
+
+	spin_lock_bh(&hnat_priv->flow_entry_lock);
+	for (i = 0; i < CFG_PPE_NUM; i++) {
+		for (index = 0; index < DEF_ETRY_NUM / 4; index++) {
+			head = &hnat_priv->foe_flow[i][index];
+			hlist_for_each_entry_safe(flow_entry, n, head, list) {
+				/* If the entry is updated before 30 seconds, teardown it. */
+				if (time_after(now, flow_entry->last_update + msecs_to_jiffies(30 * 1000))) {
+					hnat_flow_entry_delete(flow_entry);
+					cnt++;
+				}
+			}
+		}
+	}
+	spin_unlock_bh(&hnat_priv->flow_entry_lock);
+
+	if (debug_level >= 2 && cnt > 0)
+		pr_info("[%s]: Teardown %d entries\n", __func__, cnt);
+
+	schedule_delayed_work(&_hnat_flow_entry_teardown_work, msecs_to_jiffies(1000));
+}
+
+static void hnat_flow_entry_teardown_enable(void)
+{
+	INIT_DELAYED_WORK(&_hnat_flow_entry_teardown_work, hnat_flow_entry_teardown_handler);
+	schedule_delayed_work(&_hnat_flow_entry_teardown_work, msecs_to_jiffies(1000));
+}
+
+static void hnat_flow_entry_teardown_disable(void)
+{
+	cancel_delayed_work(&_hnat_flow_entry_teardown_work);
+}
+
 static int hnat_hw_init(u32 ppe_id)
 {
 	if (ppe_id >= CFG_PPE_NUM)
@@ -654,6 +713,7 @@ static int hnat_hw_init(u32 ppe_id)
 	dev_info(hnat_priv->dev, "PPE%d hwnat start\n", ppe_id);
 
 	spin_lock_init(&hnat_priv->entry_lock);
+	spin_lock_init(&hnat_priv->flow_entry_lock);
 	return 0;
 }
 
@@ -661,6 +721,7 @@ static int hnat_start(u32 ppe_id)
 {
 	u32 foe_table_sz;
 	u32 foe_mib_tb_sz;
+	u32 foe_flow_sz;
 	int etry_num_cfg;
 
 	if (ppe_id >= CFG_PPE_NUM)
@@ -673,12 +734,21 @@ static int hnat_start(u32 ppe_id)
 		hnat_priv->foe_table_cpu[ppe_id] = dma_alloc_coherent(
 				hnat_priv->dev, foe_table_sz,
 				&hnat_priv->foe_table_dev[ppe_id], GFP_KERNEL);
-
-		if (hnat_priv->foe_table_cpu[ppe_id])
-			break;
+		if (hnat_priv->foe_table_cpu[ppe_id]) {
+			foe_flow_sz = (hnat_priv->foe_etry_num / 4) * sizeof(struct hlist_head);
+			hnat_priv->foe_flow[ppe_id] = devm_kzalloc(hnat_priv->dev, foe_flow_sz, GFP_KERNEL);
+			if (!hnat_priv->foe_flow[ppe_id]) {
+				dma_free_coherent(hnat_priv->dev, foe_table_sz,
+						  hnat_priv->foe_table_cpu[ppe_id],
+						  hnat_priv->foe_table_dev[ppe_id]);
+				hnat_priv->foe_table_cpu[ppe_id] = NULL;
+			} else {
+				break;
+			}
+		}
 	}
 
-	if (!hnat_priv->foe_table_cpu[ppe_id])
+	if (!hnat_priv->foe_table_cpu[ppe_id] || !hnat_priv->foe_flow[ppe_id])
 		return -1;
 	dev_info(hnat_priv->dev, "PPE%d entry number = %d\n",
 		 ppe_id, hnat_priv->foe_etry_num);
@@ -813,6 +883,10 @@ static void hnat_stop(u32 ppe_id)
 		writel(0, hnat_priv->ppe_base[ppe_id] + PPE_MIB_TB_BASE);
 		kfree(hnat_priv->acct[ppe_id]);
 	}
+
+	/* Release the allocated hnat_flow_entry nodes */
+	if (hnat_priv->foe_flow[ppe_id])
+		hnat_flow_entry_teardown_all(ppe_id);
 }
 
 static void hnat_release_netdev(void)
@@ -1150,6 +1224,8 @@ static int hnat_probe(struct platform_device *pdev)
 	if (err)
 		pr_info("hnat roaming work fail\n");
 
+	hnat_flow_entry_teardown_enable();
+
 	INIT_LIST_HEAD(&hnat_priv->xlat.map_list);
 
 	return 0;
@@ -1175,6 +1251,7 @@ static int hnat_remove(struct platform_device *pdev)
 	int i;
 
 	hnat_roaming_disable();
+	hnat_flow_entry_teardown_disable();
 	unregister_netdevice_notifier(&nf_hnat_netdevice_nb);
 	unregister_netevent_notifier(&nf_hnat_netevent_nb);
 	hnat_disable_hook();
diff --git a/drivers/net/ethernet/mediatek/mtk_hnat/hnat.h b/drivers/net/ethernet/mediatek/mtk_hnat/hnat.h
index c2ee453..788f819 100644
--- a/drivers/net/ethernet/mediatek/mtk_hnat/hnat.h
+++ b/drivers/net/ethernet/mediatek/mtk_hnat/hnat.h
@@ -966,6 +966,16 @@ struct mtk_hnat {
 	bool nf_stat_en;
 	struct xlat_conf xlat;
 	spinlock_t		entry_lock;
+	spinlock_t		flow_entry_lock;
+	struct hlist_head *foe_flow[MAX_PPE_NUM];
+};
+
+struct hnat_flow_entry {
+	struct hlist_node list;
+	struct foe_entry data;
+	unsigned long last_update;
+	u16 ppe_index;
+	u16 hash;
 };
 
 struct extdev_entry {
@@ -1371,19 +1381,20 @@ int mtk_ppe_get_xlat_v6_by_v4(u32 *ipv4, struct in6_addr *ipv6,
 
 struct hnat_accounting *hnat_get_count(struct mtk_hnat *h, u32 ppe_id,
 				       u32 index, struct hnat_accounting *diff);
-
-static inline u16 foe_timestamp(struct mtk_hnat *h)
+bool hnat_flow_entry_match(struct foe_entry *entry, struct foe_entry *data);
+void hnat_flow_entry_delete(struct hnat_flow_entry *flow_entry);
+static inline u16 foe_timestamp(struct mtk_hnat *h, bool mcast)
 {
-	return (readl(hnat_priv->fe_base + 0x0010)) & 0xffff;
-}
+	u16 time_stamp;
 
-static inline int hnat_check_entry_pkt_type_match(struct foe_entry *entry, struct foe_entry *foe)
-{
-	if ((IS_IPV4_HNAPT(entry) && !IS_IPV4_HNAPT(foe)) ||
-	    (IS_IPV6_5T_ROUTE(entry) && !IS_IPV6_5T_ROUTE(foe)))
-		return -1;
+	if (mcast)
+		time_stamp = (readl(h->fe_base + 0x0010)) & 0xffff;
+	else if (h->data->version == MTK_HNAT_V2 || h->data->version == MTK_HNAT_V3)
+		time_stamp = readl(h->fe_base + 0x0010) & (0xFF);
+	else
+		time_stamp = readl(h->fe_base + 0x0010) & (0x7FFF);
 
-	return 0;
+	return time_stamp;
 }
 
 #endif /* NF_HNAT_H */
diff --git a/drivers/net/ethernet/mediatek/mtk_hnat/hnat_mcast.c b/drivers/net/ethernet/mediatek/mtk_hnat/hnat_mcast.c
index 2c9c3fd..976fbe9 100644
--- a/drivers/net/ethernet/mediatek/mtk_hnat/hnat_mcast.c
+++ b/drivers/net/ethernet/mediatek/mtk_hnat/hnat_mcast.c
@@ -272,7 +272,7 @@ static void hnat_mcast_check_timestamp(struct timer_list *t)
 			entry = hnat_priv->foe_table_cpu[i] + hash_index;
 			if (entry->bfib1.sta == 1) {
 				e_ts = (entry->ipv4_hnapt.m_timestamp) & 0xffff;
-				foe_ts = foe_timestamp(hnat_priv);
+				foe_ts = foe_timestamp(hnat_priv, true);
 				if ((foe_ts - e_ts) > 0x3000)
 					foe_ts = (~(foe_ts)) & 0xffff;
 				if (abs(foe_ts - e_ts) > 20)
diff --git a/drivers/net/ethernet/mediatek/mtk_hnat/hnat_nf_hook.c b/drivers/net/ethernet/mediatek/mtk_hnat/hnat_nf_hook.c
index d9d7121..06618bf 100644
--- a/drivers/net/ethernet/mediatek/mtk_hnat/hnat_nf_hook.c
+++ b/drivers/net/ethernet/mediatek/mtk_hnat/hnat_nf_hook.c
@@ -184,6 +184,55 @@ int ext_if_del(struct extdev_entry *ext_entry)
 	return i;
 }
 
+bool hnat_flow_entry_match(struct foe_entry *entry, struct foe_entry *data)
+{
+	int len;
+
+	if (entry->udib1.udp != data->udib1.udp)
+		return false;
+
+	switch (entry->udib1.pkt_type) {
+	case IPV4_HNAPT:
+	case IPV4_HNAT:
+	case IPV4_DSLITE:
+	case IPV4_MAP_T:
+	case IPV4_MAP_E:
+		len = offsetof(struct hnat_ipv4_hnapt, info_blk2);
+		break;
+	case IPV6_3T_ROUTE:
+	case IPV6_5T_ROUTE:
+	case IPV6_6RD:
+	case IPV6_HNAPT:
+	case IPV6_HNAT:
+		len = offsetof(struct hnat_ipv6_5t_route, resv1);
+		break;
+	}
+
+	return !memcmp(&entry->ipv4_hnapt.sip, &data->ipv4_hnapt.sip, len - 4);
+}
+
+void hnat_flow_entry_delete(struct hnat_flow_entry *flow_entry)
+{
+	hlist_del_init(&flow_entry->list);
+	kfree(flow_entry);
+}
+
+static struct hnat_flow_entry *hnat_flow_entry_search(u16 ppe_index, u16 hash, struct foe_entry *data)
+{
+	struct hnat_flow_entry *flow_entry;
+	struct hlist_head *head = &hnat_priv->foe_flow[ppe_index][hash / 4];
+	struct hlist_node *n;
+
+	hlist_for_each_entry_safe(flow_entry, n, head, list) {
+		if (flow_entry->ppe_index == ppe_index &&
+		    flow_entry->hash == hash &&
+		    hnat_flow_entry_match(&flow_entry->data, data))
+		    return flow_entry;
+	}
+
+	return NULL;
+}
+
 static void foe_clear_ethdev_bind_entries(struct net_device *dev)
 {
 	struct net_device *master_dev = dev;
@@ -238,8 +287,7 @@ static void foe_clear_ethdev_bind_entries(struct net_device *dev)
 
 			if (match_dev) {
 				entry->bfib1.state = INVALID;
-				entry->bfib1.time_stamp =
-					readl((hnat_priv->fe_base + 0x0010)) & 0xFF;
+				entry->bfib1.time_stamp = foe_timestamp(hnat_priv, false);
 				total++;
 			}
 		}
@@ -262,9 +310,8 @@ void foe_clear_all_bind_entries(void)
 		for (hash_index = 0; hash_index < hnat_priv->foe_etry_num; hash_index++) {
 			entry = hnat_priv->foe_table_cpu[i] + hash_index;
 			if (entry->bfib1.state == BIND) {
-				entry->ipv4_hnapt.udib1.state = INVALID;
-				entry->ipv4_hnapt.udib1.time_stamp =
-					readl((hnat_priv->fe_base + 0x0010)) & 0xFF;
+				entry->udib1.state = INVALID;
+				entry->udib1.time_stamp = foe_timestamp(hnat_priv, false);
 			}
 		}
 	}
@@ -379,9 +426,8 @@ void foe_clear_entry(struct neighbour *neigh)
 					cr_set_field(hnat_priv->ppe_base[i] + PPE_TB_CFG,
 						     SMA, SMA_ONLY_FWD_CPU);
 
-					entry->ipv4_hnapt.udib1.state = INVALID;
-					entry->ipv4_hnapt.udib1.time_stamp =
-						readl((hnat_priv->fe_base + 0x0010)) & 0xFF;
+					entry->udib1.state = INVALID;
+					entry->udib1.time_stamp = foe_timestamp(hnat_priv, false);
 
 					/* clear HWNAT cache */
 					hnat_cache_ebl(1);
@@ -1287,11 +1333,7 @@ struct foe_entry ppe_fill_info_blk(struct foe_entry entry,
 	entry.bfib1.psn = (hw_path->flags & FLOW_OFFLOAD_PATH_PPPOE) ? 1 : 0;
 	entry.bfib1.vlan_layer += (hw_path->flags & FLOW_OFFLOAD_PATH_VLAN) ? 1 : 0;
 	entry.bfib1.vpm = (entry.bfib1.vlan_layer) ? 1 : 0;
-	entry.bfib1.cah = 0;
-	entry.bfib1.time_stamp = (hnat_priv->data->version == MTK_HNAT_V2 ||
-				  hnat_priv->data->version == MTK_HNAT_V3) ?
-		readl(hnat_priv->fe_base + 0x0010) & (0xFF) :
-		readl(hnat_priv->fe_base + 0x0010) & (0x7FFF);
+	entry.bfib1.cah = 1;
 
 	switch ((int)entry.bfib1.pkt_type) {
 	case IPV4_HNAPT:
@@ -1301,7 +1343,7 @@ struct foe_entry ppe_fill_info_blk(struct foe_entry entry,
 			entry.ipv4_hnapt.iblk2.mcast = 1;
 			if (hnat_priv->data->version == MTK_HNAT_V1_3) {
 				entry.bfib1.sta = 1;
-				entry.ipv4_hnapt.m_timestamp = foe_timestamp(hnat_priv);
+				entry.ipv4_hnapt.m_timestamp = foe_timestamp(hnat_priv, true);
 			}
 		} else {
 			entry.ipv4_hnapt.iblk2.mcast = 0;
@@ -1324,7 +1366,7 @@ struct foe_entry ppe_fill_info_blk(struct foe_entry entry,
 			entry.ipv6_5t_route.iblk2.mcast = 1;
 			if (hnat_priv->data->version == MTK_HNAT_V1_3) {
 				entry.bfib1.sta = 1;
-				entry.ipv4_hnapt.m_timestamp = foe_timestamp(hnat_priv);
+				entry.ipv4_hnapt.m_timestamp = foe_timestamp(hnat_priv, true);
 			}
 		} else {
 			entry.ipv6_5t_route.iblk2.mcast = 0;
@@ -1597,6 +1639,7 @@ static unsigned int skb_to_hnat_info(struct sk_buff *skb,
 	struct net_device *slave_dev[10];
 	struct list_head *iter;
 	struct foe_entry entry = { 0 };
+	struct hnat_flow_entry *flow_entry;
 	struct mtk_mac *mac;
 	struct iphdr *iph;
 	struct ipv6hdr *ip6h;
@@ -2287,59 +2330,64 @@ static unsigned int skb_to_hnat_info(struct sk_buff *skb,
 	 * by Wi-Fi whnat engine. These data and INFO2.dp will be updated and
 	 * the entry is set to BIND state in mtk_sw_nat_hook_tx().
 	 */
-	if (!whnat) {
-		entry.bfib1.ttl = 1;
-		entry.bfib1.state = BIND;
-	} else {
-		if (spin_trylock(&hnat_priv->entry_lock)) {
-			/* If this entry is already lock, we should not modify it right now */
-			if (is_hnat_entry_locked(foe)) {
-				skb_hnat_filled(skb) = HNAT_INFO_FILLED;
-				spin_unlock(&hnat_priv->entry_lock);
-				return 0;
-			}
+	if (whnat) {
+		/* Final check if the entry is not in UNBIND state,
+		 * we should not modify it right now.
+		 */
+		if (unlikely(foe->udib1.state != UNBIND))
+			return -1;
 
-			/* Final check if the entry is not in UNBIND state,
-			 * we should not modify it right now.
-			 */
-			if (foe->udib1.state != UNBIND) {
-				spin_unlock(&hnat_priv->entry_lock);
-				return 0;
+		spin_lock_bh(&hnat_priv->flow_entry_lock);
+
+		flow_entry = hnat_flow_entry_search(skb_hnat_ppe(skb),
+						    skb_hnat_entry(skb),
+						    &entry);
+		if (flow_entry) {
+			/* If the flow_entry is existed, replace it with the new one */
+			memcpy(&flow_entry->data, &entry, sizeof(entry));
+			flow_entry->last_update = jiffies;
+		} else {
+			flow_entry = kmalloc(sizeof(*flow_entry), GFP_KERNEL);
+			if (!flow_entry) {
+				printk_ratelimited(KERN_WARNING
+						   "%s: Allocate failed for flow_entry\n", __func__);
+				spin_unlock_bh(&hnat_priv->flow_entry_lock);
+				return -1;
 			}
+			flow_entry->ppe_index = skb_hnat_ppe(skb);
+			flow_entry->hash = skb_hnat_entry(skb);
+			flow_entry->last_update = jiffies;
+			memcpy(&flow_entry->data, &entry, sizeof(entry));
+			hlist_add_head(&flow_entry->list, &hnat_priv->foe_flow[skb_hnat_ppe(skb)][skb_hnat_entry(skb) / 4]);
+		}
 
-			/* Keep the entry locked until hook_tx is called */
-			hnat_set_entry_lock(&entry, true);
+		/* We must ensure all info has been updated */
+		wmb();
+		skb_hnat_filled(skb) = HNAT_INFO_FILLED;
 
-			/* We must ensure all info has been updated before set to hw */
-			wmb();
-			memcpy(foe, &entry, sizeof(entry));
+		spin_unlock_bh(&hnat_priv->flow_entry_lock);
 
-			skb_hnat_filled(skb) = HNAT_INFO_FILLED;
-			spin_unlock(&hnat_priv->entry_lock);
-		}
 		return 0;
 	}
 
+	entry.bfib1.ttl = 1;
+	entry.bfib1.state = BIND;
+
 hnat_entry_skip_bind:
-	if (spin_trylock(&hnat_priv->entry_lock)) {
-		if (foe->udib1.sta) {
-			spin_unlock(&hnat_priv->entry_lock);
+	if (spin_trylock_bh(&hnat_priv->entry_lock)) {
+		/* Final check if the entry is not in UNBIND state,
+		 * we should not modify it right now.
+		 */
+		if (unlikely(foe->udib1.state != UNBIND)) {
+			spin_unlock_bh(&hnat_priv->entry_lock);
 			return 0;
 		}
 
-		/* Set static mode to lock hardware entry */
 		foe->udib1.sta = 1;
+		/* Renew the entry timestamp */
+		entry.bfib1.time_stamp = foe_timestamp(hnat_priv, false);
 		/* We must ensure all info has been updated before set to hw */
 		wmb();
-		/* Final check if the entry is not in UNBIND state,
-		 * we should not modify it right now.
-		 */
-		if (foe->udib1.state != UNBIND ||
-		    hnat_check_entry_pkt_type_match(&entry, foe) < 0) {
-			foe->udib1.sta = 0;
-			spin_unlock(&hnat_priv->entry_lock);
-			return 0;
-		}
 
 		/* Before entry enter BIND state, write other fields first,
 		 * prevent racing with hardware accesses.
@@ -2350,10 +2398,11 @@ static unsigned int skb_to_hnat_info(struct sk_buff *skb,
 		wmb();
 		/* After other fields have been written, write info1 to BIND the entry */
 		memcpy(&foe->bfib1, &entry.bfib1, sizeof(entry.bfib1));
-
-		foe->bfib1.cah = 1;
+		/* We must ensure all info has been updated */
 		wmb();
 
+		spin_unlock_bh(&hnat_priv->entry_lock);
+
 		/* reset statistic for this entry */
 		if (hnat_priv->data->per_flow_accounting &&
 		    skb_hnat_entry(skb) < hnat_priv->foe_etry_num &&
@@ -2368,8 +2417,6 @@ static unsigned int skb_to_hnat_info(struct sk_buff *skb,
 					CTINFO2DIR(ctinfo);
 			}
 		}
-
-		spin_unlock(&hnat_priv->entry_lock);
 	}
 
 	return 0;
@@ -2378,27 +2425,17 @@ static unsigned int skb_to_hnat_info(struct sk_buff *skb,
 int mtk_sw_nat_hook_tx(struct sk_buff *skb, int gmac_no)
 {
 	struct foe_entry *hw_entry, entry;
+	struct hnat_flow_entry *flow_entry;
 	struct ethhdr *eth;
-	struct iphdr *iph;
-	struct ipv6hdr *ip6h;
 	struct nf_conn *ct;
 	enum ip_conntrack_info ctinfo;
 
 	if (!skb_hnat_is_hashed(skb) || skb_hnat_ppe(skb) >= CFG_PPE_NUM)
 		return NF_ACCEPT;
 
-	hw_entry = &hnat_priv->foe_table_cpu[skb_hnat_ppe(skb)][skb_hnat_entry(skb)];
-	memcpy(&entry, hw_entry, sizeof(entry));
-
-	if (!is_hnat_entry_locked(&entry))
-		return NF_ACCEPT;
-
-	if (unlikely(entry.bfib1.state != UNBIND))
-		goto check_release_entry_lock;
-
 	if (skb_hnat_alg(skb) || !is_hnat_info_filled(skb) ||
 	    !is_magic_tag_valid(skb) || !IS_SPACE_AVAILABLE_HEAD(skb))
-		goto check_release_entry_lock;
+		return NF_ACCEPT;
 
 	if (debug_level >= 7)
 		trace_printk(
@@ -2409,20 +2446,44 @@ int mtk_sw_nat_hook_tx(struct sk_buff *skb, int gmac_no)
 
 	if ((gmac_no != NR_WDMA0_PORT) && (gmac_no != NR_WDMA1_PORT) &&
 	    (gmac_no != NR_WDMA2_PORT) && (gmac_no != NR_WHNAT_WDMA_PORT))
-		goto check_release_entry_lock;
+		return NF_ACCEPT;
 
 	if (unlikely(!skb_mac_header_was_set(skb)))
-		goto check_release_entry_lock;
+		return NF_ACCEPT;
 
 	if (skb_hnat_reason(skb) != HIT_UNBIND_RATE_REACH)
-		goto check_release_entry_lock;
+		return NF_ACCEPT;
+
+	spin_lock_bh(&hnat_priv->flow_entry_lock);
+
+	hw_entry = &hnat_priv->foe_table_cpu[skb_hnat_ppe(skb)][skb_hnat_entry(skb)];
+
+	flow_entry = hnat_flow_entry_search(skb_hnat_ppe(skb),
+					    skb_hnat_entry(skb),
+					    hw_entry);
+	/* If the flow_entry is not prepared */
+	if (!flow_entry) {
+		spin_unlock_bh(&hnat_priv->flow_entry_lock);
+		return NF_ACCEPT;
+	}
+
+	if (unlikely(hw_entry->bfib1.state != UNBIND) ||
+	    unlikely(time_after_eq(jiffies, flow_entry->last_update + msecs_to_jiffies(3000)))) {
+		hnat_flow_entry_delete(flow_entry);
+		spin_unlock_bh(&hnat_priv->flow_entry_lock);
+		return NF_ACCEPT;
+	}
+
+	memcpy(&entry, &flow_entry->data, sizeof(entry));
+
+	spin_unlock_bh(&hnat_priv->flow_entry_lock);
 
 	eth = eth_hdr(skb);
 
 	/* not bind multicast if PPE mcast not enable */
 	if (!hnat_priv->data->mcast) {
 		if (is_multicast_ether_addr(eth->h_dest))
-			goto check_release_entry_lock;
+			return NF_ACCEPT;
 
 		if (IS_IPV4_GRP(&entry))
 			entry.ipv4_hnapt.iblk2.mcast = 0;
@@ -2430,22 +2491,6 @@ int mtk_sw_nat_hook_tx(struct sk_buff *skb, int gmac_no)
 			entry.ipv6_5t_route.iblk2.mcast = 0;
 	}
 
-	/* not bind if IP of entry and skb are different */
-	if (IS_IPV4_HNAPT(&entry)) {
-		iph = ip_hdr(skb);
-		if (!iph ||
-		    entry.ipv4_hnapt.new_sip != ntohl(iph->saddr) ||
-		    entry.ipv4_hnapt.new_dip != ntohl(iph->daddr))
-			goto check_release_entry_lock;
-	} else if (IS_IPV6_5T_ROUTE(&entry)) {
-		ip6h = ipv6_hdr(skb);
-		if (!ip6h ||
-		    !hnat_ipv6_addr_equal(&entry.ipv6_5t_route.ipv6_sip0, &ip6h->saddr) ||
-		    !hnat_ipv6_addr_equal(&entry.ipv6_5t_route.ipv6_dip0, &ip6h->daddr)) {
-			goto check_release_entry_lock;
-		}
-	}
-
 	/* Some mt_wifi virtual interfaces, such as apcli,
 	 * will change the smac for specail purpose.
 	 */
@@ -2655,25 +2700,22 @@ int mtk_sw_nat_hook_tx(struct sk_buff *skb, int gmac_no)
 
 	entry.bfib1.ttl = 1;
 	entry.bfib1.state = BIND;
-	entry.bfib1.sta = 0;
-
-	if (hw_entry->udib1.sta)
-		return NF_ACCEPT;
-
-	/* Set static mode to lock hardware entry */
-	hw_entry->udib1.sta = 1;
-	/* We must ensure all info has been updated before set to hw */
-	wmb();
 
+	spin_lock_bh(&hnat_priv->entry_lock);
 	/* Final check if the entry is not in UNBIND state,
 	 * we should not modify it right now.
 	 */
-	if (hw_entry->udib1.state != UNBIND ||
-	    hw_entry->udib1.pkt_type != entry.bfib1.pkt_type) {
-		hw_entry->udib1.sta = 0;
-		goto check_release_entry_lock;
+	if (unlikely(hw_entry->bfib1.state != UNBIND)) {
+		spin_unlock_bh(&hnat_priv->entry_lock);
+		return NF_ACCEPT;
 	}
 
+	hw_entry->udib1.sta = 1;
+	/* Renew the entry timestamp */
+	entry.bfib1.time_stamp = foe_timestamp(hnat_priv, false);
+	/* We must ensure the info has been updated before set to hw */
+	wmb();
+
 	/* Before entry enter BIND state, write other fields first,
 	 * prevent racing with hardware accesses.
 	 */
@@ -2683,11 +2725,16 @@ int mtk_sw_nat_hook_tx(struct sk_buff *skb, int gmac_no)
 	wmb();
 	/* After other fields have been written, write info1 to BIND the entry */
 	memcpy(&hw_entry->bfib1, &entry.bfib1, sizeof(entry.bfib1));
-
-	hnat_set_entry_lock(hw_entry, false);
-	hw_entry->bfib1.cah = 1;
+	/* We must ensure the info has been updated */
 	wmb();
 
+	spin_unlock_bh(&hnat_priv->entry_lock);
+
+	spin_lock_bh(&hnat_priv->flow_entry_lock);
+	/* Clear the flow entry node in the foe_flow table */
+	hnat_flow_entry_delete(flow_entry);
+	spin_unlock_bh(&hnat_priv->flow_entry_lock);
+
 	/* reset statistic for this entry */
 	if (hnat_priv->data->per_flow_accounting) {
 		memset(&hnat_priv->acct[skb_hnat_ppe(skb)][skb_hnat_entry(skb)],
@@ -2754,10 +2801,6 @@ int mtk_sw_nat_hook_tx(struct sk_buff *skb, int gmac_no)
 	}
 #endif
 	return NF_ACCEPT;
-
-check_release_entry_lock:
-	hnat_check_release_entry_lock(hw_entry);
-	return NF_ACCEPT;
 }
 
 int mtk_sw_nat_hook_rx(struct sk_buff *skb)
@@ -3316,7 +3359,7 @@ static unsigned int mtk_hnat_nf_post_routing(
 		/* update mcast timestamp*/
 		if (hnat_priv->data->version == MTK_HNAT_V1_3 &&
 		    hnat_priv->data->mcast && entry->bfib1.sta == 1)
-			entry->ipv4_hnapt.m_timestamp = foe_timestamp(hnat_priv);
+			entry->ipv4_hnapt.m_timestamp = foe_timestamp(hnat_priv, true);
 
 		if (entry_hnat_is_bound(entry)) {
 			memset(skb_hnat_info(skb), 0, sizeof(struct hnat_desc));
-- 
2.45.2

